{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuexi21/RL_CO_training/blob/main/r_co_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCeWC9rX-bjU"
      },
      "source": [
        "<!-- ## prepare the data -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n4kS8vKR-bjV"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "from sklearn.datasets import make_moons as moon\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# define the classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e7oFEh4o-bjW"
      },
      "outputs": [],
      "source": [
        "# set the dataset.\n",
        "dataset = moon(5000, noise=0.3, random_state=42)\n",
        "X,y = dataset\n",
        "\n",
        "# split the training(labeled) as 10% of dataset\n",
        "X_l, X_ul, y_l, y_ul = train_test_split(X, y, test_size=0.8, random_state=0)\n",
        "\n",
        "\n",
        "# split the training(labeled) as 50% of  labeled dataset\n",
        "X_l_train, X_l_test, y_l_train, y_l_test = train_test_split(X_l, y_l, test_size=0.5, random_state=0)\n",
        "\n",
        "# 2-classifier\n",
        "clf_1 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"knn\", KNeighborsClassifier(n_neighbors=11))\n",
        "        ]\n",
        ")\n",
        "\n",
        "# clf 1\n",
        "clf_2 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"RF\", RandomForestClassifier())\n",
        "        ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AJqps6K9-bjW"
      },
      "outputs": [],
      "source": [
        "# define ENV\n",
        "\n",
        "# for clustering the unlabeld data\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Env():\n",
        "    def __init__(self, classifier_1, classifier_2, input_ul_data, k, X_test, y_test, X_reset, y_reset):\n",
        "        # super().__init__\n",
        "        self.model_1 = classifier_1\n",
        "        self.model_2 = classifier_2\n",
        "        # UN LABEL DATA\n",
        "        self.X_ul = input_ul_data\n",
        "        # define the evaluate data, later use for the reward\n",
        "        self.X_eval = X_test\n",
        "        self.y_eval = y_test\n",
        "        self.X_reset = X_reset\n",
        "        self.y_reset = y_reset\n",
        "        # cluster the data\n",
        "        self.action_size = k\n",
        "        self.kmeans = KMeans(n_clusters=k,  n_init=10)\n",
        "        self.cluster_label = self.kmeans.fit_predict(self.X_ul)\n",
        "        self.u_cluster_label = np.unique(self.cluster_label)\n",
        "        self.centroids = self.kmeans.cluster_centers_\n",
        "        self.observation_size = self.get_state(reset=True).shape[1]\n",
        "\n",
        "    # def cluster_plot(self):\n",
        "    #     for i in self.u_cluster_label:\n",
        "    #         plt.scatter(self.X_ul[self.cluster_label == i , 0] ,\n",
        "    #                     self.X_ul[self.cluster_label == i , 1] ,\n",
        "    #                     label = i)\n",
        "    #     plt.scatter(self.centroids[:,0],\n",
        "    #                 self.centroids[:,1],\n",
        "    #                 s=80,\n",
        "    #                 color='k')\n",
        "    #     # plt.legend()\n",
        "    #     plt.title(f'{self.k} cluster (centroids) of unlabeled data')\n",
        "    #     plt.show()\n",
        "\n",
        "    # update 2 clf\n",
        "    def train_2_clf(self, X, y):\n",
        "        self.model_1.fit(X, y)\n",
        "        self.model_2.fit(X, y)\n",
        "\n",
        "    def get_state(self,reset=False):\n",
        "        np.random.seed(123)\n",
        "        if reset:\n",
        "            self.train_2_clf(self.X_reset, self.y_reset)\n",
        "            print(\"reset\")\n",
        "        out_1 = self.model_1.predict_proba(self.centroids)\n",
        "        out_2 = self.model_2.predict_proba(self.centroids)\n",
        "        state_proba = np.concatenate((out_1, out_2), axis=1)\n",
        "        return  torch.from_numpy(state_proba).to(torch.float32).reshape(1, -1)\n",
        "\n",
        "\n",
        "    def get_acc(self):\n",
        "        pred_1 = self.model_1.predict(self.X_eval)\n",
        "        pred_2 = self.model_1.predict(self.X_eval)\n",
        "        acc_1 = accuracy_score(pred_1, self.y_eval)\n",
        "        acc_2 = accuracy_score(pred_2, self.y_eval)\n",
        "        return acc_1, acc_2\n",
        "\n",
        "    ######\n",
        "    ######\n",
        "    def get_subset(self, action):\n",
        "        # choose subset\n",
        "        subset = self.X_ul[self.cluster_label == action]\n",
        "        return subset\n",
        "\n",
        "    def co_training(self, subset):\n",
        "        ## get posodu label\n",
        "        clf_0_p_label = self.model_1.predict(subset)\n",
        "        clf_1_p_label = self.model_2.predict(subset)\n",
        "\n",
        "        ## get proba_\n",
        "        clf_0_p_y = self.model_1.predict_proba(subset)\n",
        "        clf_1_p_y = self.model_2.predict_proba(subset)\n",
        "\n",
        "        #get the label size\n",
        "        y_num = subset.shape[0]\n",
        "        # set empty y  #type=ndarray\n",
        "        y_ul_action = np.zeros(y_num,)\n",
        "\n",
        "        #############\n",
        "        # confidence_diff = 0\n",
        "        # combine the lable from two classifier, choose the most conffidence\n",
        "        for i in range(y_num):\n",
        "            if max(clf_0_p_y[i, ]) > max(clf_1_p_y[i, ]):\n",
        "                y_ul_action[i] = clf_0_p_label[i]\n",
        "                # print('0')\n",
        "            else:\n",
        "                y_ul_action[i] = clf_1_p_label[i]\n",
        "                # print('1')\n",
        "\n",
        "        ########### update the label_set for traning\n",
        "        X_updated = np.concatenate((X_l_train, subset), axis=0)\n",
        "        y_updated = np.concatenate((y_l_train, y_ul_action), axis=0)\n",
        "\n",
        "        # print(f'X shape is {X_updated.shape} \\ny shape is {y_updated.shape}')\n",
        "\n",
        "        ############# use the updated labeld dateset retrain those 2 classifier\n",
        "        self.train_2_clf(X_updated, y_updated)\n",
        "\n",
        "        # RETURN THE co-trained 2 CLASSIFIER'S accuracy.\n",
        "        acc_1_, acc_2_ = self.get_acc()\n",
        "        return acc_1_, acc_2_\n",
        "\n",
        "    def step(self, action):\n",
        "        # GET THE bigining state accuracy, later use to calculate the reward\n",
        "        pre_acc_1, pre_acc_2 = self.get_acc()\n",
        "\n",
        "        # choose subset\n",
        "        choosen_subset =  self.get_subset(action)\n",
        "\n",
        "        # cotraining the 2 classifier\n",
        "        acc1_, acc2_ = self.co_training(choosen_subset )\n",
        "\n",
        "        # get the next state_\n",
        "        n_state = self.get_state()\n",
        "\n",
        "        ##############\n",
        "        # calculate the reward\n",
        "        ##############\n",
        "        reward_0 =  acc1_ - pre_acc_1\n",
        "        reward_1 = acc2_ - pre_acc_2\n",
        "\n",
        "        if reward_0 > 0 and reward_1 > 0:\n",
        "            reward = reward_0 * reward_1\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        return n_state, reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_AkP5ovW-bjX",
        "outputId": "62f1176e-53f5-4612-e17f-e7e533ab1c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "reset\n"
          ]
        }
      ],
      "source": [
        "k = 20\n",
        "env = Env(clf_1, clf_2, input_ul_data=X_ul, k=k, X_test=X_l_test, y_test=y_l_test, X_reset=X_l_train, y_reset=y_l_train)\n",
        "state_0 = env.get_state(reset=True)\n",
        "# env.cluster_plot()\n",
        "# env.get_acc()\n",
        "# env.get_subset(2)\n",
        "state, reward = env.step(19)\n",
        "# state.shape\n",
        "# reward\n",
        "\n",
        "# print(state_0)\n",
        "# print(state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuqYSBf7-bjX"
      },
      "source": [
        "<!-- #### DQN\n",
        "\n",
        "<img src=\"https://yinyoupoet.github.io/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_8coZ4g_pRtfyoHmsuzMH6g.png\" alt=\"Description of the image\" width=\"400\" height=\"300\">\n",
        "\n",
        "##### loss\n",
        "\n",
        "\n",
        "<img src=\"https://yinyoupoet.github.io/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_YCgMUijhU4p_y3sctvu-kQ.png\" alt=\"Description of the image\" width=\"300\" height=\"50\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vm4jSekd-bjX"
      },
      "outputs": [],
      "source": [
        "## RL functions##\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the Q-network (a simple feedforward neural network)\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        # # bug - some times the state is not\n",
        "        # if len(state) != 4:\n",
        "        #     state = state[0]\n",
        "        experience_tuple = (state, action, reward, next_state)\n",
        "        # Append experience_tuple to the memory buffer\n",
        "        self.memory.append(experience_tuple)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Draw a random sample of size batch_size\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        # Transform batch into a tuple of lists\n",
        "        states, actions, rewards, next_states = (zip(*batch))\n",
        "        return states, actions, rewards, next_states\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5XYUqe9H-bjY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "#\n",
        "EPS_START = 1\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "steps_done = 0\n",
        "\n",
        "####\n",
        "buffer_size = 10000\n",
        "###\n",
        "episodes = 500\n",
        "max_step = 100\n",
        "batch_size = 64\n",
        "TAU = 0.005\n",
        "gamma = 0.99\n",
        "\n",
        "# hyper parameter\n",
        "observation_size = env.observation_size\n",
        "action_size = env.action_size\n",
        "lr = 1e-4\n",
        "\n",
        "###\n",
        "# Initialize networks and optimizer\n",
        "q_network = QNetwork(observation_size, action_size)\n",
        "target_network = QNetwork(observation_size, action_size)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "# Replay memory\n",
        "replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return the largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            action = torch.argmax(q_network(state)).item()\n",
        "            return action\n",
        "    else:\n",
        "        return np.random.choice(range(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FSFEIzc7-bjY"
      },
      "outputs": [],
      "source": [
        "# Function to update the Q-network\n",
        "def train():\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "        # prepare the training data\n",
        "    states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "\n",
        "    rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "    actions = torch.tensor(np.array(actions)).unsqueeze(1)\n",
        "    states = torch.tensor(np.array(states)).squeeze(1)\n",
        "    next_states = torch.tensor(np.array(next_states)).squeeze(1)\n",
        "\n",
        "    # # Compute current Q values\n",
        "    q_values = q_network(states).gather(1, actions)\n",
        "\n",
        "\n",
        "\n",
        "        # Compute target Q values\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_network(next_states).max(1).values.unsqueeze(1)\n",
        "    targets = rewards + (gamma * next_q_values)\n",
        "\n",
        "\n",
        "        # Update the network\n",
        "    loss = loss_fn(q_values, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_haF2H6b-bjZ",
        "outputId": "bb72cd53-d4a8-4e20-e012-9895eb3a4f70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "Episode: 0, Total Reward: 0.0005000000000000009\n",
            "reset\n",
            "Episode: 1, Total Reward: 0.0016000000000000033\n",
            "reset\n",
            "Episode: 2, Total Reward: 0.001500000000000003\n",
            "reset\n",
            "Episode: 3, Total Reward: 0.0017560000000000032\n",
            "reset\n",
            "Episode: 4, Total Reward: 0.0016040000000000032\n",
            "reset\n",
            "Episode: 5, Total Reward: 0.0016560000000000032\n",
            "reset\n",
            "Episode: 6, Total Reward: 0.0021560000000000043\n",
            "reset\n",
            "Episode: 7, Total Reward: 0.001932000000000004\n",
            "reset\n",
            "Episode: 8, Total Reward: 0.001948000000000004\n",
            "reset\n",
            "Episode: 9, Total Reward: 0.0025600000000000054\n",
            "reset\n",
            "Episode: 10, Total Reward: 0.002480000000000005\n",
            "reset\n",
            "Episode: 11, Total Reward: 0.0018760000000000035\n",
            "reset\n",
            "Episode: 12, Total Reward: 0.0023320000000000046\n",
            "reset\n"
          ]
        }
      ],
      "source": [
        "#training loop\n",
        "results = []\n",
        "for episode in range(episodes):\n",
        "    state = env.get_state(reset=True)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(max_step):\n",
        "        action = select_action(state)\n",
        "        observation, reward = env.step(action)\n",
        "# '''\n",
        "#         done = terminated or truncated\n",
        "#         if terminated:\n",
        "#             next_state = None\n",
        "#         else:\n",
        "#             next_state = observation\n",
        "# '''\n",
        "        next_state = observation\n",
        "\n",
        "\n",
        "        replay_buffer.push(state, action, reward, next_state)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        train()\n",
        "\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        target_net_state_dict = q_network.state_dict()\n",
        "        policy_net_state_dict = target_network.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_network.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        # if terminated or truncated:\n",
        "        #     break\n",
        "\n",
        "\n",
        "    results.append(total_reward)\n",
        "    print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "    ## for debug - train() - just get some replay memory.\n",
        "    # if episode == 3:\n",
        "    #     print('break')\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-IRAV9y-bjZ"
      },
      "outputs": [],
      "source": [
        "# states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "# print(np.array(states).shape)\n",
        "# print(np.array(actions).shape)\n",
        "# print(np.array(rewards).shape)\n",
        "# print(np.array(next_states).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtE0pfB9-bjZ"
      },
      "outputs": [],
      "source": [
        "  # states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "\n",
        "  # # # Compute a mask of non-final states and concatenate the batch elements\n",
        "  # # non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "  # #                                       next_states)))\n",
        "  # # non_final_next_states = torch.tensor(np.array([s for s in next_states\n",
        "  # #                                             if s is not None]))\n",
        "  # # #\n",
        "  # rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "  # actions = torch.tensor(np.array(actions)).unsqueeze(1)\n",
        "  # states = torch.tensor(np.array(states)).squeeze(1)\n",
        "  # next_states = torch.tensor(np.array(next_states)).squeeze(1)\n",
        "\n",
        "  # # # Compute current Q values\n",
        "  # q_values = q_network(states).gather(1, actions)\n",
        "\n",
        "\n",
        "\n",
        "  #     # Compute target Q values\n",
        "  # with torch.no_grad():\n",
        "  #     next_q_values = target_network(next_states).max(1).values.unsqueeze(1)\n",
        "  # targets = rewards + (gamma * next_q_values)\n",
        "\n",
        "  # #     # Update the network\n",
        "  # loss = loss_fn(q_values, targets)\n",
        "  # optimizer.zero_grad()\n",
        "  # loss.backward()\n",
        "  # optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9-6qrqH-bjZ"
      },
      "outputs": [],
      "source": [
        "# # rewards\n",
        "# # actions.shape\n",
        "# # states.shape\n",
        "# # next_states.shape\n",
        "# q_values.shape\n",
        "# targets.shape\n",
        "# next_q_values.unsqueeze(1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4b8AWyg-bjZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}