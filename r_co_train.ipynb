{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuexi21/RL_CO_training/blob/main/r_co_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCeWC9rX-bjU"
      },
      "source": [
        "<!-- ## prepare the data -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n4kS8vKR-bjV"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "from sklearn.datasets import make_moons as moon\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# define the classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e7oFEh4o-bjW"
      },
      "outputs": [],
      "source": [
        "# set the dataset.\n",
        "dataset = moon(5000, noise=0.3, random_state=42)\n",
        "X,y = dataset\n",
        "\n",
        "# split the training(labeled) as 10% of dataset\n",
        "X_l, X_ul, y_l, y_ul = train_test_split(X, y, test_size=0.8, random_state=0)\n",
        "\n",
        "\n",
        "# split the training(labeled) as 50% of  labeled dataset\n",
        "X_l_train, X_l_test, y_l_train, y_l_test = train_test_split(X_l, y_l, test_size=0.5, random_state=0)\n",
        "\n",
        "# 2-classifier\n",
        "clf_1 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"knn\", KNeighborsClassifier(n_neighbors=11))\n",
        "        ]\n",
        ")\n",
        "\n",
        "# clf 1\n",
        "clf_2 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"RF\", RandomForestClassifier())\n",
        "        ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AJqps6K9-bjW"
      },
      "outputs": [],
      "source": [
        "# define ENV\n",
        "\n",
        "# for clustering the unlabeld data\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Env():\n",
        "    def __init__(self, classifier_1, classifier_2, input_ul_data, k, X_test, y_test, X_reset, y_reset):\n",
        "        # super().__init__\n",
        "        self.model_1 = classifier_1\n",
        "        self.model_2 = classifier_2\n",
        "        # UN LABEL DATA\n",
        "        self.X_ul = input_ul_data\n",
        "        # define the evaluate data, later use for the reward\n",
        "        self.X_eval = X_test\n",
        "        self.y_eval = y_test\n",
        "        self.X_reset = X_reset\n",
        "        self.y_reset = y_reset\n",
        "        # cluster the data\n",
        "        self.action_size = k\n",
        "        self.kmeans = KMeans(n_clusters=k,  n_init=10)\n",
        "        self.cluster_label = self.kmeans.fit_predict(self.X_ul)\n",
        "        self.u_cluster_label = np.unique(self.cluster_label)\n",
        "        self.centroids = self.kmeans.cluster_centers_\n",
        "        self.observation_size = self.get_state(reset=True).shape[1]\n",
        "\n",
        "    # def cluster_plot(self):\n",
        "    #     for i in self.u_cluster_label:\n",
        "    #         plt.scatter(self.X_ul[self.cluster_label == i , 0] ,\n",
        "    #                     self.X_ul[self.cluster_label == i , 1] ,\n",
        "    #                     label = i)\n",
        "    #     plt.scatter(self.centroids[:,0],\n",
        "    #                 self.centroids[:,1],\n",
        "    #                 s=80,\n",
        "    #                 color='k')\n",
        "    #     # plt.legend()\n",
        "    #     plt.title(f'{self.k} cluster (centroids) of unlabeled data')\n",
        "    #     plt.show()\n",
        "\n",
        "    # update 2 clf\n",
        "    def train_2_clf(self, X, y):\n",
        "        self.model_1.fit(X, y)\n",
        "        self.model_2.fit(X, y)\n",
        "\n",
        "    def get_state(self,reset=False):\n",
        "        np.random.seed(123)\n",
        "        if reset:\n",
        "            self.train_2_clf(self.X_reset, self.y_reset)\n",
        "            print(\"reset\")\n",
        "        out_1 = self.model_1.predict_proba(self.centroids)\n",
        "        out_2 = self.model_2.predict_proba(self.centroids)\n",
        "        state_proba = np.concatenate((out_1, out_2), axis=1)\n",
        "        return  torch.from_numpy(state_proba).to(torch.float32).reshape(1, -1)\n",
        "\n",
        "\n",
        "    def get_acc(self):\n",
        "        pred_1 = self.model_1.predict(self.X_eval)\n",
        "        pred_2 = self.model_1.predict(self.X_eval)\n",
        "        acc_1 = accuracy_score(pred_1, self.y_eval)\n",
        "        acc_2 = accuracy_score(pred_2, self.y_eval)\n",
        "        return acc_1, acc_2\n",
        "\n",
        "    ######\n",
        "    ######\n",
        "    def get_subset(self, action):\n",
        "        # choose subset\n",
        "        subset = self.X_ul[self.cluster_label == action]\n",
        "        return subset\n",
        "\n",
        "    def co_training(self, subset):\n",
        "        ## get posodu label\n",
        "        clf_0_p_label = self.model_1.predict(subset)\n",
        "        clf_1_p_label = self.model_2.predict(subset)\n",
        "\n",
        "        ## get proba_\n",
        "        clf_0_p_y = self.model_1.predict_proba(subset)\n",
        "        clf_1_p_y = self.model_2.predict_proba(subset)\n",
        "\n",
        "        #get the label size\n",
        "        y_num = subset.shape[0]\n",
        "        # set empty y  #type=ndarray\n",
        "        y_ul_action = np.zeros(y_num,)\n",
        "\n",
        "        #############\n",
        "        # confidence_diff = 0\n",
        "        # combine the lable from two classifier, choose the most conffidence\n",
        "        for i in range(y_num):\n",
        "            if max(clf_0_p_y[i, ]) > max(clf_1_p_y[i, ]):\n",
        "                y_ul_action[i] = clf_0_p_label[i]\n",
        "                # print('0')\n",
        "            else:\n",
        "                y_ul_action[i] = clf_1_p_label[i]\n",
        "                # print('1')\n",
        "\n",
        "        ########### update the label_set for traning\n",
        "        X_updated = np.concatenate((X_l_train, subset), axis=0)\n",
        "        y_updated = np.concatenate((y_l_train, y_ul_action), axis=0)\n",
        "\n",
        "        # print(f'X shape is {X_updated.shape} \\ny shape is {y_updated.shape}')\n",
        "\n",
        "        ############# use the updated labeld dateset retrain those 2 classifier\n",
        "        self.train_2_clf(X_updated, y_updated)\n",
        "\n",
        "        # RETURN THE co-trained 2 CLASSIFIER'S accuracy.\n",
        "        acc_1_, acc_2_ = self.get_acc()\n",
        "        return acc_1_, acc_2_\n",
        "\n",
        "    def step(self, action):\n",
        "        # GET THE bigining state accuracy, later use to calculate the reward\n",
        "        pre_acc_1, pre_acc_2 = self.get_acc()\n",
        "\n",
        "        # choose subset\n",
        "        choosen_subset =  self.get_subset(action)\n",
        "\n",
        "        # cotraining the 2 classifier\n",
        "        acc1_, acc2_ = self.co_training(choosen_subset )\n",
        "\n",
        "        # get the next state_\n",
        "        n_state = self.get_state()\n",
        "\n",
        "        ##############\n",
        "        # calculate the reward\n",
        "        ##############\n",
        "        reward_0 =  acc1_ - pre_acc_1\n",
        "        reward_1 = acc2_ - pre_acc_2\n",
        "\n",
        "        if reward_0 > 0 and reward_1 > 0:\n",
        "            reward = reward_0 * reward_1\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        return n_state, reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_AkP5ovW-bjX",
        "outputId": "62f1176e-53f5-4612-e17f-e7e533ab1c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "reset\n"
          ]
        }
      ],
      "source": [
        "k = 20\n",
        "env = Env(clf_1, clf_2, input_ul_data=X_ul, k=k, X_test=X_l_test, y_test=y_l_test, X_reset=X_l_train, y_reset=y_l_train)\n",
        "state_0 = env.get_state(reset=True)\n",
        "# env.cluster_plot()\n",
        "# env.get_acc()\n",
        "# env.get_subset(2)\n",
        "state, reward = env.step(19)\n",
        "# state.shape\n",
        "# reward\n",
        "\n",
        "# print(state_0)\n",
        "# print(state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuqYSBf7-bjX"
      },
      "source": [
        "<!-- #### DQN\n",
        "\n",
        "<img src=\"https://yinyoupoet.github.io/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_8coZ4g_pRtfyoHmsuzMH6g.png\" alt=\"Description of the image\" width=\"400\" height=\"300\">\n",
        "\n",
        "##### loss\n",
        "\n",
        "\n",
        "<img src=\"https://yinyoupoet.github.io/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_YCgMUijhU4p_y3sctvu-kQ.png\" alt=\"Description of the image\" width=\"300\" height=\"50\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vm4jSekd-bjX"
      },
      "outputs": [],
      "source": [
        "## RL functions##\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the Q-network (a simple feedforward neural network)\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        # # bug - some times the state is not\n",
        "        # if len(state) != 4:\n",
        "        #     state = state[0]\n",
        "        experience_tuple = (state, action, reward, next_state)\n",
        "        # Append experience_tuple to the memory buffer\n",
        "        self.memory.append(experience_tuple)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Draw a random sample of size batch_size\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        # Transform batch into a tuple of lists\n",
        "        states, actions, rewards, next_states = (zip(*batch))\n",
        "        return states, actions, rewards, next_states\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5XYUqe9H-bjY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "#\n",
        "EPS_START = 1\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "steps_done = 0\n",
        "\n",
        "####\n",
        "buffer_size = 10000\n",
        "###\n",
        "episodes = 500\n",
        "max_step = 100\n",
        "batch_size = 64\n",
        "TAU = 0.005\n",
        "gamma = 0.99\n",
        "\n",
        "# hyper parameter\n",
        "observation_size = env.observation_size\n",
        "action_size = env.action_size\n",
        "lr = 1e-4\n",
        "\n",
        "###\n",
        "# Initialize networks and optimizer\n",
        "q_network = QNetwork(observation_size, action_size)\n",
        "target_network = QNetwork(observation_size, action_size)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "# Replay memory\n",
        "replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return the largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            action = torch.argmax(q_network(state)).item()\n",
        "            return action\n",
        "    else:\n",
        "        return np.random.choice(range(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FSFEIzc7-bjY"
      },
      "outputs": [],
      "source": [
        "# Function to update the Q-network\n",
        "def train():\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "        # prepare the training data\n",
        "    states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "\n",
        "    rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "    actions = torch.tensor(np.array(actions)).unsqueeze(1)\n",
        "    states = torch.tensor(np.array(states)).squeeze(1)\n",
        "    next_states = torch.tensor(np.array(next_states)).squeeze(1)\n",
        "\n",
        "    # # Compute current Q values\n",
        "    q_values = q_network(states).gather(1, actions)\n",
        "\n",
        "\n",
        "\n",
        "        # Compute target Q values\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_network(next_states).max(1).values.unsqueeze(1)\n",
        "    targets = rewards + (gamma * next_q_values)\n",
        "\n",
        "\n",
        "        # Update the network\n",
        "    loss = loss_fn(q_values, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_haF2H6b-bjZ",
        "outputId": "098bf226-bfde-410d-ba71-4dbd5dab7470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "Episode: 0, Total Reward: 0.0005000000000000009\n",
            "reset\n",
            "Episode: 1, Total Reward: 0.0016000000000000033\n",
            "reset\n",
            "Episode: 2, Total Reward: 0.001500000000000003\n",
            "reset\n",
            "Episode: 3, Total Reward: 0.0017560000000000032\n",
            "reset\n",
            "Episode: 4, Total Reward: 0.0016040000000000032\n",
            "reset\n",
            "Episode: 5, Total Reward: 0.0016560000000000032\n",
            "reset\n",
            "Episode: 6, Total Reward: 0.0021560000000000043\n",
            "reset\n",
            "Episode: 7, Total Reward: 0.001932000000000004\n",
            "reset\n",
            "Episode: 8, Total Reward: 0.001948000000000004\n",
            "reset\n",
            "Episode: 9, Total Reward: 0.0025600000000000054\n",
            "reset\n",
            "Episode: 10, Total Reward: 0.002480000000000005\n",
            "reset\n",
            "Episode: 11, Total Reward: 0.0018760000000000035\n",
            "reset\n",
            "Episode: 12, Total Reward: 0.0023320000000000046\n",
            "reset\n",
            "Episode: 13, Total Reward: 0.0016480000000000034\n",
            "reset\n",
            "Episode: 14, Total Reward: 0.002148000000000004\n",
            "reset\n",
            "Episode: 15, Total Reward: 0.0016760000000000034\n",
            "reset\n",
            "Episode: 16, Total Reward: 0.0018000000000000034\n",
            "reset\n",
            "Episode: 17, Total Reward: 0.0018360000000000028\n",
            "reset\n",
            "Episode: 18, Total Reward: 0.0015160000000000028\n",
            "reset\n",
            "Episode: 19, Total Reward: 0.001548000000000003\n",
            "reset\n",
            "Episode: 20, Total Reward: 0.0012400000000000026\n",
            "reset\n",
            "Episode: 21, Total Reward: 0.001184000000000002\n",
            "reset\n",
            "Episode: 22, Total Reward: 0.0014880000000000023\n",
            "reset\n",
            "Episode: 23, Total Reward: 0.0013480000000000024\n",
            "reset\n",
            "Episode: 24, Total Reward: 0.001736000000000003\n",
            "reset\n",
            "Episode: 25, Total Reward: 0.0012480000000000021\n",
            "reset\n",
            "Episode: 26, Total Reward: 0.001380000000000003\n",
            "reset\n",
            "Episode: 27, Total Reward: 0.0014200000000000029\n",
            "reset\n",
            "Episode: 28, Total Reward: 0.0009200000000000019\n",
            "reset\n",
            "Episode: 29, Total Reward: 0.0011040000000000017\n",
            "reset\n",
            "Episode: 30, Total Reward: 0.0015560000000000031\n",
            "reset\n",
            "Episode: 31, Total Reward: 0.002096000000000004\n",
            "reset\n",
            "Episode: 32, Total Reward: 0.002560000000000005\n",
            "reset\n",
            "Episode: 33, Total Reward: 0.0018920000000000035\n",
            "reset\n",
            "Episode: 34, Total Reward: 0.0017040000000000033\n",
            "reset\n",
            "Episode: 35, Total Reward: 0.0016920000000000032\n",
            "reset\n",
            "Episode: 36, Total Reward: 0.001780000000000003\n",
            "reset\n",
            "Episode: 37, Total Reward: 0.0019200000000000037\n",
            "reset\n",
            "Episode: 38, Total Reward: 0.002460000000000005\n",
            "reset\n",
            "Episode: 39, Total Reward: 0.0015160000000000028\n",
            "reset\n",
            "Episode: 40, Total Reward: 0.0015480000000000027\n",
            "reset\n",
            "Episode: 41, Total Reward: 0.0015600000000000026\n",
            "reset\n",
            "Episode: 42, Total Reward: 0.0021320000000000037\n",
            "reset\n",
            "Episode: 43, Total Reward: 0.0009520000000000019\n",
            "reset\n",
            "Episode: 44, Total Reward: 0.0014400000000000025\n",
            "reset\n",
            "Episode: 45, Total Reward: 0.001352000000000002\n",
            "reset\n",
            "Episode: 46, Total Reward: 0.002380000000000005\n",
            "reset\n",
            "Episode: 47, Total Reward: 0.0013080000000000019\n",
            "reset\n",
            "Episode: 48, Total Reward: 0.0018960000000000036\n",
            "reset\n",
            "Episode: 49, Total Reward: 0.0017600000000000024\n",
            "reset\n",
            "Episode: 50, Total Reward: 0.0019440000000000035\n",
            "reset\n",
            "Episode: 51, Total Reward: 0.0020840000000000043\n",
            "reset\n",
            "Episode: 52, Total Reward: 0.0021400000000000043\n",
            "reset\n",
            "Episode: 53, Total Reward: 0.002120000000000004\n",
            "reset\n",
            "Episode: 54, Total Reward: 0.0020080000000000033\n",
            "reset\n",
            "Episode: 55, Total Reward: 0.001988000000000004\n",
            "reset\n",
            "Episode: 56, Total Reward: 0.0013440000000000023\n",
            "reset\n",
            "Episode: 57, Total Reward: 0.001972000000000003\n",
            "reset\n",
            "Episode: 58, Total Reward: 0.0020760000000000036\n",
            "reset\n",
            "Episode: 59, Total Reward: 0.0020240000000000032\n",
            "reset\n",
            "Episode: 60, Total Reward: 0.0009480000000000018\n",
            "reset\n",
            "Episode: 61, Total Reward: 0.0016160000000000029\n",
            "reset\n",
            "Episode: 62, Total Reward: 0.0016080000000000029\n",
            "reset\n",
            "Episode: 63, Total Reward: 0.0019080000000000034\n",
            "reset\n",
            "Episode: 64, Total Reward: 0.0025600000000000054\n",
            "reset\n",
            "Episode: 65, Total Reward: 0.0037360000000000075\n",
            "reset\n",
            "Episode: 66, Total Reward: 0.0019320000000000032\n",
            "reset\n",
            "Episode: 67, Total Reward: 0.0017080000000000027\n",
            "reset\n",
            "Episode: 68, Total Reward: 0.001888000000000003\n",
            "reset\n",
            "Episode: 69, Total Reward: 0.0015880000000000022\n",
            "reset\n",
            "Episode: 70, Total Reward: 0.001968000000000003\n",
            "reset\n",
            "Episode: 71, Total Reward: 0.0012200000000000023\n",
            "reset\n",
            "Episode: 72, Total Reward: 0.0022240000000000038\n",
            "reset\n",
            "Episode: 73, Total Reward: 0.0013000000000000017\n",
            "reset\n",
            "Episode: 74, Total Reward: 0.0011600000000000022\n",
            "reset\n",
            "Episode: 75, Total Reward: 0.001368000000000002\n",
            "reset\n",
            "Episode: 76, Total Reward: 0.0011200000000000019\n",
            "reset\n",
            "Episode: 77, Total Reward: 0.001272000000000002\n",
            "reset\n",
            "Episode: 78, Total Reward: 0.0012160000000000016\n",
            "reset\n",
            "Episode: 79, Total Reward: 0.001264000000000002\n",
            "reset\n",
            "Episode: 80, Total Reward: 0.001612000000000002\n",
            "reset\n",
            "Episode: 81, Total Reward: 0.001216000000000002\n",
            "reset\n",
            "Episode: 82, Total Reward: 0.001692000000000003\n",
            "reset\n",
            "Episode: 83, Total Reward: 0.0023640000000000037\n",
            "reset\n",
            "Episode: 84, Total Reward: 0.0021520000000000033\n",
            "reset\n",
            "Episode: 85, Total Reward: 0.0017840000000000035\n",
            "reset\n",
            "Episode: 86, Total Reward: 0.0017520000000000031\n",
            "reset\n",
            "Episode: 87, Total Reward: 0.001996000000000004\n",
            "reset\n",
            "Episode: 88, Total Reward: 0.0024160000000000045\n",
            "reset\n",
            "Episode: 89, Total Reward: 0.0029760000000000056\n",
            "reset\n",
            "Episode: 90, Total Reward: 0.002780000000000005\n",
            "reset\n",
            "Episode: 91, Total Reward: 0.003140000000000006\n",
            "reset\n",
            "Episode: 92, Total Reward: 0.003104000000000006\n",
            "reset\n",
            "Episode: 93, Total Reward: 0.002200000000000004\n",
            "reset\n",
            "Episode: 94, Total Reward: 0.0027240000000000055\n",
            "reset\n",
            "Episode: 95, Total Reward: 0.002200000000000004\n",
            "reset\n",
            "Episode: 96, Total Reward: 0.0020520000000000035\n",
            "reset\n",
            "Episode: 97, Total Reward: 0.0028400000000000057\n",
            "reset\n",
            "Episode: 98, Total Reward: 0.001964000000000004\n",
            "reset\n",
            "Episode: 99, Total Reward: 0.002596000000000005\n",
            "reset\n",
            "Episode: 100, Total Reward: 0.0024360000000000046\n",
            "reset\n",
            "Episode: 101, Total Reward: 0.002260000000000004\n",
            "reset\n",
            "Episode: 102, Total Reward: 0.0021920000000000043\n",
            "reset\n",
            "Episode: 103, Total Reward: 0.0022640000000000043\n",
            "reset\n",
            "Episode: 104, Total Reward: 0.0036440000000000075\n",
            "reset\n",
            "Episode: 105, Total Reward: 0.002552000000000005\n",
            "reset\n",
            "Episode: 106, Total Reward: 0.0023240000000000036\n",
            "reset\n",
            "Episode: 107, Total Reward: 0.002984000000000006\n",
            "reset\n",
            "Episode: 108, Total Reward: 0.0028320000000000055\n",
            "reset\n",
            "Episode: 109, Total Reward: 0.0034760000000000073\n",
            "reset\n",
            "Episode: 110, Total Reward: 0.0028880000000000064\n",
            "reset\n",
            "Episode: 111, Total Reward: 0.001596000000000003\n",
            "reset\n",
            "Episode: 112, Total Reward: 0.0023240000000000044\n",
            "reset\n",
            "Episode: 113, Total Reward: 0.0024080000000000043\n",
            "reset\n",
            "Episode: 114, Total Reward: 0.0022720000000000045\n",
            "reset\n",
            "Episode: 115, Total Reward: 0.002300000000000004\n",
            "reset\n",
            "Episode: 116, Total Reward: 0.0021200000000000034\n",
            "reset\n",
            "Episode: 117, Total Reward: 0.003372000000000007\n",
            "reset\n",
            "Episode: 118, Total Reward: 0.002208000000000004\n",
            "reset\n",
            "Episode: 119, Total Reward: 0.002792000000000005\n",
            "reset\n",
            "Episode: 120, Total Reward: 0.0020120000000000034\n",
            "reset\n",
            "Episode: 121, Total Reward: 0.0029560000000000055\n",
            "reset\n",
            "Episode: 122, Total Reward: 0.0026600000000000052\n",
            "reset\n",
            "Episode: 123, Total Reward: 0.003112000000000006\n",
            "reset\n",
            "Episode: 124, Total Reward: 0.003980000000000009\n",
            "reset\n",
            "Episode: 125, Total Reward: 0.003108000000000006\n",
            "reset\n",
            "Episode: 126, Total Reward: 0.0024160000000000045\n",
            "reset\n",
            "Episode: 127, Total Reward: 0.0026640000000000045\n",
            "reset\n",
            "Episode: 128, Total Reward: 0.0024680000000000045\n",
            "reset\n",
            "Episode: 129, Total Reward: 0.0022120000000000043\n",
            "reset\n",
            "Episode: 130, Total Reward: 0.002312000000000004\n",
            "reset\n",
            "Episode: 131, Total Reward: 0.0017680000000000026\n",
            "reset\n",
            "Episode: 132, Total Reward: 0.002176000000000003\n",
            "reset\n",
            "Episode: 133, Total Reward: 0.0017240000000000029\n",
            "reset\n",
            "Episode: 134, Total Reward: 0.002364000000000004\n",
            "reset\n",
            "Episode: 135, Total Reward: 0.002804000000000005\n",
            "reset\n",
            "Episode: 136, Total Reward: 0.0020320000000000034\n",
            "reset\n",
            "Episode: 137, Total Reward: 0.0031680000000000054\n",
            "reset\n",
            "Episode: 138, Total Reward: 0.0028080000000000054\n",
            "reset\n",
            "Episode: 139, Total Reward: 0.0016680000000000024\n",
            "reset\n",
            "Episode: 140, Total Reward: 0.002052000000000003\n",
            "reset\n",
            "Episode: 141, Total Reward: 0.0020400000000000027\n",
            "reset\n",
            "Episode: 142, Total Reward: 0.0028240000000000058\n",
            "reset\n",
            "Episode: 143, Total Reward: 0.0023680000000000046\n",
            "reset\n",
            "Episode: 144, Total Reward: 0.0028920000000000052\n",
            "reset\n",
            "Episode: 145, Total Reward: 0.0034360000000000068\n",
            "reset\n",
            "Episode: 146, Total Reward: 0.0027480000000000048\n",
            "reset\n",
            "Episode: 147, Total Reward: 0.002380000000000004\n",
            "reset\n",
            "Episode: 148, Total Reward: 0.0032120000000000065\n",
            "reset\n",
            "Episode: 149, Total Reward: 0.0026200000000000043\n",
            "reset\n",
            "Episode: 150, Total Reward: 0.0019200000000000033\n",
            "reset\n",
            "Episode: 151, Total Reward: 0.003320000000000006\n",
            "reset\n",
            "Episode: 152, Total Reward: 0.0015920000000000023\n",
            "reset\n",
            "Episode: 153, Total Reward: 0.0016160000000000029\n",
            "reset\n",
            "Episode: 154, Total Reward: 0.002312000000000004\n",
            "reset\n",
            "Episode: 155, Total Reward: 0.0024840000000000044\n",
            "reset\n",
            "Episode: 156, Total Reward: 0.003048000000000006\n",
            "reset\n",
            "Episode: 157, Total Reward: 0.0018680000000000033\n",
            "reset\n",
            "Episode: 158, Total Reward: 0.0016520000000000024\n",
            "reset\n",
            "Episode: 159, Total Reward: 0.0020800000000000037\n",
            "reset\n",
            "Episode: 160, Total Reward: 0.0031240000000000065\n",
            "reset\n",
            "Episode: 161, Total Reward: 0.00664000000000001\n",
            "reset\n",
            "Episode: 162, Total Reward: 0.005036000000000009\n",
            "reset\n",
            "Episode: 163, Total Reward: 0.002980000000000005\n",
            "reset\n",
            "Episode: 164, Total Reward: 0.006768000000000014\n",
            "reset\n",
            "Episode: 165, Total Reward: 0.0032440000000000064\n",
            "reset\n",
            "Episode: 166, Total Reward: 0.004592000000000009\n",
            "reset\n",
            "Episode: 167, Total Reward: 0.0030960000000000063\n",
            "reset\n",
            "Episode: 168, Total Reward: 0.0021360000000000038\n",
            "reset\n",
            "Episode: 169, Total Reward: 0.0019560000000000033\n",
            "reset\n",
            "Episode: 170, Total Reward: 0.001180000000000002\n",
            "reset\n",
            "Episode: 171, Total Reward: 0.0016040000000000026\n",
            "reset\n",
            "Episode: 172, Total Reward: 0.0026720000000000055\n",
            "reset\n",
            "Episode: 173, Total Reward: 0.00396800000000001\n",
            "reset\n",
            "Episode: 174, Total Reward: 0.00440800000000001\n",
            "reset\n",
            "Episode: 175, Total Reward: 0.00428800000000001\n",
            "reset\n",
            "Episode: 176, Total Reward: 0.0035560000000000075\n",
            "reset\n",
            "Episode: 177, Total Reward: 0.003328000000000007\n",
            "reset\n"
          ]
        }
      ],
      "source": [
        "#training loop\n",
        "results = []\n",
        "for episode in range(episodes):\n",
        "    state = env.get_state(reset=True)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(max_step):\n",
        "        action = select_action(state)\n",
        "        observation, reward = env.step(action)\n",
        "# '''\n",
        "#         done = terminated or truncated\n",
        "#         if terminated:\n",
        "#             next_state = None\n",
        "#         else:\n",
        "#             next_state = observation\n",
        "# '''\n",
        "        next_state = observation\n",
        "\n",
        "\n",
        "        replay_buffer.push(state, action, reward, next_state)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        train()\n",
        "\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        target_net_state_dict = q_network.state_dict()\n",
        "        policy_net_state_dict = target_network.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_network.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        # if terminated or truncated:\n",
        "        #     break\n",
        "\n",
        "\n",
        "    results.append(total_reward)\n",
        "    print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "    ## for debug - train() - just get some replay memory.\n",
        "    # if episode == 3:\n",
        "    #     print('break')\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-IRAV9y-bjZ"
      },
      "outputs": [],
      "source": [
        "# states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "# print(np.array(states).shape)\n",
        "# print(np.array(actions).shape)\n",
        "# print(np.array(rewards).shape)\n",
        "# print(np.array(next_states).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtE0pfB9-bjZ"
      },
      "outputs": [],
      "source": [
        "  # states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "\n",
        "  # # # Compute a mask of non-final states and concatenate the batch elements\n",
        "  # # non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "  # #                                       next_states)))\n",
        "  # # non_final_next_states = torch.tensor(np.array([s for s in next_states\n",
        "  # #                                             if s is not None]))\n",
        "  # # #\n",
        "  # rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "  # actions = torch.tensor(np.array(actions)).unsqueeze(1)\n",
        "  # states = torch.tensor(np.array(states)).squeeze(1)\n",
        "  # next_states = torch.tensor(np.array(next_states)).squeeze(1)\n",
        "\n",
        "  # # # Compute current Q values\n",
        "  # q_values = q_network(states).gather(1, actions)\n",
        "\n",
        "\n",
        "\n",
        "  #     # Compute target Q values\n",
        "  # with torch.no_grad():\n",
        "  #     next_q_values = target_network(next_states).max(1).values.unsqueeze(1)\n",
        "  # targets = rewards + (gamma * next_q_values)\n",
        "\n",
        "  # #     # Update the network\n",
        "  # loss = loss_fn(q_values, targets)\n",
        "  # optimizer.zero_grad()\n",
        "  # loss.backward()\n",
        "  # optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9-6qrqH-bjZ"
      },
      "outputs": [],
      "source": [
        "# # rewards\n",
        "# # actions.shape\n",
        "# # states.shape\n",
        "# # next_states.shape\n",
        "# q_values.shape\n",
        "# targets.shape\n",
        "# next_q_values.unsqueeze(1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4b8AWyg-bjZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}