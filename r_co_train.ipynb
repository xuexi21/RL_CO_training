{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuexi21/RL_CO_training/blob/main/r_co_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCeWC9rX-bjU"
      },
      "source": [
        "<!-- ## prepare the data -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n4kS8vKR-bjV"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "from sklearn.datasets import make_moons as moon\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# define the classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e7oFEh4o-bjW"
      },
      "outputs": [],
      "source": [
        "# set the dataset.\n",
        "dataset = moon(5000, noise=0.3, random_state=42)\n",
        "X,y = dataset\n",
        "\n",
        "# split the training(labeled) as 10% of dataset\n",
        "X_l, X_ul, y_l, y_ul = train_test_split(X, y, test_size=0.8, random_state=0)\n",
        "\n",
        "\n",
        "# split the training(labeled) as 50% of  labeled dataset\n",
        "X_l_train, X_l_test, y_l_train, y_l_test = train_test_split(X_l, y_l, test_size=0.5, random_state=0)\n",
        "\n",
        "# 2-classifier\n",
        "clf_1 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"knn\", KNeighborsClassifier(n_neighbors=11))\n",
        "        ]\n",
        ")\n",
        "\n",
        "# clf 1\n",
        "clf_2 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"RF\", RandomForestClassifier())\n",
        "        ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AJqps6K9-bjW"
      },
      "outputs": [],
      "source": [
        "# define ENV\n",
        "\n",
        "# for clustering the unlabeld data\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Env():\n",
        "    def __init__(self, classifier_1, classifier_2, input_ul_data, k, X_test, y_test, X_reset, y_reset):\n",
        "        # super().__init__\n",
        "        self.model_1 = classifier_1\n",
        "        self.model_2 = classifier_2\n",
        "        # UN LABEL DATA\n",
        "        self.X_ul = input_ul_data\n",
        "        # define the evaluate data, later use for the reward\n",
        "        self.X_eval = X_test\n",
        "        self.y_eval = y_test\n",
        "        self.X_reset = X_reset\n",
        "        self.y_reset = y_reset\n",
        "        # cluster the data\n",
        "        self.action_size = k\n",
        "        self.kmeans = KMeans(n_clusters=k,  n_init=10)\n",
        "        self.cluster_label = self.kmeans.fit_predict(self.X_ul)\n",
        "        self.u_cluster_label = np.unique(self.cluster_label)\n",
        "        self.centroids = self.kmeans.cluster_centers_\n",
        "        self.observation_size = self.get_state(reset=True).shape[1]\n",
        "        self.prev_macro_f1 = 0.0\n",
        "\n",
        "    # def cluster_plot(self):\n",
        "    #     for i in self.u_cluster_label:\n",
        "    #         plt.scatter(self.X_ul[self.cluster_label == i , 0] ,\n",
        "    #                     self.X_ul[self.cluster_label == i , 1] ,\n",
        "    #                     label = i)\n",
        "    #     plt.scatter(self.centroids[:,0],\n",
        "    #                 self.centroids[:,1],\n",
        "    #                 s=80,\n",
        "    #                 color='k')\n",
        "    #     # plt.legend()\n",
        "    #     plt.title(f'{self.k} cluster (centroids) of unlabeled data')\n",
        "    #     plt.show()\n",
        "\n",
        "    # update 2 clf\n",
        "    def train_2_clf(self, X, y):\n",
        "        self.model_1.fit(X, y)\n",
        "        self.model_2.fit(X, y)\n",
        "\n",
        "    def get_state(self,reset=False):\n",
        "        np.random.seed(123)\n",
        "        if reset:\n",
        "            self.train_2_clf(self.X_reset, self.y_reset)\n",
        "            print(\"reset\")\n",
        "        out_1 = self.model_1.predict_proba(self.centroids)\n",
        "        out_2 = self.model_2.predict_proba(self.centroids)\n",
        "        state_proba = np.concatenate((out_1, out_2), axis=1)\n",
        "        return  torch.from_numpy(state_proba).to(torch.float32).reshape(1, -1)\n",
        "\n",
        "\n",
        "    # def get_acc(self):\n",
        "    #     pred_1 = self.model_1.predict(self.X_eval)\n",
        "    #     pred_2 = self.model_1.predict(self.X_eval)\n",
        "    #     acc_1 = accuracy_score(pred_1, self.y_eval)\n",
        "    #     acc_2 = accuracy_score(pred_2, self.y_eval)\n",
        "    #     return acc_1, acc_2\n",
        "    def get_f1(self):\n",
        "        classifier_weights = [clf.score(self.X_eval, self.y_eval) for clf in [self.model_1, self.model_2]]  # Weights based on validation accuracy\n",
        "        combined_probabilities = np.average(\n",
        "            [clf.predict_proba(self.X_eval) for clf in [self.model_1, self.model_2]],\n",
        "            axis=0,\n",
        "            weights=classifier_weights\n",
        "        )\n",
        "\n",
        "        # Get final predictions from combined probabilities\n",
        "        combined_predictions = np.argmax(combined_probabilities, axis=1)\n",
        "\n",
        "\n",
        "        # Calculate F1 scores per class (harmonic means)\n",
        "        precision, recall, f1_per_class, _ = precision_recall_fscore_support(self.y_eval, combined_predictions, average=None)\n",
        "\n",
        "        # Compute Macro-F1 as arithmetic mean of F1 scores\n",
        "        macro_f1 = np.mean(f1_per_class)\n",
        "\n",
        "        return macro_f1\n",
        "\n",
        "    # def get_acc(self):\n",
        "    #     pred_1 = self.model_1.predict(self.X_eval)\n",
        "    #     pred_2 = self.model_1.predict(self.X_eval)\n",
        "    #     acc_1 = accuracy_score(pred_1, self.y_eval)\n",
        "    #     acc_2 = accuracy_score(pred_2, self.y_eval)\n",
        "    #     return acc_1, acc_2\n",
        "\n",
        "    ######\n",
        "    ######\n",
        "    def get_subset(self, action):\n",
        "        # choose subset\n",
        "        subset = self.X_ul[self.cluster_label == action]\n",
        "        return subset\n",
        "\n",
        "    def co_training(self, subset):\n",
        "        ## get posodu label\n",
        "        clf_0_p_label = self.model_1.predict(subset)\n",
        "        clf_1_p_label = self.model_2.predict(subset)\n",
        "\n",
        "        ## get proba_\n",
        "        clf_0_p_y = self.model_1.predict_proba(subset)\n",
        "        clf_1_p_y = self.model_2.predict_proba(subset)\n",
        "\n",
        "        #get the label size\n",
        "        y_num = subset.shape[0]\n",
        "        # set empty y  #type=ndarray\n",
        "        y_ul_action = np.zeros(y_num,)\n",
        "\n",
        "        #############\n",
        "        # confidence_diff = 0\n",
        "        # combine the lable from two classifier, choose the most conffidence\n",
        "        for i in range(y_num):\n",
        "            if max(clf_0_p_y[i, ]) > max(clf_1_p_y[i, ]):\n",
        "                y_ul_action[i] = clf_0_p_label[i]\n",
        "                # print('0')\n",
        "            else:\n",
        "                y_ul_action[i] = clf_1_p_label[i]\n",
        "                # print('1')\n",
        "\n",
        "        ########### update the label_set for traning\n",
        "        X_updated = np.concatenate((X_l_train, subset), axis=0)\n",
        "        y_updated = np.concatenate((y_l_train, y_ul_action), axis=0)\n",
        "\n",
        "        # print(f'X shape is {X_updated.shape} \\ny shape is {y_updated.shape}')\n",
        "\n",
        "        ############# use the updated labeld dateset retrain those 2 classifier\n",
        "        self.train_2_clf(X_updated, y_updated)\n",
        "\n",
        "        # RETURN THE co-trained CLASSIFIER'S mean marcof1.\n",
        "        marco_f1 = self.get_f1()\n",
        "        return marco_f1\n",
        "\n",
        "    def step(self, action):\n",
        "        # GET THE bigining state accuracy, later use to calculate the reward\n",
        "        pre_marco_f1 = self.get_f1()\n",
        "\n",
        "        # choose subset\n",
        "        choosen_subset =  self.get_subset(action)\n",
        "\n",
        "        # cotraining the 2 classifier\n",
        "        new_marco_f1 = self.co_training(choosen_subset)\n",
        "\n",
        "        # get the next state_\n",
        "        n_state = self.get_state()\n",
        "\n",
        "        ##############\n",
        "        # calculate the reward\n",
        "        ##############\n",
        "        if new_marco_f1 > pre_marco_f1:\n",
        "            reward_0 = new_marco_f1 - pre_marco_f1\n",
        "        else:\n",
        "            reward_0 = 0\n",
        "\n",
        "        return n_state, reward_0, new_marco_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_AkP5ovW-bjX",
        "outputId": "eba85be5-304f-46ad-fe3c-a60b31227b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "reset\n"
          ]
        }
      ],
      "source": [
        "k = 20\n",
        "env = Env(clf_1, clf_2, input_ul_data=X_ul, k=k, X_test=X_l_test, y_test=y_l_test, X_reset=X_l_train, y_reset=y_l_train)\n",
        "state_0 = env.get_state(reset=True)\n",
        "# env.cluster_plot()\n",
        "# env.get_acc()\n",
        "# env.get_subset(2)\n",
        "state, reward, marco_f1 = env.step(19)\n",
        "# state.shape\n",
        "# reward\n",
        "\n",
        "# print(state_0)\n",
        "# print(state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuqYSBf7-bjX"
      },
      "source": [
        "<!-- #### DQN\n",
        "\n",
        "<img src=\"https://yinyoupoet.github.io/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_8coZ4g_pRtfyoHmsuzMH6g.png\" alt=\"Description of the image\" width=\"400\" height=\"300\">\n",
        "\n",
        "##### loss\n",
        "\n",
        "\n",
        "<img src=\"https://yinyoupoet.github.io/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_YCgMUijhU4p_y3sctvu-kQ.png\" alt=\"Description of the image\" width=\"300\" height=\"50\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vm4jSekd-bjX"
      },
      "outputs": [],
      "source": [
        "## RL functions##\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the Q-network (a simple feedforward neural network)\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        # # bug - some times the state is not\n",
        "        # if len(state) != 4:\n",
        "        #     state = state[0]\n",
        "        experience_tuple = (state, action, reward, next_state)\n",
        "        # Append experience_tuple to the memory buffer\n",
        "        self.memory.append(experience_tuple)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Draw a random sample of size batch_size\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        # Transform batch into a tuple of lists\n",
        "        states, actions, rewards, next_states = (zip(*batch))\n",
        "        return states, actions, rewards, next_states\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5XYUqe9H-bjY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "#\n",
        "EPS_START = 1\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "steps_done = 0\n",
        "\n",
        "####\n",
        "buffer_size = 10000\n",
        "###\n",
        "episodes = 500\n",
        "max_step = 100\n",
        "batch_size = 64\n",
        "TAU = 0.005\n",
        "gamma = 0.99\n",
        "\n",
        "# hyper parameter\n",
        "observation_size = env.observation_size\n",
        "action_size = env.action_size\n",
        "lr = 1e-4\n",
        "\n",
        "###\n",
        "# Initialize networks and optimizer\n",
        "q_network = QNetwork(observation_size, action_size)\n",
        "target_network = QNetwork(observation_size, action_size)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "# Replay memory\n",
        "replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return the largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            action = torch.argmax(q_network(state)).item()\n",
        "            return action\n",
        "    else:\n",
        "        return np.random.choice(range(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FSFEIzc7-bjY"
      },
      "outputs": [],
      "source": [
        "# Function to update the Q-network\n",
        "def train():\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "        # prepare the training data\n",
        "    states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "\n",
        "    rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "    actions = torch.tensor(np.array(actions)).unsqueeze(1)\n",
        "    states = torch.tensor(np.array(states)).squeeze(1)\n",
        "    next_states = torch.tensor(np.array(next_states)).squeeze(1)\n",
        "\n",
        "    # # Compute current Q values\n",
        "    q_values = q_network(states).gather(1, actions)\n",
        "\n",
        "\n",
        "\n",
        "        # Compute target Q values\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_network(next_states).max(1).values.unsqueeze(1)\n",
        "    targets = rewards + (gamma * next_q_values)\n",
        "\n",
        "\n",
        "        # Update the network\n",
        "    loss = loss_fn(q_values, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_haF2H6b-bjZ",
        "outputId": "2a9dbc31-8d98-4317-d09c-44d2104d87db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "Episode: 0, marco_f1: 0.9098958395905666, Total Reward: 0.08175039505544512\n",
            "reset\n",
            "Episode: 1, marco_f1: 0.9098958395905666, Total Reward: 0.04193650390629322\n",
            "reset\n",
            "Episode: 2, marco_f1: 0.9098958395905666, Total Reward: 0.06581654933899339\n",
            "reset\n",
            "Episode: 3, marco_f1: 0.9039246769467262, Total Reward: 0.11955701313355749\n",
            "reset\n",
            "Episode: 4, marco_f1: 0.9098958395905666, Total Reward: 0.13747050106507885\n",
            "reset\n",
            "Episode: 5, marco_f1: 0.9098958395905666, Total Reward: 0.17926863957196204\n",
            "reset\n",
            "Episode: 6, marco_f1: 0.9039246769467262, Total Reward: 0.20315329014732386\n",
            "reset\n",
            "Episode: 7, marco_f1: 0.9098958395905666, Total Reward: 0.1852398022158025\n",
            "reset\n",
            "Episode: 8, marco_f1: 0.9079278154072793, Total Reward: 0.18727491649306827\n",
            "reset\n",
            "Episode: 9, marco_f1: 0.9039246769467262, Total Reward: 0.22106677807884523\n",
            "reset\n",
            "Episode: 10, marco_f1: 0.9098958395905666, Total Reward: 0.22106677807884523\n",
            "reset\n",
            "Episode: 11, marco_f1: 0.9039246769467262, Total Reward: 0.2509225912980475\n",
            "reset\n",
            "Episode: 12, marco_f1: 0.9098958395905666, Total Reward: 0.24495142865420705\n",
            "reset\n",
            "Episode: 13, marco_f1: 0.9098958395905666, Total Reward: 0.23300910336652614\n",
            "reset\n",
            "Episode: 14, marco_f1: 0.9039246769467262, Total Reward: 0.2389802660103666\n",
            "reset\n",
            "Episode: 15, marco_f1: 0.9098958395905666, Total Reward: 0.2668680550462815\n",
            "reset\n",
            "Episode: 16, marco_f1: 0.9098958395905666, Total Reward: 0.25689375394188796\n",
            "reset\n",
            "Episode: 17, marco_f1: 0.9039246769467262, Total Reward: 0.2509225912980475\n",
            "reset\n",
            "Episode: 18, marco_f1: 0.9039246769467262, Total Reward: 0.2748072418734093\n",
            "reset\n",
            "Episode: 19, marco_f1: 0.9098958395905666, Total Reward: 0.2668009649523031\n",
            "reset\n",
            "Episode: 20, marco_f1: 0.9098958395905666, Total Reward: 0.26279782649175\n",
            "reset\n",
            "Episode: 21, marco_f1: 0.9098958395905666, Total Reward: 0.26483294076901576\n",
            "reset\n",
            "Episode: 22, marco_f1: 0.9098958395905666, Total Reward: 0.26279782649175\n",
            "reset\n",
            "Episode: 23, marco_f1: 0.9098958395905666, Total Reward: 0.252756435293378\n",
            "reset\n",
            "Episode: 24, marco_f1: 0.9039246769467262, Total Reward: 0.27677526605669667\n",
            "reset\n",
            "Episode: 25, marco_f1: 0.9098958395905666, Total Reward: 0.2748072418734093\n",
            "reset\n",
            "Episode: 26, marco_f1: 0.9039246769467262, Total Reward: 0.2668009649523031\n",
            "reset\n",
            "Episode: 27, marco_f1: 0.9098958395905666, Total Reward: 0.2708757986494963\n",
            "reset\n",
            "Episode: 28, marco_f1: 0.9039246769467262, Total Reward: 0.26273073639777156\n",
            "reset\n",
            "Episode: 29, marco_f1: 0.9098958395905666, Total Reward: 0.28878928658101766\n",
            "reset\n",
            "Episode: 30, marco_f1: 0.9039246769467262, Total Reward: 0.2828181239371772\n",
            "reset\n",
            "Episode: 31, marco_f1: 0.9079278154072793, Total Reward: 0.25099889167734923\n",
            "reset\n",
            "Episode: 32, marco_f1: 0.9079278154072793, Total Reward: 0.2510914152310949\n",
            "reset\n",
            "Episode: 33, marco_f1: 0.9039246769467262, Total Reward: 0.2331108372055951\n",
            "reset\n",
            "Episode: 34, marco_f1: 0.9098958395905666, Total Reward: 0.21541484273045297\n",
            "reset\n",
            "Episode: 35, marco_f1: 0.9079278154072793, Total Reward: 0.26082739456200366\n",
            "reset\n",
            "Episode: 36, marco_f1: 0.9039246769467262, Total Reward: 0.2988028365731634\n",
            "reset\n",
            "Episode: 37, marco_f1: 0.9098958395905666, Total Reward: 0.23515055662552253\n",
            "reset\n",
            "Episode: 38, marco_f1: 0.9039246769467262, Total Reward: 0.20540349013450987\n",
            "reset\n",
            "Episode: 39, marco_f1: 0.9039246769467262, Total Reward: 0.17543432504445633\n",
            "reset\n",
            "Episode: 40, marco_f1: 0.9079278154072793, Total Reward: 0.20145121859349124\n",
            "reset\n",
            "Episode: 41, marco_f1: 0.9079278154072793, Total Reward: 0.20332671922303291\n",
            "reset\n",
            "Episode: 42, marco_f1: 0.9039246769467262, Total Reward: 0.17147744836077605\n",
            "reset\n",
            "Episode: 43, marco_f1: 0.9079278154072793, Total Reward: 0.17740234922774367\n",
            "reset\n",
            "Episode: 44, marco_f1: 0.9079278154072793, Total Reward: 0.2114463480149904\n",
            "reset\n",
            "Episode: 45, marco_f1: 0.9039246769467262, Total Reward: 0.22131190239119436\n",
            "reset\n",
            "Episode: 46, marco_f1: 0.9079278154072793, Total Reward: 0.22542378757993708\n",
            "reset\n",
            "Episode: 47, marco_f1: 0.9079278154072793, Total Reward: 0.21934387820790702\n",
            "reset\n",
            "Episode: 48, marco_f1: 0.9098958395905666, Total Reward: 0.2013170384055344\n",
            "reset\n",
            "Episode: 49, marco_f1: 0.9098958395905666, Total Reward: 0.2470558304216539\n",
            "reset\n",
            "Episode: 50, marco_f1: 0.9098958395905666, Total Reward: 0.25897732739222923\n",
            "reset\n",
            "Episode: 51, marco_f1: 0.9079278154072793, Total Reward: 0.24112632441202464\n",
            "reset\n",
            "Episode: 52, marco_f1: 0.8999423668032787, Total Reward: 0.23126077003582068\n",
            "reset\n",
            "Episode: 53, marco_f1: 0.9079278154072793, Total Reward: 0.2550158455658873\n",
            "reset\n",
            "Episode: 54, marco_f1: 0.8999423668032787, Total Reward: 0.294788080080828\n",
            "reset\n",
            "Episode: 55, marco_f1: 0.9079278154072793, Total Reward: 0.3147412874322768\n",
            "reset\n",
            "Episode: 56, marco_f1: 0.9098958395905666, Total Reward: 0.3405314772396092\n",
            "reset\n",
            "Episode: 57, marco_f1: 0.8999423668032787, Total Reward: 0.28680723661948904\n",
            "reset\n",
            "Episode: 58, marco_f1: 0.9098958395905666, Total Reward: 0.3446063109368024\n",
            "reset\n",
            "Episode: 59, marco_f1: 0.9098958395905666, Total Reward: 0.3625244040109854\n",
            "reset\n",
            "Episode: 60, marco_f1: 0.9098958395905666, Total Reward: 0.4201684698232363\n",
            "reset\n",
            "Episode: 61, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 62, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 63, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 64, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 65, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 66, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 67, marco_f1: 0.9098958395905666, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 68, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 69, marco_f1: 0.9098958395905666, Total Reward: 0.4997962521215401\n",
            "reset\n",
            "Episode: 70, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 71, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 72, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 73, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 74, marco_f1: 0.9098958395905666, Total Reward: 0.4997962521215401\n",
            "reset\n",
            "Episode: 75, marco_f1: 0.9098958395905666, Total Reward: 0.4699358337596762\n",
            "reset\n",
            "Episode: 76, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 77, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 78, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 79, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 80, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 81, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 82, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 83, marco_f1: 0.9098958395905666, Total Reward: 0.4997962521215401\n",
            "reset\n",
            "Episode: 84, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 85, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 86, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 87, marco_f1: 0.9098958395905666, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 88, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 89, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 90, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 91, marco_f1: 0.9098958395905666, Total Reward: 0.4997962521215401\n",
            "reset\n",
            "Episode: 92, marco_f1: 0.9098958395905666, Total Reward: 0.4699358337596762\n",
            "reset\n",
            "Episode: 93, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 94, marco_f1: 0.9098958395905666, Total Reward: 0.4699358337596762\n",
            "reset\n",
            "Episode: 95, marco_f1: 0.9098958395905666, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 96, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 97, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 98, marco_f1: 0.9098958395905666, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 99, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 100, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 101, marco_f1: 0.9098958395905666, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 102, marco_f1: 0.9098958395905666, Total Reward: 0.4699358337596762\n",
            "reset\n",
            "Episode: 103, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 104, marco_f1: 0.9098958395905666, Total Reward: 0.4997962521215401\n",
            "reset\n",
            "Episode: 105, marco_f1: 0.9098958395905666, Total Reward: 0.4997962521215401\n",
            "reset\n",
            "Episode: 106, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 107, marco_f1: 0.8999423668032787, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 108, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 109, marco_f1: 0.9098958395905666, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 110, marco_f1: 0.8999423668032787, Total Reward: 0.4699358337596762\n",
            "reset\n",
            "Episode: 111, marco_f1: 0.9098958395905666, Total Reward: 0.4997962521215401\n",
            "reset\n",
            "Episode: 112, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 113, marco_f1: 0.9098958395905666, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 114, marco_f1: 0.9098958395905666, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 115, marco_f1: 0.9098958395905666, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 116, marco_f1: 0.8999423668032787, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 117, marco_f1: 0.9098958395905666, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 118, marco_f1: 0.8999423668032787, Total Reward: 0.47590699640351664\n",
            "reset\n",
            "Episode: 119, marco_f1: 0.8999423668032787, Total Reward: 0.4440947770328093\n",
            "reset\n",
            "Episode: 120, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 121, marco_f1: 0.8999423668032787, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 122, marco_f1: 0.8999423668032787, Total Reward: 0.4699358337596762\n",
            "reset\n",
            "Episode: 123, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 124, marco_f1: 0.9098958395905666, Total Reward: 0.4997962521215401\n",
            "reset\n",
            "Episode: 125, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 126, marco_f1: 0.8999423668032787, Total Reward: 0.47590699640351664\n",
            "reset\n",
            "Episode: 127, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 128, marco_f1: 0.8999423668032787, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 129, marco_f1: 0.9098958395905666, Total Reward: 0.4461553247698423\n",
            "reset\n",
            "Episode: 130, marco_f1: 0.8999423668032787, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 131, marco_f1: 0.9098958395905666, Total Reward: 0.4958139419780926\n",
            "reset\n",
            "Episode: 132, marco_f1: 0.9098958395905666, Total Reward: 0.48984277933425213\n",
            "reset\n",
            "Episode: 133, marco_f1: 0.9098958395905666, Total Reward: 0.4958139419780926\n",
            "reset\n",
            "Episode: 134, marco_f1: 0.8999423668032787, Total Reward: 0.47590699640351664\n",
            "reset\n",
            "Episode: 135, marco_f1: 0.8999423668032787, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 136, marco_f1: 0.9098958395905666, Total Reward: 0.47590699640351664\n",
            "reset\n",
            "Episode: 137, marco_f1: 0.8999423668032787, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 138, marco_f1: 0.8999423668032787, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 139, marco_f1: 0.9098958395905666, Total Reward: 0.4600031892894938\n",
            "reset\n",
            "Episode: 140, marco_f1: 0.9039246769467262, Total Reward: 0.27298941070226646\n",
            "reset\n",
            "Episode: 141, marco_f1: 0.8999423668032787, Total Reward: 0.4381698761658417\n",
            "reset\n",
            "Episode: 142, marco_f1: 0.9098958395905666, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 143, marco_f1: 0.9098958395905666, Total Reward: 0.47988930654696416\n",
            "reset\n",
            "Episode: 144, marco_f1: 0.8999423668032787, Total Reward: 0.4958139419780926\n",
            "reset\n",
            "Episode: 145, marco_f1: 0.8999423668032787, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 146, marco_f1: 0.9098958395905666, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 147, marco_f1: 0.8999423668032787, Total Reward: 0.4958139419780926\n",
            "reset\n",
            "Episode: 148, marco_f1: 0.8999423668032787, Total Reward: 0.47590699640351664\n",
            "reset\n",
            "Episode: 149, marco_f1: 0.9098958395905666, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 150, marco_f1: 0.9098958395905666, Total Reward: 0.4958139419780926\n",
            "reset\n",
            "Episode: 151, marco_f1: 0.8999423668032787, Total Reward: 0.4858604691908046\n",
            "reset\n",
            "Episode: 152, marco_f1: 0.8999423668032787, Total Reward: 0.4958139419780926\n",
            "reset\n",
            "Episode: 153, marco_f1: 0.9098958395905666, Total Reward: 0.4958139419780926\n",
            "reset\n",
            "Episode: 154, marco_f1: 0.8999423668032787, Total Reward: 0.47590699640351664\n",
            "reset\n"
          ]
        }
      ],
      "source": [
        "#training loop\n",
        "results = []\n",
        "for episode in range(episodes):\n",
        "    state = env.get_state(reset=True)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(max_step):\n",
        "        action = select_action(state)\n",
        "        observation, reward, marco_f1 = env.step(action)\n",
        "# '''\n",
        "#         done = terminated or truncated\n",
        "#         if terminated:\n",
        "#             next_state = None\n",
        "#         else:\n",
        "#             next_state = observation\n",
        "# '''\n",
        "        next_state = observation\n",
        "\n",
        "\n",
        "        replay_buffer.push(state, action, reward, next_state)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        train()\n",
        "\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        target_net_state_dict = q_network.state_dict()\n",
        "        policy_net_state_dict = target_network.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_network.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        # if terminated or truncated:\n",
        "        #     break\n",
        "\n",
        "\n",
        "    results.append((episode, total_reward, marco_f1))\n",
        "    print(f\"Episode: {episode}, marco_f1: {marco_f1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    ## for debug - train() - just get some replay memory.\n",
        "    # if episode == 3:\n",
        "    #     print('break')\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4b8AWyg-bjZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftfsjOaodbE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6KS3-T8dbCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}