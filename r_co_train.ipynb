{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuexi21/RL_CO_training/blob/main/r_co_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCeWC9rX-bjU"
      },
      "source": [
        "<!-- ## prepare the data -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4kS8vKR-bjV"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.datasets import make_moons as moon\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# define the classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7oFEh4o-bjW"
      },
      "outputs": [],
      "source": [
        "# set the dataset.\n",
        "dataset = moon(5000, noise=0.3, random_state=42)\n",
        "X,y = dataset\n",
        "\n",
        "# split the training(labeled) as 10% of dataset\n",
        "X_l, X_ul, y_l, y_ul = train_test_split(X, y, test_size=0.8, random_state=0)\n",
        "\n",
        "\n",
        "# split the training(labeled) as 50% of  labeled dataset\n",
        "X_l_train, X_l_test, y_l_train, y_l_test = train_test_split(X_l, y_l, test_size=0.5, random_state=0)\n",
        "\n",
        "# 2-classifier\n",
        "clf_1 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"knn\", KNeighborsClassifier(n_neighbors=11))\n",
        "        ]\n",
        ")\n",
        "\n",
        "# clf 1\n",
        "clf_2 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"RF\", RandomForestClassifier())\n",
        "        ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJqps6K9-bjW"
      },
      "outputs": [],
      "source": [
        "# define ENV\n",
        "\n",
        "# for clustering the unlabeld data\n",
        "\n",
        "\n",
        "\n",
        "class Env():\n",
        "    def __init__(self, classifier_1, classifier_2, input_ul_data, k, X_test, y_test, X_reset, y_reset):\n",
        "        # super().__init__\n",
        "        self.model_1 = classifier_1\n",
        "        self.model_2 = classifier_2\n",
        "        # UN LABEL DATA\n",
        "        self.X_ul = input_ul_data\n",
        "        # define the evaluate data, later use for the reward\n",
        "        self.X_eval = X_test\n",
        "        self.y_eval = y_test\n",
        "        self.X_reset = X_reset\n",
        "        self.y_reset = y_reset\n",
        "        # cluster the data\n",
        "        self.action_size = k\n",
        "        self.kmeans = KMeans(n_clusters=k,  n_init=10)\n",
        "        self.cluster_label = self.kmeans.fit_predict(self.X_ul)\n",
        "        self.u_cluster_label = np.unique(self.cluster_label)\n",
        "        self.centroids = self.kmeans.cluster_centers_\n",
        "        self.observation_size = self.get_state(reset=True).shape[1]\n",
        "        self.prev_macro_f1 = 0.0\n",
        "\n",
        "    # def cluster_plot(self):\n",
        "    #     for i in self.u_cluster_label:\n",
        "    #         plt.scatter(self.X_ul[self.cluster_label == i , 0] ,\n",
        "    #                     self.X_ul[self.cluster_label == i , 1] ,\n",
        "    #                     label = i)\n",
        "    #     plt.scatter(self.centroids[:,0],\n",
        "    #                 self.centroids[:,1],\n",
        "    #                 s=80,\n",
        "    #                 color='k')\n",
        "    #     # plt.legend()\n",
        "    #     plt.title(f'{self.k} cluster (centroids) of unlabeled data')\n",
        "    #     plt.show()\n",
        "\n",
        "    # update 2 clf\n",
        "    def train_2_clf(self, X, y):\n",
        "        self.model_1.fit(X, y)\n",
        "        self.model_2.fit(X, y)\n",
        "\n",
        "    def get_state(self,reset=False):\n",
        "        np.random.seed(123)\n",
        "        if reset:\n",
        "            self.train_2_clf(self.X_reset, self.y_reset)\n",
        "            print(\"reset\")\n",
        "        out_1 = self.model_1.predict_proba(self.centroids)\n",
        "        out_2 = self.model_2.predict_proba(self.centroids)\n",
        "        state_proba = np.concatenate((out_1, out_2), axis=1)\n",
        "        return  torch.from_numpy(state_proba).to(torch.float32).reshape(1, -1)\n",
        "\n",
        "\n",
        "    # def get_acc(self):\n",
        "    #     pred_1 = self.model_1.predict(self.X_eval)\n",
        "    #     pred_2 = self.model_1.predict(self.X_eval)\n",
        "    #     acc_1 = accuracy_score(pred_1, self.y_eval)\n",
        "    #     acc_2 = accuracy_score(pred_2, self.y_eval)\n",
        "    #     return acc_1, acc_2\n",
        "    def get_f1(self):\n",
        "        classifier_weights = [clf.score(self.X_eval, self.y_eval) for clf in [self.model_1, self.model_2]]  # Weights based on validation accuracy\n",
        "        combined_probabilities = np.average(\n",
        "            [clf.predict_proba(self.X_eval) for clf in [self.model_1, self.model_2]],\n",
        "            axis=0,\n",
        "            weights=classifier_weights\n",
        "        )\n",
        "\n",
        "        # Get final predictions from combined probabilities\n",
        "        combined_predictions = np.argmax(combined_probabilities, axis=1)\n",
        "\n",
        "\n",
        "        # Calculate F1 scores per class (harmonic means)\n",
        "        precision, recall, f1_per_class, _ = precision_recall_fscore_support(self.y_eval, combined_predictions, average=None)\n",
        "\n",
        "        # Compute Macro-F1 as arithmetic mean of F1 scores\n",
        "        macro_f1 = np.mean(f1_per_class)\n",
        "\n",
        "        return macro_f1\n",
        "\n",
        "    # def get_acc(self):\n",
        "    #     pred_1 = self.model_1.predict(self.X_eval)\n",
        "    #     pred_2 = self.model_1.predict(self.X_eval)\n",
        "    #     acc_1 = accuracy_score(pred_1, self.y_eval)\n",
        "    #     acc_2 = accuracy_score(pred_2, self.y_eval)\n",
        "    #     return acc_1, acc_2\n",
        "\n",
        "    ######\n",
        "    ######\n",
        "    def get_subset(self, action):\n",
        "        # choose subset\n",
        "        subset = self.X_ul[self.cluster_label == action]\n",
        "        return subset\n",
        "\n",
        "    def co_training(self, subset):\n",
        "        ## get posodu label\n",
        "        clf_0_p_label = self.model_1.predict(subset)\n",
        "        clf_1_p_label = self.model_2.predict(subset)\n",
        "\n",
        "        ## get proba_\n",
        "        clf_0_p_y = self.model_1.predict_proba(subset)\n",
        "        clf_1_p_y = self.model_2.predict_proba(subset)\n",
        "\n",
        "        #get the label size\n",
        "        y_num = subset.shape[0]\n",
        "        # set empty y  #type=ndarray\n",
        "        y_ul_action = np.zeros(y_num,)\n",
        "\n",
        "        #############\n",
        "        # confidence_diff = 0\n",
        "        # combine the lable from two classifier, choose the most conffidence\n",
        "        for i in range(y_num):\n",
        "            if max(clf_0_p_y[i, ]) > max(clf_1_p_y[i, ]):\n",
        "                y_ul_action[i] = clf_0_p_label[i]\n",
        "                # print('0')\n",
        "            else:\n",
        "                y_ul_action[i] = clf_1_p_label[i]\n",
        "                # print('1')\n",
        "\n",
        "        ########### update the label_set for traning\n",
        "        X_updated = np.concatenate((X_l_train, subset), axis=0)\n",
        "        y_updated = np.concatenate((y_l_train, y_ul_action), axis=0)\n",
        "\n",
        "        # print(f'X shape is {X_updated.shape} \\ny shape is {y_updated.shape}')\n",
        "\n",
        "        ############# use the updated labeld dateset retrain those 2 classifier\n",
        "        self.train_2_clf(X_updated, y_updated)\n",
        "\n",
        "        # RETURN THE co-trained CLASSIFIER'S mean marcof1.\n",
        "        marco_f1 = self.get_f1()\n",
        "        return marco_f1\n",
        "\n",
        "    def step(self, action):\n",
        "        # GET THE bigining state accuracy, later use to calculate the reward\n",
        "        pre_marco_f1 = self.get_f1()\n",
        "\n",
        "        # choose subset\n",
        "        choosen_subset =  self.get_subset(action)\n",
        "\n",
        "        # cotraining the 2 classifier\n",
        "        new_marco_f1 = self.co_training(choosen_subset)\n",
        "\n",
        "        # get the next state_\n",
        "        n_state = self.get_state()\n",
        "\n",
        "        ##############\n",
        "        # calculate the reward\n",
        "        ##############\n",
        "        if new_marco_f1 > pre_marco_f1:\n",
        "            reward_0 = new_marco_f1 - pre_marco_f1\n",
        "        else:\n",
        "            reward_0 = 0\n",
        "\n",
        "        return n_state, reward_0, new_marco_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AkP5ovW-bjX",
        "outputId": "eba85be5-304f-46ad-fe3c-a60b31227b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reset\n",
            "reset\n"
          ]
        }
      ],
      "source": [
        "k = 20\n",
        "env = Env(clf_1, clf_2, input_ul_data=X_ul, k=k, X_test=X_l_test, y_test=y_l_test, X_reset=X_l_train, y_reset=y_l_train)\n",
        "state_0 = env.get_state(reset=True)\n",
        "# env.cluster_plot()\n",
        "# env.get_acc()\n",
        "# env.get_subset(2)\n",
        "state, reward, marco_f1 = env.step(19)\n",
        "# state.shape\n",
        "# reward\n",
        "\n",
        "# print(state_0)\n",
        "# print(state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuqYSBf7-bjX"
      },
      "source": [
        "<!-- #### DQN\n",
        "\n",
        "<img src=\"https://yinyoupoet.github.io/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_8coZ4g_pRtfyoHmsuzMH6g.png\" alt=\"Description of the image\" width=\"400\" height=\"300\">\n",
        "\n",
        "##### loss\n",
        "\n",
        "\n",
        "<img src=\"https://yinyoupoet.github.io/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_YCgMUijhU4p_y3sctvu-kQ.png\" alt=\"Description of the image\" width=\"300\" height=\"50\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm4jSekd-bjX"
      },
      "outputs": [],
      "source": [
        "## RL functions##\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the Q-network (a simple feedforward neural network)\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        # # bug - some times the state is not\n",
        "        # if len(state) != 4:\n",
        "        #     state = state[0]\n",
        "        experience_tuple = (state, action, reward, next_state)\n",
        "        # Append experience_tuple to the memory buffer\n",
        "        self.memory.append(experience_tuple)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Draw a random sample of size batch_size\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        # Transform batch into a tuple of lists\n",
        "        states, actions, rewards, next_states = (zip(*batch))\n",
        "        return states, actions, rewards, next_states\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XYUqe9H-bjY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "#\n",
        "EPS_START = 1\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "steps_done = 0\n",
        "\n",
        "####\n",
        "buffer_size = 10000\n",
        "###\n",
        "episodes = 500\n",
        "max_step = 100\n",
        "batch_size = 64\n",
        "TAU = 0.005\n",
        "gamma = 0.99\n",
        "\n",
        "# hyper parameter\n",
        "observation_size = env.observation_size\n",
        "action_size = env.action_size\n",
        "lr = 1e-4\n",
        "\n",
        "###\n",
        "# Initialize networks and optimizer\n",
        "q_network = QNetwork(observation_size, action_size)\n",
        "target_network = QNetwork(observation_size, action_size)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "# Replay memory\n",
        "replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return the largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            action = torch.argmax(q_network(state)).item()\n",
        "            return action\n",
        "    else:\n",
        "        return np.random.choice(range(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSFEIzc7-bjY"
      },
      "outputs": [],
      "source": [
        "# Function to update the Q-network\n",
        "def train():\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "        # prepare the training data\n",
        "    states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "\n",
        "    rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "    actions = torch.tensor(np.array(actions)).unsqueeze(1)\n",
        "    states = torch.tensor(np.array(states)).squeeze(1)\n",
        "    next_states = torch.tensor(np.array(next_states)).squeeze(1)\n",
        "\n",
        "    # # Compute current Q values\n",
        "    q_values = q_network(states).gather(1, actions)\n",
        "\n",
        "\n",
        "\n",
        "        # Compute target Q values\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_network(next_states).max(1).values.unsqueeze(1)\n",
        "    targets = rewards + (gamma * next_q_values)\n",
        "\n",
        "\n",
        "        # Update the network\n",
        "    loss = loss_fn(q_values, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_haF2H6b-bjZ",
        "outputId": "2a9dbc31-8d98-4317-d09c-44d2104d87db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reset\n",
            "Episode: 0, marco_f1: 0.9038137834848265, Total Reward: 0.011814642554529464\n",
            "reset\n",
            "Episode: 1, marco_f1: 0.9038137834848265, Total Reward: 0.045064270228768466\n",
            "reset\n",
            "Episode: 2, marco_f1: 0.9038137834848265, Total Reward: 0.05475630002615661\n",
            "reset\n",
            "Episode: 3, marco_f1: 0.9058338909836953, Total Reward: 0.07239155569652611\n",
            "reset\n",
            "Episode: 4, marco_f1: 0.9038137834848265, Total Reward: 0.0806958057007382\n",
            "reset\n",
            "Episode: 5, marco_f1: 0.9078215425062922, Total Reward: 0.07092687975093448\n",
            "reset\n",
            "Episode: 6, marco_f1: 0.8999423668032787, Total Reward: 0.07276900254677698\n",
            "reset\n",
            "Episode: 7, marco_f1: 0.9038137834848265, Total Reward: 0.07287442049760029\n",
            "reset\n",
            "Episode: 8, marco_f1: 0.9078215425062922, Total Reward: 0.08666587800797099\n",
            "reset\n",
            "Episode: 9, marco_f1: 0.8999423668032787, Total Reward: 0.15893901102783436\n",
            "reset\n",
            "Episode: 10, marco_f1: 0.8999423668032787, Total Reward: 0.1252424975374573\n",
            "reset\n",
            "Episode: 11, marco_f1: 0.9038754225476218, Total Reward: 0.17079941851350766\n",
            "reset\n",
            "Episode: 12, marco_f1: 0.8999423668032787, Total Reward: 0.17675253475472885\n",
            "reset\n",
            "Episode: 13, marco_f1: 0.9038461538461539, Total Reward: 0.18647264111551975\n",
            "reset\n",
            "Episode: 14, marco_f1: 0.8999423668032787, Total Reward: 0.1907114480888693\n",
            "reset\n",
            "Episode: 15, marco_f1: 0.9018582833611736, Total Reward: 0.11265335742121829\n",
            "reset\n",
            "Episode: 16, marco_f1: 0.9038137834848265, Total Reward: 0.1319999827819225\n",
            "reset\n",
            "Episode: 17, marco_f1: 0.9038754225476218, Total Reward: 0.18335699617551782\n",
            "reset\n",
            "Episode: 18, marco_f1: 0.9038754225476218, Total Reward: 0.17907990246398942\n",
            "reset\n",
            "Episode: 19, marco_f1: 0.9018582833611736, Total Reward: 0.11719389505461353\n",
            "reset\n",
            "Episode: 20, marco_f1: 0.9118293015277577, Total Reward: 0.12928837987483277\n",
            "reset\n",
            "Episode: 21, marco_f1: 0.9038754225476218, Total Reward: 0.1746051779407174\n",
            "reset\n",
            "Episode: 22, marco_f1: 0.9018582833611736, Total Reward: 0.1642845144229812\n",
            "reset\n",
            "Episode: 23, marco_f1: 0.9018582833611736, Total Reward: 0.13695233964499776\n",
            "reset\n",
            "Episode: 24, marco_f1: 0.9038754225476218, Total Reward: 0.16061699976673194\n",
            "reset\n",
            "Episode: 25, marco_f1: 0.9118293015277577, Total Reward: 0.1627397386151772\n",
            "reset\n",
            "Episode: 26, marco_f1: 0.9038461538461539, Total Reward: 0.15485164529841777\n",
            "reset\n",
            "Episode: 27, marco_f1: 0.8999423668032787, Total Reward: 0.16221944520076814\n",
            "reset\n",
            "Episode: 28, marco_f1: 0.9038754225476218, Total Reward: 0.17082475245532813\n",
            "reset\n",
            "Episode: 29, marco_f1: 0.9078215425062922, Total Reward: 0.168950326484806\n",
            "reset\n",
            "Episode: 30, marco_f1: 0.8998397435897436, Total Reward: 0.16861375680905977\n",
            "reset\n",
            "Episode: 31, marco_f1: 0.9058338909836953, Total Reward: 0.13934675669723773\n",
            "reset\n",
            "Episode: 32, marco_f1: 0.8999423668032787, Total Reward: 0.1549408661021342\n",
            "reset\n",
            "Episode: 33, marco_f1: 0.9038461538461539, Total Reward: 0.16635030284740782\n",
            "reset\n",
            "Episode: 34, marco_f1: 0.9018582833611736, Total Reward: 0.17317546816826634\n",
            "reset\n",
            "Episode: 35, marco_f1: 0.9018582833611736, Total Reward: 0.14924941185932794\n",
            "reset\n",
            "Episode: 36, marco_f1: 0.8978524990085683, Total Reward: 0.15073183110019994\n",
            "reset\n",
            "Episode: 37, marco_f1: 0.9038754225476218, Total Reward: 0.16695507146941857\n",
            "reset\n",
            "Episode: 38, marco_f1: 0.9038461538461539, Total Reward: 0.17833039128512473\n",
            "reset\n",
            "Episode: 39, marco_f1: 0.8998702318204392, Total Reward: 0.13333020969101483\n",
            "reset\n",
            "Episode: 40, marco_f1: 0.8999423668032787, Total Reward: 0.19283048369767575\n",
            "reset\n",
            "Episode: 41, marco_f1: 0.8978524990085683, Total Reward: 0.16055799491283518\n",
            "reset\n",
            "Episode: 42, marco_f1: 0.9018582833611736, Total Reward: 0.15472241650216045\n",
            "reset\n",
            "Episode: 43, marco_f1: 0.9018582833611736, Total Reward: 0.18047542892324153\n",
            "reset\n",
            "Episode: 44, marco_f1: 0.9058338909836953, Total Reward: 0.1784405891604799\n",
            "reset\n",
            "Episode: 45, marco_f1: 0.8998397435897436, Total Reward: 0.15694461974627338\n",
            "reset\n",
            "Episode: 46, marco_f1: 0.9018582833611736, Total Reward: 0.17266230618398692\n",
            "reset\n",
            "Episode: 47, marco_f1: 0.9038754225476218, Total Reward: 0.1588602008200004\n",
            "reset\n",
            "Episode: 48, marco_f1: 0.9038461538461539, Total Reward: 0.15918595461354634\n",
            "reset\n",
            "Episode: 49, marco_f1: 0.9038461538461539, Total Reward: 0.18661824598168275\n",
            "reset\n",
            "Episode: 50, marco_f1: 0.9038754225476218, Total Reward: 0.18268207255018332\n",
            "reset\n",
            "Episode: 51, marco_f1: 0.9038754225476218, Total Reward: 0.1898206965097482\n",
            "reset\n",
            "Episode: 52, marco_f1: 0.8999423668032787, Total Reward: 0.17397865054388884\n",
            "reset\n",
            "Episode: 53, marco_f1: 0.9038461538461539, Total Reward: 0.19416065381888614\n",
            "reset\n",
            "Episode: 54, marco_f1: 0.8999423668032787, Total Reward: 0.18250602592360055\n",
            "reset\n",
            "Episode: 55, marco_f1: 0.8999423668032787, Total Reward: 0.19186562237607763\n",
            "reset\n",
            "Episode: 56, marco_f1: 0.9038461538461539, Total Reward: 0.19025284084392136\n",
            "reset\n",
            "Episode: 57, marco_f1: 0.9118293015277577, Total Reward: 0.1879424801779489\n",
            "reset\n",
            "Episode: 58, marco_f1: 0.9038754225476218, Total Reward: 0.1830343016096444\n",
            "reset\n",
            "Episode: 59, marco_f1: 0.9038754225476218, Total Reward: 0.19028904471478403\n",
            "reset\n",
            "Episode: 60, marco_f1: 0.9038754225476218, Total Reward: 0.18059375874336514\n",
            "reset\n",
            "Episode: 61, marco_f1: 0.9018582833611736, Total Reward: 0.18797165884525602\n",
            "reset\n",
            "Episode: 62, marco_f1: 0.9058338909836953, Total Reward: 0.2020729784630444\n",
            "reset\n",
            "Episode: 63, marco_f1: 0.9019337071860578, Total Reward: 0.21621493654422252\n",
            "reset\n",
            "Episode: 64, marco_f1: 0.9058640677137787, Total Reward: 0.21623921465034912\n",
            "reset\n",
            "Episode: 65, marco_f1: 0.9058338909836953, Total Reward: 0.19637989917598386\n",
            "reset\n",
            "Episode: 66, marco_f1: 0.9038461538461539, Total Reward: 0.21507129486543286\n",
            "reset\n",
            "Episode: 67, marco_f1: 0.8999423668032787, Total Reward: 0.20851869726792327\n",
            "reset\n",
            "Episode: 68, marco_f1: 0.8999423668032787, Total Reward: 0.22427551827351555\n",
            "reset\n",
            "Episode: 69, marco_f1: 0.9118293015277577, Total Reward: 0.23477428146268986\n",
            "reset\n",
            "Episode: 70, marco_f1: 0.8998397435897436, Total Reward: 0.21262991295529243\n",
            "reset\n",
            "Episode: 71, marco_f1: 0.9038461538461539, Total Reward: 0.22031041228716153\n",
            "reset\n",
            "Episode: 72, marco_f1: 0.9038461538461539, Total Reward: 0.17303883646405482\n",
            "reset\n",
            "Episode: 73, marco_f1: 0.9118293015277577, Total Reward: 0.2188588063554716\n",
            "reset\n",
            "Episode: 74, marco_f1: 0.8999423668032787, Total Reward: 0.1921975046843426\n",
            "reset\n",
            "Episode: 75, marco_f1: 0.8999423668032787, Total Reward: 0.20633425027467633\n",
            "reset\n",
            "Episode: 76, marco_f1: 0.9038754225476218, Total Reward: 0.21873453468994053\n",
            "reset\n",
            "Episode: 77, marco_f1: 0.8999423668032787, Total Reward: 0.2373254389809023\n",
            "reset\n",
            "Episode: 78, marco_f1: 0.9018582833611736, Total Reward: 0.18282773784238626\n",
            "reset\n",
            "Episode: 79, marco_f1: 0.8999423668032787, Total Reward: 0.21000158762809484\n",
            "reset\n",
            "Episode: 80, marco_f1: 0.9038754225476218, Total Reward: 0.22478755062844935\n",
            "reset\n",
            "Episode: 81, marco_f1: 0.8978524990085683, Total Reward: 0.2187950593959037\n",
            "reset\n",
            "Episode: 82, marco_f1: 0.9018582833611736, Total Reward: 0.21009944064271968\n",
            "reset\n",
            "Episode: 83, marco_f1: 0.8999423668032787, Total Reward: 0.21239421878922704\n",
            "reset\n",
            "Episode: 84, marco_f1: 0.9098091561744652, Total Reward: 0.21844195159801028\n",
            "reset\n",
            "Episode: 85, marco_f1: 0.9038461538461539, Total Reward: 0.17082225246705585\n",
            "reset\n",
            "Episode: 86, marco_f1: 0.9058338909836953, Total Reward: 0.15423444881831017\n",
            "reset\n",
            "Episode: 87, marco_f1: 0.9058338909836953, Total Reward: 0.22642993959309732\n",
            "reset\n",
            "Episode: 88, marco_f1: 0.9018582833611736, Total Reward: 0.2468264774819714\n",
            "reset\n",
            "Episode: 89, marco_f1: 0.8999423668032787, Total Reward: 0.2366429650002564\n",
            "reset\n",
            "Episode: 90, marco_f1: 0.9038461538461539, Total Reward: 0.22829722273552622\n",
            "reset\n",
            "Episode: 91, marco_f1: 0.9118293015277577, Total Reward: 0.2528169654310368\n",
            "reset\n",
            "Episode: 92, marco_f1: 0.9018582833611736, Total Reward: 0.20459903608091223\n",
            "reset\n",
            "Episode: 93, marco_f1: 0.9038754225476218, Total Reward: 0.2043150668697301\n",
            "reset\n",
            "Episode: 94, marco_f1: 0.9018582833611736, Total Reward: 0.23681384064213828\n",
            "reset\n",
            "Episode: 95, marco_f1: 0.9018582833611736, Total Reward: 0.24078399555564345\n",
            "reset\n",
            "Episode: 96, marco_f1: 0.9018582833611736, Total Reward: 0.24660293990516957\n",
            "reset\n",
            "Episode: 97, marco_f1: 0.8999423668032787, Total Reward: 0.22884706667118038\n",
            "reset\n",
            "Episode: 98, marco_f1: 0.8999423668032787, Total Reward: 0.15483130158694347\n",
            "reset\n",
            "Episode: 99, marco_f1: 0.8978524990085683, Total Reward: 0.12653235057743506\n",
            "reset\n",
            "Episode: 100, marco_f1: 0.8999423668032787, Total Reward: 0.1964340387473974\n",
            "reset\n",
            "Episode: 101, marco_f1: 0.9058338909836953, Total Reward: 0.25049934200572765\n",
            "reset\n",
            "Episode: 102, marco_f1: 0.9018582833611736, Total Reward: 0.24901829270329856\n",
            "reset\n",
            "Episode: 103, marco_f1: 0.9018582833611736, Total Reward: 0.24082002278762615\n",
            "reset\n",
            "Episode: 104, marco_f1: 0.8999423668032787, Total Reward: 0.2368501896050963\n",
            "reset\n",
            "Episode: 105, marco_f1: 0.9058338909836953, Total Reward: 0.23834795966510847\n",
            "reset\n",
            "Episode: 106, marco_f1: 0.9038754225476218, Total Reward: 0.23071644043289008\n",
            "reset\n",
            "Episode: 107, marco_f1: 0.9018582833611736, Total Reward: 0.16475117044229604\n",
            "reset\n",
            "Episode: 108, marco_f1: 0.8978524990085683, Total Reward: 0.11886456172538817\n",
            "reset\n",
            "Episode: 109, marco_f1: 0.8999423668032787, Total Reward: 0.12284820485890124\n",
            "reset\n",
            "Episode: 110, marco_f1: 0.8999423668032787, Total Reward: 0.16076517987582073\n",
            "reset\n",
            "Episode: 111, marco_f1: 0.8998397435897436, Total Reward: 0.17883079920945122\n",
            "reset\n",
            "Episode: 112, marco_f1: 0.9018582833611736, Total Reward: 0.1896488109051473\n",
            "reset\n",
            "Episode: 113, marco_f1: 0.9058338909836953, Total Reward: 0.19074749568994853\n",
            "reset\n",
            "Episode: 114, marco_f1: 0.9058338909836953, Total Reward: 0.19638340689098166\n",
            "reset\n",
            "Episode: 115, marco_f1: 0.8998397435897436, Total Reward: 0.1784805121986578\n",
            "reset\n",
            "Episode: 116, marco_f1: 0.9078215425062922, Total Reward: 0.20677196485585414\n",
            "reset\n",
            "Episode: 117, marco_f1: 0.9118293015277577, Total Reward: 0.2187030346096014\n",
            "reset\n",
            "Episode: 118, marco_f1: 0.8999423668032787, Total Reward: 0.1907443450024372\n",
            "reset\n",
            "Episode: 119, marco_f1: 0.8998397435897436, Total Reward: 0.20880792530017267\n",
            "reset\n",
            "Episode: 120, marco_f1: 0.9018582833611736, Total Reward: 0.26484774773641784\n",
            "reset\n",
            "Episode: 121, marco_f1: 0.9038461538461539, Total Reward: 0.2770979530849067\n",
            "reset\n",
            "Episode: 122, marco_f1: 0.9018582833611736, Total Reward: 0.24142552708433274\n",
            "reset\n",
            "Episode: 123, marco_f1: 0.9078215425062922, Total Reward: 0.20660751691397428\n",
            "reset\n",
            "Episode: 124, marco_f1: 0.9098091561744652, Total Reward: 0.21265173435742635\n",
            "reset\n",
            "Episode: 125, marco_f1: 0.9058640677137787, Total Reward: 0.2309959651427259\n",
            "reset\n",
            "Episode: 126, marco_f1: 0.8999423668032787, Total Reward: 0.23472420520706205\n",
            "reset\n",
            "Episode: 127, marco_f1: 0.9118293015277577, Total Reward: 0.2774032900003417\n",
            "reset\n",
            "Episode: 128, marco_f1: 0.9118293015277577, Total Reward: 0.27116268175160174\n",
            "reset\n",
            "Episode: 129, marco_f1: 0.9038461538461539, Total Reward: 0.26701271595084286\n",
            "reset\n",
            "Episode: 130, marco_f1: 0.9058640677137787, Total Reward: 0.2590388202832329\n",
            "reset\n",
            "Episode: 131, marco_f1: 0.9058338909836953, Total Reward: 0.21083098392312338\n",
            "reset\n",
            "Episode: 132, marco_f1: 0.9038461538461539, Total Reward: 0.2563847242179089\n",
            "reset\n",
            "Episode: 133, marco_f1: 0.9078215425062922, Total Reward: 0.21876225302720997\n",
            "reset\n",
            "Episode: 134, marco_f1: 0.9118293015277577, Total Reward: 0.2873798545835867\n",
            "reset\n",
            "Episode: 135, marco_f1: 0.9058338909836953, Total Reward: 0.2789458220058898\n",
            "reset\n",
            "Episode: 136, marco_f1: 0.9058640677137787, Total Reward: 0.29106730566983186\n",
            "reset\n",
            "Episode: 137, marco_f1: 0.9018582833611736, Total Reward: 0.2890915575596382\n",
            "reset\n",
            "Episode: 138, marco_f1: 0.9058640677137787, Total Reward: 0.22678310027251858\n",
            "reset\n",
            "Episode: 139, marco_f1: 0.8999423668032787, Total Reward: 0.15285070785281107\n",
            "reset\n",
            "Episode: 140, marco_f1: 0.9038754225476218, Total Reward: 0.18072989526142114\n",
            "reset\n",
            "Episode: 141, marco_f1: 0.901886580887506, Total Reward: 0.227085824805626\n",
            "reset\n",
            "Episode: 142, marco_f1: 0.8978524990085683, Total Reward: 0.23755951551992327\n",
            "reset\n",
            "Episode: 143, marco_f1: 0.9058338909836953, Total Reward: 0.28756832884810724\n",
            "reset\n",
            "Episode: 144, marco_f1: 0.9058640677137787, Total Reward: 0.2934585745935736\n",
            "reset\n",
            "Episode: 145, marco_f1: 0.9018582833611736, Total Reward: 0.2607136785459344\n",
            "reset\n",
            "Episode: 146, marco_f1: 0.8998397435897436, Total Reward: 0.2629220511405125\n",
            "reset\n",
            "Episode: 147, marco_f1: 0.9018582833611736, Total Reward: 0.26711425576561865\n",
            "reset\n",
            "Episode: 148, marco_f1: 0.8998397435897436, Total Reward: 0.17670998503799362\n",
            "reset\n",
            "Episode: 149, marco_f1: 0.9018582833611736, Total Reward: 0.2045667110894227\n",
            "reset\n",
            "Episode: 150, marco_f1: 0.9058640677137787, Total Reward: 0.15281935637120758\n",
            "reset\n",
            "Episode: 151, marco_f1: 0.9038754225476218, Total Reward: 0.1727193671775814\n",
            "reset\n",
            "Episode: 152, marco_f1: 0.8999423668032787, Total Reward: 0.19853495067112414\n",
            "reset\n",
            "Episode: 153, marco_f1: 0.9018582833611736, Total Reward: 0.16857023640875424\n",
            "reset\n",
            "Episode: 154, marco_f1: 0.9018582833611736, Total Reward: 0.19254605777244116\n",
            "reset\n",
            "Episode: 155, marco_f1: 0.9058338909836953, Total Reward: 0.22862783033770828\n",
            "reset\n",
            "Episode: 156, marco_f1: 0.8978524990085683, Total Reward: 0.23018853691973562\n",
            "reset\n",
            "Episode: 157, marco_f1: 0.8998397435897436, Total Reward: 0.20299134410437714\n",
            "reset\n",
            "Episode: 158, marco_f1: 0.9118293015277577, Total Reward: 0.2424870550993602\n",
            "reset\n",
            "Episode: 159, marco_f1: 0.9058640677137787, Total Reward: 0.2829943169520862\n",
            "reset\n",
            "Episode: 160, marco_f1: 0.8938773221844453, Total Reward: 0.2670146231012709\n",
            "reset\n",
            "Episode: 161, marco_f1: 0.8998397435897436, Total Reward: 0.264959667282625\n",
            "reset\n",
            "Episode: 162, marco_f1: 0.8978524990085683, Total Reward: 0.2751321075284645\n",
            "reset\n",
            "Episode: 163, marco_f1: 0.9058640677137787, Total Reward: 0.2748093050205388\n",
            "reset\n",
            "Episode: 164, marco_f1: 0.8978524990085683, Total Reward: 0.29328474626649537\n",
            "reset\n",
            "Episode: 165, marco_f1: 0.9058640677137787, Total Reward: 0.30112250562656084\n",
            "reset\n",
            "Episode: 166, marco_f1: 0.9018582833611736, Total Reward: 0.24654509875949826\n",
            "reset\n",
            "Episode: 167, marco_f1: 0.9118293015277577, Total Reward: 0.204963009306246\n",
            "reset\n",
            "Episode: 168, marco_f1: 0.9018582833611736, Total Reward: 0.2147085332058173\n",
            "reset\n",
            "Episode: 169, marco_f1: 0.9018582833611736, Total Reward: 0.24455636081071908\n",
            "reset\n",
            "Episode: 170, marco_f1: 0.9058640677137787, Total Reward: 0.24460414752179627\n",
            "reset\n",
            "Episode: 171, marco_f1: 0.9098091561744652, Total Reward: 0.21486000309558018\n",
            "reset\n",
            "Episode: 172, marco_f1: 0.9078215425062922, Total Reward: 0.21276406309828866\n",
            "reset\n",
            "Episode: 173, marco_f1: 0.9058338909836953, Total Reward: 0.24692612154367577\n",
            "reset\n",
            "Episode: 174, marco_f1: 0.9118293015277577, Total Reward: 0.2572032544914623\n",
            "reset\n",
            "Episode: 175, marco_f1: 0.9058338909836953, Total Reward: 0.2887769420782613\n",
            "reset\n",
            "Episode: 176, marco_f1: 0.9058640677137787, Total Reward: 0.23293532424023755\n",
            "reset\n",
            "Episode: 177, marco_f1: 0.9118293015277577, Total Reward: 0.30312759167961334\n",
            "reset\n",
            "Episode: 178, marco_f1: 0.9058338909836953, Total Reward: 0.29708913260206116\n",
            "reset\n",
            "Episode: 179, marco_f1: 0.9018582833611736, Total Reward: 0.270988604422416\n",
            "reset\n",
            "Episode: 180, marco_f1: 0.9018582833611736, Total Reward: 0.27690012573179645\n",
            "reset\n",
            "Episode: 181, marco_f1: 0.9118293015277577, Total Reward: 0.28288953817632834\n",
            "reset\n",
            "Episode: 182, marco_f1: 0.8998702318204392, Total Reward: 0.2470767259052551\n",
            "reset\n",
            "Episode: 183, marco_f1: 0.8938773221844453, Total Reward: 0.28723982115985147\n",
            "reset\n",
            "Episode: 184, marco_f1: 0.8978524990085683, Total Reward: 0.3133089087584138\n",
            "reset\n",
            "Episode: 185, marco_f1: 0.9058338909836953, Total Reward: 0.2892448882588966\n",
            "reset\n",
            "Episode: 186, marco_f1: 0.8998397435897436, Total Reward: 0.31725427525647865\n",
            "reset\n",
            "Episode: 187, marco_f1: 0.8998702318204392, Total Reward: 0.295007097445975\n",
            "reset\n",
            "Episode: 188, marco_f1: 0.9058338909836953, Total Reward: 0.29290835430085627\n",
            "reset\n",
            "Episode: 189, marco_f1: 0.9058338909836953, Total Reward: 0.2611132860505023\n",
            "reset\n",
            "Episode: 190, marco_f1: 0.9118293015277577, Total Reward: 0.3190663054507278\n",
            "reset\n",
            "Episode: 191, marco_f1: 0.9118293015277577, Total Reward: 0.30122880157507137\n",
            "reset\n",
            "Episode: 192, marco_f1: 0.9018582833611736, Total Reward: 0.2912414605857465\n",
            "reset\n",
            "Episode: 193, marco_f1: 0.9058640677137787, Total Reward: 0.3191811241949638\n",
            "reset\n",
            "Episode: 194, marco_f1: 0.9058640677137787, Total Reward: 0.3252073963326132\n",
            "reset\n",
            "Episode: 195, marco_f1: 0.9058640677137787, Total Reward: 0.31298535900466684\n",
            "reset\n",
            "Episode: 196, marco_f1: 0.9118293015277577, Total Reward: 0.3271277872043439\n",
            "reset\n",
            "Episode: 197, marco_f1: 0.8999423668032787, Total Reward: 0.2990825482438504\n",
            "reset\n",
            "Episode: 198, marco_f1: 0.9058640677137787, Total Reward: 0.3050415746187344\n",
            "reset\n",
            "Episode: 199, marco_f1: 0.9058338909836953, Total Reward: 0.3270981621056216\n",
            "reset\n",
            "Episode: 200, marco_f1: 0.9018582833611736, Total Reward: 0.31116356332823003\n",
            "reset\n",
            "Episode: 201, marco_f1: 0.8999423668032787, Total Reward: 0.31496280579509084\n",
            "reset\n",
            "Episode: 202, marco_f1: 0.9058338909836953, Total Reward: 0.31917117692710584\n",
            "reset\n",
            "Episode: 203, marco_f1: 0.9058640677137787, Total Reward: 0.3192342544293064\n",
            "reset\n",
            "Episode: 204, marco_f1: 0.9058640677137787, Total Reward: 0.2731984640628944\n",
            "reset\n",
            "Episode: 205, marco_f1: 0.9118293015277577, Total Reward: 0.31921247291024224\n",
            "reset\n",
            "Episode: 206, marco_f1: 0.9118293015277577, Total Reward: 0.3250436788485097\n",
            "reset\n",
            "Episode: 207, marco_f1: 0.9118293015277577, Total Reward: 0.3131742734477664\n",
            "reset\n",
            "Episode: 208, marco_f1: 0.9098091561744652, Total Reward: 0.32301384453473214\n",
            "reset\n",
            "Episode: 209, marco_f1: 0.8998397435897436, Total Reward: 0.3050453134281432\n",
            "reset\n",
            "Episode: 210, marco_f1: 0.8998397435897436, Total Reward: 0.3133103882065962\n",
            "reset\n",
            "Episode: 211, marco_f1: 0.9018582833611736, Total Reward: 0.3090665238287853\n",
            "reset\n",
            "Episode: 212, marco_f1: 0.9058640677137787, Total Reward: 0.3150246201471807\n",
            "reset\n",
            "Episode: 213, marco_f1: 0.9058640677137787, Total Reward: 0.31707380386342987\n",
            "reset\n",
            "Episode: 214, marco_f1: 0.9018582833611736, Total Reward: 0.3272299796194803\n",
            "reset\n",
            "Episode: 215, marco_f1: 0.8998397435897436, Total Reward: 0.30497185213600686\n",
            "reset\n",
            "Episode: 216, marco_f1: 0.9058640677137787, Total Reward: 0.3211690158726094\n",
            "reset\n",
            "Episode: 217, marco_f1: 0.9058640677137787, Total Reward: 0.3008914866563108\n",
            "reset\n",
            "Episode: 218, marco_f1: 0.9118293015277577, Total Reward: 0.30502342071351995\n",
            "reset\n",
            "Episode: 219, marco_f1: 0.9018582833611736, Total Reward: 0.2950822519793225\n",
            "reset\n",
            "Episode: 220, marco_f1: 0.9018582833611736, Total Reward: 0.3190387167788332\n",
            "reset\n",
            "Episode: 221, marco_f1: 0.9058640677137787, Total Reward: 0.3092775881502492\n",
            "reset\n",
            "Episode: 222, marco_f1: 0.9058640677137787, Total Reward: 0.2910945347797623\n",
            "reset\n",
            "Episode: 223, marco_f1: 0.9058640677137787, Total Reward: 0.33117676202867596\n",
            "reset\n",
            "Episode: 224, marco_f1: 0.9058640677137787, Total Reward: 0.309103711920772\n",
            "reset\n",
            "Episode: 225, marco_f1: 0.9038137834848265, Total Reward: 0.31500119856576725\n",
            "reset\n",
            "Episode: 226, marco_f1: 0.8998702318204392, Total Reward: 0.2910429626321158\n",
            "reset\n",
            "Episode: 227, marco_f1: 0.9058640677137787, Total Reward: 0.22749089758139895\n",
            "reset\n",
            "Episode: 228, marco_f1: 0.9058338909836953, Total Reward: 0.287197954723621\n",
            "reset\n",
            "Episode: 229, marco_f1: 0.8978524990085683, Total Reward: 0.25704606583236633\n",
            "reset\n",
            "Episode: 230, marco_f1: 0.8998702318204392, Total Reward: 0.3132950601593687\n",
            "reset\n",
            "Episode: 231, marco_f1: 0.9058338909836953, Total Reward: 0.31724463808801395\n",
            "reset\n",
            "Episode: 232, marco_f1: 0.9118293015277577, Total Reward: 0.28918165739539803\n",
            "reset\n",
            "Episode: 233, marco_f1: 0.9018582833611736, Total Reward: 0.2669796774425661\n",
            "reset\n",
            "Episode: 234, marco_f1: 0.9058338909836953, Total Reward: 0.2990707606820987\n",
            "reset\n",
            "Episode: 235, marco_f1: 0.9058640677137787, Total Reward: 0.2869957201144373\n",
            "reset\n",
            "Episode: 236, marco_f1: 0.9018582833611736, Total Reward: 0.2792318866028559\n",
            "reset\n",
            "Episode: 237, marco_f1: 0.9058640677137787, Total Reward: 0.24905498466462483\n",
            "reset\n",
            "Episode: 238, marco_f1: 0.9018582833611736, Total Reward: 0.20909622204031464\n",
            "reset\n",
            "Episode: 239, marco_f1: 0.9038754225476218, Total Reward: 0.1571212466986892\n",
            "reset\n",
            "Episode: 240, marco_f1: 0.8999423668032787, Total Reward: 0.18317076044268132\n",
            "reset\n",
            "Episode: 241, marco_f1: 0.8998702318204392, Total Reward: 0.2436394779097103\n",
            "reset\n",
            "Episode: 242, marco_f1: 0.9058640677137787, Total Reward: 0.2950993668596894\n",
            "reset\n",
            "Episode: 243, marco_f1: 0.9058338909836953, Total Reward: 0.2828886316500744\n",
            "reset\n",
            "Episode: 244, marco_f1: 0.9118293015277577, Total Reward: 0.31721205168924915\n",
            "reset\n",
            "Episode: 245, marco_f1: 0.9018582833611736, Total Reward: 0.28098457377559616\n",
            "reset\n",
            "Episode: 246, marco_f1: 0.9058338909836953, Total Reward: 0.2590642397656663\n",
            "reset\n",
            "Episode: 247, marco_f1: 0.9018582833611736, Total Reward: 0.3192342811315023\n",
            "reset\n",
            "Episode: 248, marco_f1: 0.8998397435897436, Total Reward: 0.3309779732743997\n",
            "reset\n",
            "Episode: 249, marco_f1: 0.9058640677137787, Total Reward: 0.2992705439482545\n",
            "reset\n",
            "Episode: 250, marco_f1: 0.9018268225149164, Total Reward: 0.2848161754499541\n",
            "reset\n",
            "Episode: 251, marco_f1: 0.9038754225476218, Total Reward: 0.26866298742357\n",
            "reset\n",
            "Episode: 252, marco_f1: 0.9058338909836953, Total Reward: 0.29737832357978444\n",
            "reset\n",
            "Episode: 253, marco_f1: 0.9058338909836953, Total Reward: 0.3215757189007844\n",
            "reset\n",
            "Episode: 254, marco_f1: 0.9018582833611736, Total Reward: 0.29905825446857837\n",
            "reset\n",
            "Episode: 255, marco_f1: 0.8998397435897436, Total Reward: 0.2266663380080014\n",
            "reset\n",
            "Episode: 256, marco_f1: 0.8998702318204392, Total Reward: 0.28341895239040304\n",
            "reset\n",
            "Episode: 257, marco_f1: 0.9038754225476218, Total Reward: 0.23473809031566006\n",
            "reset\n",
            "Episode: 258, marco_f1: 0.9118293015277577, Total Reward: 0.2551481564606918\n",
            "reset\n",
            "Episode: 259, marco_f1: 0.9118293015277577, Total Reward: 0.2252132479578256\n",
            "reset\n",
            "Episode: 260, marco_f1: 0.8998397435897436, Total Reward: 0.19701305089118393\n",
            "reset\n",
            "Episode: 261, marco_f1: 0.9118293015277577, Total Reward: 0.2930769251284414\n",
            "reset\n",
            "Episode: 262, marco_f1: 0.9118293015277577, Total Reward: 0.3352556670647233\n",
            "reset\n",
            "Episode: 263, marco_f1: 0.9118293015277577, Total Reward: 0.29726474477214715\n",
            "reset\n",
            "Episode: 264, marco_f1: 0.9118293015277577, Total Reward: 0.298921477929877\n",
            "reset\n",
            "Episode: 265, marco_f1: 0.9058338909836953, Total Reward: 0.24466427160032955\n",
            "reset\n",
            "Episode: 266, marco_f1: 0.8998397435897436, Total Reward: 0.29501018626878184\n",
            "reset\n",
            "Episode: 267, marco_f1: 0.9118293015277577, Total Reward: 0.30892208758594375\n",
            "reset\n",
            "Episode: 268, marco_f1: 0.9058640677137787, Total Reward: 0.2671478431008504\n",
            "reset\n",
            "Episode: 269, marco_f1: 0.9018582833611736, Total Reward: 0.25736202334457603\n",
            "reset\n",
            "Episode: 270, marco_f1: 0.9018582833611736, Total Reward: 0.2629870912173049\n",
            "reset\n",
            "Episode: 271, marco_f1: 0.8998397435897436, Total Reward: 0.2787561933514435\n",
            "reset\n",
            "Episode: 272, marco_f1: 0.9118293015277577, Total Reward: 0.2968849481692982\n",
            "reset\n",
            "Episode: 273, marco_f1: 0.8978524990085683, Total Reward: 0.2227779434212288\n",
            "reset\n",
            "Episode: 274, marco_f1: 0.9038754225476218, Total Reward: 0.20096182752096348\n",
            "reset\n",
            "Episode: 275, marco_f1: 0.8999423668032787, Total Reward: 0.225476794539462\n",
            "reset\n",
            "Episode: 276, marco_f1: 0.9058640677137787, Total Reward: 0.24690898966572383\n",
            "reset\n",
            "Episode: 277, marco_f1: 0.9058338909836953, Total Reward: 0.21879965815734514\n",
            "reset\n",
            "Episode: 278, marco_f1: 0.9018582833611736, Total Reward: 0.18913257745880252\n",
            "reset\n",
            "Episode: 279, marco_f1: 0.9058640677137787, Total Reward: 0.2650879381866832\n",
            "reset\n",
            "Episode: 280, marco_f1: 0.9038754225476218, Total Reward: 0.3010499287827342\n",
            "reset\n",
            "Episode: 281, marco_f1: 0.9118293015277577, Total Reward: 0.32931625229722616\n",
            "reset\n",
            "Episode: 282, marco_f1: 0.8999423668032787, Total Reward: 0.26288314890815334\n",
            "reset\n",
            "Episode: 283, marco_f1: 0.8978524990085683, Total Reward: 0.22484151079819048\n",
            "reset\n",
            "Episode: 284, marco_f1: 0.8999423668032787, Total Reward: 0.2970524843421982\n",
            "reset\n",
            "Episode: 285, marco_f1: 0.8998702318204392, Total Reward: 0.17314259836069557\n",
            "reset\n",
            "Episode: 286, marco_f1: 0.9118293015277577, Total Reward: 0.19176164226751513\n",
            "reset\n",
            "Episode: 287, marco_f1: 0.9078215425062922, Total Reward: 0.2389294628377282\n",
            "reset\n",
            "Episode: 288, marco_f1: 0.9058640677137787, Total Reward: 0.26322766516253926\n",
            "reset\n",
            "Episode: 289, marco_f1: 0.9058338909836953, Total Reward: 0.3109737133056847\n",
            "reset\n",
            "Episode: 290, marco_f1: 0.8978524990085683, Total Reward: 0.2930578991002041\n",
            "reset\n",
            "Episode: 291, marco_f1: 0.9058640677137787, Total Reward: 0.3170685796241707\n",
            "reset\n",
            "Episode: 292, marco_f1: 0.9018582833611736, Total Reward: 0.2630731993417752\n",
            "reset\n",
            "Episode: 293, marco_f1: 0.8998397435897436, Total Reward: 0.2952688165232443\n",
            "reset\n",
            "Episode: 294, marco_f1: 0.8938773221844453, Total Reward: 0.2806944770578659\n",
            "reset\n",
            "Episode: 295, marco_f1: 0.8999423668032787, Total Reward: 0.29892844228866766\n",
            "reset\n",
            "Episode: 296, marco_f1: 0.9118293015277577, Total Reward: 0.3372041599368705\n",
            "reset\n",
            "Episode: 297, marco_f1: 0.9018582833611736, Total Reward: 0.24712073311969007\n",
            "reset\n",
            "Episode: 298, marco_f1: 0.8999423668032787, Total Reward: 0.3089373704658117\n",
            "reset\n",
            "Episode: 299, marco_f1: 0.9118293015277577, Total Reward: 0.2551634497022134\n",
            "reset\n",
            "Episode: 300, marco_f1: 0.9058338909836953, Total Reward: 0.3129385573004224\n",
            "reset\n",
            "Episode: 301, marco_f1: 0.9018582833611736, Total Reward: 0.26885344621666674\n",
            "reset\n",
            "Episode: 302, marco_f1: 0.9018582833611736, Total Reward: 0.22075893482705378\n",
            "reset\n",
            "Episode: 303, marco_f1: 0.9118293015277577, Total Reward: 0.2610656944119656\n",
            "reset\n",
            "Episode: 304, marco_f1: 0.9038754225476218, Total Reward: 0.23699136890366623\n",
            "reset\n",
            "Episode: 305, marco_f1: 0.9038754225476218, Total Reward: 0.2728716515516444\n",
            "reset\n",
            "Episode: 306, marco_f1: 0.9018582833611736, Total Reward: 0.176885907532209\n",
            "reset\n",
            "Episode: 307, marco_f1: 0.9118293015277577, Total Reward: 0.23724814910206427\n",
            "reset\n",
            "Episode: 308, marco_f1: 0.8938773221844453, Total Reward: 0.23706601586916232\n",
            "reset\n",
            "Episode: 309, marco_f1: 0.9038754225476218, Total Reward: 0.24083316555761802\n",
            "reset\n",
            "Episode: 310, marco_f1: 0.9058640677137787, Total Reward: 0.3228777540375438\n",
            "reset\n",
            "Episode: 311, marco_f1: 0.9118293015277577, Total Reward: 0.3391206474864803\n",
            "reset\n",
            "Episode: 312, marco_f1: 0.9038754225476218, Total Reward: 0.3272714210497363\n",
            "reset\n",
            "Episode: 313, marco_f1: 0.8998397435897436, Total Reward: 0.3065509242190254\n",
            "reset\n",
            "Episode: 314, marco_f1: 0.9118293015277577, Total Reward: 0.2828839480783074\n",
            "reset\n",
            "Episode: 315, marco_f1: 0.9018582833611736, Total Reward: 0.1733447427028525\n",
            "reset\n",
            "Episode: 316, marco_f1: 0.9018582833611736, Total Reward: 0.2432122560159522\n",
            "reset\n",
            "Episode: 317, marco_f1: 0.9058338909836953, Total Reward: 0.23479802751242385\n",
            "reset\n",
            "Episode: 318, marco_f1: 0.9058640677137787, Total Reward: 0.22896784604238019\n",
            "reset\n",
            "Episode: 319, marco_f1: 0.8999423668032787, Total Reward: 0.2948657287979617\n",
            "reset\n",
            "Episode: 320, marco_f1: 0.8998397435897436, Total Reward: 0.294734505080155\n",
            "reset\n",
            "Episode: 321, marco_f1: 0.8999423668032787, Total Reward: 0.27085340919169876\n",
            "reset\n",
            "Episode: 322, marco_f1: 0.9018582833611736, Total Reward: 0.2710408491887156\n",
            "reset\n",
            "Episode: 323, marco_f1: 0.8999423668032787, Total Reward: 0.3167580658477124\n",
            "reset\n",
            "Episode: 324, marco_f1: 0.9058338909836953, Total Reward: 0.3106866159436046\n",
            "reset\n",
            "Episode: 325, marco_f1: 0.8999423668032787, Total Reward: 0.3425081939810599\n",
            "reset\n",
            "Episode: 326, marco_f1: 0.9018582833611736, Total Reward: 0.2886902300535088\n",
            "reset\n",
            "Episode: 327, marco_f1: 0.9018582833611736, Total Reward: 0.3689395154181173\n",
            "reset\n",
            "Episode: 328, marco_f1: 0.9058338909836953, Total Reward: 0.3729190789795733\n",
            "reset\n",
            "Episode: 329, marco_f1: 0.9118293015277577, Total Reward: 0.38500454360955993\n",
            "reset\n",
            "Episode: 330, marco_f1: 0.9058338909836953, Total Reward: 0.37907174302622015\n",
            "reset\n",
            "Episode: 331, marco_f1: 0.8999423668032787, Total Reward: 0.3691631631401474\n",
            "reset\n",
            "Episode: 332, marco_f1: 0.8938773221844453, Total Reward: 0.4048179001474945\n",
            "reset\n",
            "Episode: 333, marco_f1: 0.8938773221844453, Total Reward: 0.3632844280990042\n",
            "reset\n",
            "Episode: 334, marco_f1: 0.9058338909836953, Total Reward: 0.29807701969194966\n",
            "reset\n",
            "Episode: 335, marco_f1: 0.8938773221844453, Total Reward: 0.4326741348485216\n",
            "reset\n",
            "Episode: 336, marco_f1: 0.9058338909836953, Total Reward: 0.4904086877535485\n",
            "reset\n",
            "Episode: 337, marco_f1: 0.8938773221844453, Total Reward: 0.4465873407004647\n",
            "reset\n",
            "Episode: 338, marco_f1: 0.9058338909836953, Total Reward: 0.532206435560197\n",
            "reset\n",
            "Episode: 339, marco_f1: 0.8938773221844453, Total Reward: 0.4073279848239916\n",
            "reset\n",
            "Episode: 340, marco_f1: 0.8938773221844453, Total Reward: 0.26678236406966316\n",
            "reset\n",
            "Episode: 341, marco_f1: 0.8999423668032787, Total Reward: 0.43325696164614824\n",
            "reset\n",
            "Episode: 342, marco_f1: 0.9058338909836953, Total Reward: 0.4272208256633673\n",
            "reset\n",
            "Episode: 343, marco_f1: 0.8938773221844453, Total Reward: 0.5105213731825226\n",
            "reset\n",
            "Episode: 344, marco_f1: 0.9058338909836953, Total Reward: 0.5084594786607871\n",
            "reset\n",
            "Episode: 345, marco_f1: 0.9058338909836953, Total Reward: 0.39542273931153893\n",
            "reset\n",
            "Episode: 346, marco_f1: 0.9058338909836953, Total Reward: 0.5282846463835688\n",
            "reset\n",
            "Episode: 347, marco_f1: 0.9018582833611736, Total Reward: 0.3475908025808514\n",
            "reset\n",
            "Episode: 348, marco_f1: 0.8938773221844453, Total Reward: 0.46894327204553676\n",
            "reset\n",
            "Episode: 349, marco_f1: 0.9019337071860578, Total Reward: 0.3101540674211396\n",
            "reset\n",
            "Episode: 350, marco_f1: 0.8938773221844453, Total Reward: 0.5481687887120522\n",
            "reset\n",
            "Episode: 351, marco_f1: 0.9058338909836953, Total Reward: 0.5522495276327022\n",
            "reset\n",
            "Episode: 352, marco_f1: 0.9058338909836953, Total Reward: 0.5402916747172786\n",
            "reset\n",
            "Episode: 353, marco_f1: 0.9058338909836953, Total Reward: 0.5283884648503918\n",
            "reset\n",
            "Episode: 354, marco_f1: 0.8938773221844453, Total Reward: 0.47055519418115754\n",
            "reset\n",
            "Episode: 355, marco_f1: 0.8938773221844453, Total Reward: 0.5462872718834063\n",
            "reset\n",
            "Episode: 356, marco_f1: 0.8938773221844453, Total Reward: 0.5263532418033021\n",
            "reset\n",
            "Episode: 357, marco_f1: 0.8938773221844453, Total Reward: 0.4806126916925213\n",
            "reset\n",
            "Episode: 358, marco_f1: 0.8938773221844453, Total Reward: 0.5202842185362154\n",
            "reset\n",
            "Episode: 359, marco_f1: 0.8938773221844453, Total Reward: 0.48262550555703854\n",
            "reset\n",
            "Episode: 360, marco_f1: 0.9058338909836953, Total Reward: 0.47472692284061335\n",
            "reset\n",
            "Episode: 361, marco_f1: 0.9058338909836953, Total Reward: 0.49243396300513675\n",
            "reset\n",
            "Episode: 362, marco_f1: 0.8998397435897436, Total Reward: 0.5381133656699804\n",
            "reset\n",
            "Episode: 363, marco_f1: 0.9058338909836953, Total Reward: 0.5104040834197678\n",
            "reset\n",
            "Episode: 364, marco_f1: 0.9058338909836953, Total Reward: 0.5322571889521845\n",
            "reset\n",
            "Episode: 365, marco_f1: 0.8938773221844453, Total Reward: 0.4825276356348547\n",
            "reset\n",
            "Episode: 366, marco_f1: 0.9058338909836953, Total Reward: 0.530301836015665\n",
            "reset\n",
            "Episode: 367, marco_f1: 0.8938773221844453, Total Reward: 0.5501761791567329\n",
            "reset\n",
            "Episode: 368, marco_f1: 0.9058338909836953, Total Reward: 0.5124684395635102\n",
            "reset\n",
            "Episode: 369, marco_f1: 0.9058338909836953, Total Reward: 0.5641527374995889\n",
            "reset\n",
            "Episode: 370, marco_f1: 0.9058338909836953, Total Reward: 0.5362528617909774\n",
            "reset\n",
            "Episode: 371, marco_f1: 0.8938773221844453, Total Reward: 0.5521974318504497\n",
            "reset\n",
            "Episode: 372, marco_f1: 0.8938773221844453, Total Reward: 0.49849127290288353\n",
            "reset\n",
            "Episode: 373, marco_f1: 0.9058338909836953, Total Reward: 0.5521961687003389\n",
            "reset\n",
            "Episode: 374, marco_f1: 0.9058338909836953, Total Reward: 0.5422851530826777\n",
            "reset\n",
            "Episode: 375, marco_f1: 0.9058338909836953, Total Reward: 0.47270579437628646\n",
            "reset\n",
            "Episode: 376, marco_f1: 0.9058338909836953, Total Reward: 0.5283147570904922\n",
            "reset\n",
            "Episode: 377, marco_f1: 0.9058338909836953, Total Reward: 0.5303030991657758\n",
            "reset\n",
            "Episode: 378, marco_f1: 0.9058338909836953, Total Reward: 0.5302485143913303\n",
            "reset\n",
            "Episode: 379, marco_f1: 0.9058338909836953, Total Reward: 0.5203603785255241\n",
            "reset\n",
            "Episode: 380, marco_f1: 0.8938773221844453, Total Reward: 0.5282829538124834\n",
            "reset\n",
            "Episode: 381, marco_f1: 0.8938773221844453, Total Reward: 0.5540995836150909\n",
            "reset\n",
            "Episode: 382, marco_f1: 0.9058338909836953, Total Reward: 0.5242747780032802\n",
            "reset\n",
            "Episode: 383, marco_f1: 0.8938773221844453, Total Reward: 0.522319325580367\n",
            "reset\n",
            "Episode: 384, marco_f1: 0.9018582833611736, Total Reward: 0.5241999941620348\n",
            "reset\n",
            "Episode: 385, marco_f1: 0.9058338909836953, Total Reward: 0.5620490839455621\n",
            "reset\n",
            "Episode: 386, marco_f1: 0.8938773221844453, Total Reward: 0.4984379139705203\n",
            "reset\n",
            "Episode: 387, marco_f1: 0.9058338909836953, Total Reward: 0.5442249070781838\n",
            "reset\n",
            "Episode: 388, marco_f1: 0.8938773221844453, Total Reward: 0.5023600343127047\n",
            "reset\n",
            "Episode: 389, marco_f1: 0.9058338909836953, Total Reward: 0.5640689078331657\n",
            "reset\n",
            "Episode: 390, marco_f1: 0.8938773221844453, Total Reward: 0.514281264536109\n",
            "reset\n",
            "Episode: 391, marco_f1: 0.9058338909836953, Total Reward: 0.5322676449359378\n",
            "reset\n",
            "Episode: 392, marco_f1: 0.9058338909836953, Total Reward: 0.569928706024929\n",
            "reset\n",
            "Episode: 393, marco_f1: 0.8938773221844453, Total Reward: 0.5401876618793214\n",
            "reset\n",
            "Episode: 394, marco_f1: 0.9058338909836953, Total Reward: 0.5282657761673448\n",
            "reset\n",
            "Episode: 395, marco_f1: 0.9058338909836953, Total Reward: 0.5262414459225514\n",
            "reset\n",
            "Episode: 396, marco_f1: 0.8938773221844453, Total Reward: 0.5223016608135371\n",
            "reset\n",
            "Episode: 397, marco_f1: 0.8938773221844453, Total Reward: 0.5282298089638978\n",
            "reset\n",
            "Episode: 398, marco_f1: 0.9058338909836953, Total Reward: 0.5501254072958763\n",
            "reset\n",
            "Episode: 399, marco_f1: 0.9058338909836953, Total Reward: 0.5461500832846198\n",
            "reset\n",
            "Episode: 400, marco_f1: 0.8938773221844453, Total Reward: 0.5242240246112926\n",
            "reset\n",
            "Episode: 401, marco_f1: 0.8938773221844453, Total Reward: 0.5640690734891681\n",
            "reset\n",
            "Episode: 402, marco_f1: 0.8998397435897436, Total Reward: 0.4923297392139989\n",
            "reset\n",
            "Episode: 403, marco_f1: 0.9058338909836953, Total Reward: 0.500375824119505\n",
            "reset\n",
            "Episode: 404, marco_f1: 0.9058338909836953, Total Reward: 0.52031203462208\n",
            "reset\n",
            "Episode: 405, marco_f1: 0.8938773221844453, Total Reward: 0.5003770309463231\n",
            "reset\n",
            "Episode: 406, marco_f1: 0.8938773221844453, Total Reward: 0.556179165875284\n",
            "reset\n",
            "Episode: 407, marco_f1: 0.8938773221844453, Total Reward: 0.5620819945639953\n",
            "reset\n",
            "Episode: 408, marco_f1: 0.8938773221844453, Total Reward: 0.5421744756620981\n",
            "reset\n",
            "Episode: 409, marco_f1: 0.8998397435897436, Total Reward: 0.5341935144853698\n",
            "reset\n",
            "Episode: 410, marco_f1: 0.9058338909836953, Total Reward: 0.5381676723357154\n",
            "reset\n",
            "Episode: 411, marco_f1: 0.8938773221844453, Total Reward: 0.5322977542654374\n",
            "reset\n",
            "Episode: 412, marco_f1: 0.9058338909836953, Total Reward: 0.5242859273300294\n",
            "reset\n",
            "Episode: 413, marco_f1: 0.8938773221844453, Total Reward: 0.5262425643827224\n",
            "reset\n",
            "Episode: 414, marco_f1: 0.9058338909836953, Total Reward: 0.5183211959225813\n",
            "reset\n",
            "Episode: 415, marco_f1: 0.8938773221844453, Total Reward: 0.5322367117766742\n",
            "reset\n",
            "Episode: 416, marco_f1: 0.8938773221844453, Total Reward: 0.5700910876110535\n",
            "reset\n",
            "Episode: 417, marco_f1: 0.8938773221844453, Total Reward: 0.5441299757857511\n",
            "reset\n",
            "Episode: 418, marco_f1: 0.8938773221844453, Total Reward: 0.43270299522001543\n",
            "reset\n",
            "Episode: 419, marco_f1: 0.8938773221844453, Total Reward: 0.5083265312767942\n",
            "reset\n",
            "Episode: 420, marco_f1: 0.9058338909836953, Total Reward: 0.5481371622097926\n",
            "reset\n",
            "Episode: 421, marco_f1: 0.8938773221844453, Total Reward: 0.5660888973767717\n",
            "reset\n",
            "Episode: 422, marco_f1: 0.9018582833611736, Total Reward: 0.4704986410649753\n",
            "reset\n",
            "Episode: 423, marco_f1: 0.9018582833611736, Total Reward: 0.4525206464198235\n",
            "reset\n",
            "Episode: 424, marco_f1: 0.9018582833611736, Total Reward: 0.3668720359305877\n",
            "reset\n",
            "Episode: 425, marco_f1: 0.8938773221844453, Total Reward: 0.5043520363828051\n",
            "reset\n",
            "Episode: 426, marco_f1: 0.8938773221844453, Total Reward: 0.5320707360722262\n",
            "reset\n",
            "Episode: 427, marco_f1: 0.8938773221844453, Total Reward: 0.46050170585197037\n",
            "reset\n",
            "Episode: 428, marco_f1: 0.9058338909836953, Total Reward: 0.5043804589385626\n",
            "reset\n",
            "Episode: 429, marco_f1: 0.9098091561744652, Total Reward: 0.5241640388136048\n",
            "reset\n",
            "Episode: 430, marco_f1: 0.9058338909836953, Total Reward: 0.5043513544526711\n",
            "reset\n",
            "Episode: 431, marco_f1: 0.8938773221844453, Total Reward: 0.5262730724248109\n",
            "reset\n",
            "Episode: 432, marco_f1: 0.8938773221844453, Total Reward: 0.5222685060187706\n",
            "reset\n",
            "Episode: 433, marco_f1: 0.9058338909836953, Total Reward: 0.5441896864576008\n",
            "reset\n",
            "Episode: 434, marco_f1: 0.8938773221844453, Total Reward: 0.49429373298103974\n",
            "reset\n",
            "Episode: 435, marco_f1: 0.9018582833611736, Total Reward: 0.46258088519202634\n",
            "reset\n",
            "Episode: 436, marco_f1: 0.9018582833611736, Total Reward: 0.49437802628393734\n",
            "reset\n",
            "Episode: 437, marco_f1: 0.8938773221844453, Total Reward: 0.4425829977722323\n",
            "reset\n",
            "Episode: 438, marco_f1: 0.9058338909836953, Total Reward: 0.5502154603538121\n",
            "reset\n",
            "Episode: 439, marco_f1: 0.8938773221844453, Total Reward: 0.5401862598078851\n",
            "reset\n",
            "Episode: 440, marco_f1: 0.9058338909836953, Total Reward: 0.49431993363872573\n",
            "reset\n",
            "Episode: 441, marco_f1: 0.8938773221844453, Total Reward: 0.5541320478571709\n",
            "reset\n",
            "Episode: 442, marco_f1: 0.9058338909836953, Total Reward: 0.5481686230560497\n",
            "reset\n",
            "Episode: 443, marco_f1: 0.8938773221844453, Total Reward: 0.3310088329294104\n",
            "reset\n",
            "Episode: 444, marco_f1: 0.8938773221844453, Total Reward: 0.4186323239687706\n",
            "reset\n",
            "Episode: 445, marco_f1: 0.8938773221844453, Total Reward: 0.5142554764216373\n",
            "reset\n",
            "Episode: 446, marco_f1: 0.8938773221844453, Total Reward: 0.5461499176286173\n",
            "reset\n",
            "Episode: 447, marco_f1: 0.9058338909836953, Total Reward: 0.5102790533357643\n",
            "reset\n",
            "Episode: 448, marco_f1: 0.9058338909836953, Total Reward: 0.5600620050203893\n",
            "reset\n",
            "Episode: 449, marco_f1: 0.8998397435897436, Total Reward: 0.516273794293203\n",
            "reset\n",
            "Episode: 450, marco_f1: 0.9058338909836953, Total Reward: 0.4624602907568518\n",
            "reset\n",
            "Episode: 451, marco_f1: 0.9058338909836953, Total Reward: 0.452553871248877\n",
            "reset\n",
            "Episode: 452, marco_f1: 0.9018582833611736, Total Reward: 0.5560898413804484\n",
            "reset\n",
            "Episode: 453, marco_f1: 0.8938773221844453, Total Reward: 0.3230195481045346\n",
            "reset\n",
            "Episode: 454, marco_f1: 0.8938773221844453, Total Reward: 0.5302496328515014\n",
            "reset\n",
            "Episode: 455, marco_f1: 0.8938773221844453, Total Reward: 0.5601215977369763\n",
            "reset\n",
            "Episode: 456, marco_f1: 0.8938773221844453, Total Reward: 0.5082635028428177\n",
            "reset\n",
            "Episode: 457, marco_f1: 0.8938773221844453, Total Reward: 0.5382004172981462\n",
            "reset\n",
            "Episode: 458, marco_f1: 0.9058338909836953, Total Reward: 0.526330454625642\n",
            "reset\n",
            "Episode: 459, marco_f1: 0.8998397435897436, Total Reward: 0.4545618248164258\n",
            "reset\n",
            "Episode: 460, marco_f1: 0.8938773221844453, Total Reward: 0.49838475356881784\n",
            "reset\n",
            "Episode: 461, marco_f1: 0.8978524990085683, Total Reward: 0.4425822848490234\n",
            "reset\n",
            "Episode: 462, marco_f1: 0.9058338909836953, Total Reward: 0.5281980645063755\n",
            "reset\n",
            "Episode: 463, marco_f1: 0.9058338909836953, Total Reward: 0.39873369283425875\n",
            "reset\n",
            "Episode: 464, marco_f1: 0.9058338909836953, Total Reward: 0.5302799752375874\n",
            "reset\n",
            "Episode: 465, marco_f1: 0.8938773221844453, Total Reward: 0.53618161238432\n",
            "reset\n",
            "Episode: 466, marco_f1: 0.9058338909836953, Total Reward: 0.5540994179590885\n",
            "reset\n",
            "Episode: 467, marco_f1: 0.9058338909836953, Total Reward: 0.5202814270935978\n",
            "reset\n",
            "Episode: 468, marco_f1: 0.8938773221844453, Total Reward: 0.5360890583999508\n",
            "reset\n",
            "Episode: 469, marco_f1: 0.9058338909836953, Total Reward: 0.5222087631664409\n",
            "reset\n",
            "Episode: 470, marco_f1: 0.9058338909836953, Total Reward: 0.554159176331678\n",
            "reset\n",
            "Episode: 471, marco_f1: 0.9058338909836953, Total Reward: 0.5161500786517964\n",
            "reset\n",
            "Episode: 472, marco_f1: 0.9058338909836953, Total Reward: 0.5282906857966585\n",
            "reset\n",
            "Episode: 473, marco_f1: 0.9058338909836953, Total Reward: 0.4703810359082842\n",
            "reset\n",
            "Episode: 474, marco_f1: 0.9058338909836953, Total Reward: 0.5481145815327109\n",
            "reset\n",
            "Episode: 475, marco_f1: 0.8938773221844453, Total Reward: 0.5122716688057048\n",
            "reset\n",
            "Episode: 476, marco_f1: 0.8938773221844453, Total Reward: 0.5161556419312411\n",
            "reset\n",
            "Episode: 477, marco_f1: 0.8938773221844453, Total Reward: 0.38093700799580854\n",
            "reset\n",
            "Episode: 478, marco_f1: 0.8938773221844453, Total Reward: 0.5382601756707357\n",
            "reset\n",
            "Episode: 479, marco_f1: 0.8938773221844453, Total Reward: 0.49434791974474723\n",
            "reset\n",
            "Episode: 480, marco_f1: 0.8938773221844453, Total Reward: 0.5402191227255786\n",
            "reset\n",
            "Episode: 481, marco_f1: 0.9058338909836953, Total Reward: 0.5740383977072429\n",
            "reset\n",
            "Episode: 482, marco_f1: 0.8938773221844453, Total Reward: 0.5322379958928478\n",
            "reset\n",
            "Episode: 483, marco_f1: 0.9058338909836953, Total Reward: 0.5182614375499918\n",
            "reset\n",
            "Episode: 484, marco_f1: 0.8938773221844453, Total Reward: 0.5462096760012068\n",
            "reset\n",
            "Episode: 485, marco_f1: 0.9058338909836953, Total Reward: 0.5441299280850113\n",
            "reset\n",
            "Episode: 486, marco_f1: 0.8938773221844453, Total Reward: 0.5561182890425233\n",
            "reset\n",
            "Episode: 487, marco_f1: 0.9058338909836953, Total Reward: 0.5382004172981462\n",
            "reset\n",
            "Episode: 488, marco_f1: 0.9058338909836953, Total Reward: 0.5162730745086453\n",
            "reset\n",
            "Episode: 489, marco_f1: 0.9058338909836953, Total Reward: 0.5241958420916137\n",
            "reset\n",
            "Episode: 490, marco_f1: 0.8938773221844453, Total Reward: 0.5362133383729734\n",
            "reset\n",
            "Episode: 491, marco_f1: 0.8938773221844453, Total Reward: 0.5262770032936627\n",
            "reset\n",
            "Episode: 492, marco_f1: 0.9038754225476218, Total Reward: 0.4565110394602212\n",
            "reset\n",
            "Episode: 493, marco_f1: 0.8998397435897436, Total Reward: 0.43450315195635014\n",
            "reset\n",
            "Episode: 494, marco_f1: 0.9058338909836953, Total Reward: 0.5222673875585995\n",
            "reset\n",
            "Episode: 495, marco_f1: 0.9058338909836953, Total Reward: 0.542207485766925\n",
            "reset\n",
            "Episode: 496, marco_f1: 0.9058338909836953, Total Reward: 0.5361804277545401\n",
            "reset\n",
            "Episode: 497, marco_f1: 0.8938773221844453, Total Reward: 0.5560280381481052\n",
            "reset\n",
            "Episode: 498, marco_f1: 0.9058338909836953, Total Reward: 0.5441299280850113\n",
            "reset\n",
            "Episode: 499, marco_f1: 0.9058338909836953, Total Reward: 0.5899703089866184\n"
          ]
        }
      ],
      "source": [
        "#training loop\n",
        "results = []\n",
        "for episode in range(episodes):\n",
        "    state = env.get_state(reset=True)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(max_step):\n",
        "        action = select_action(state)\n",
        "        observation, reward, marco_f1 = env.step(action)\n",
        "# '''\n",
        "#         done = terminated or truncated\n",
        "#         if terminated:\n",
        "#             next_state = None\n",
        "#         else:\n",
        "#             next_state = observation\n",
        "# '''\n",
        "        next_state = observation\n",
        "\n",
        "\n",
        "        replay_buffer.push(state, action, reward, next_state)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "\n",
        "        train()\n",
        "\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        target_net_state_dict = q_network.state_dict()\n",
        "        policy_net_state_dict = target_network.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_network.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        # if terminated or truncated:\n",
        "        #     break\n",
        "\n",
        "\n",
        "    results.append((episode, total_reward, marco_f1))\n",
        "    print(f\"Episode: {episode}, marco_f1: {marco_f1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    # # for debug - train() - just get some replay memory.\n",
        "    # if episode == 1:\n",
        "    #     print('break')\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwsGuzXZZ8UG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in q_network.state_dict():\n",
        "    print(param_tensor, \"\\t\", q_network.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvh3xuCMZ8UG"
      },
      "outputs": [],
      "source": [
        "#SAVING THE NETWORK.\n",
        "\n",
        "# Assume `model` is your neural network\n",
        "torch.save(q_network.state_dict(), '/Users/kai/Library/CloudStorage/OneDrive-HochschuleLuzern/HSLU/00_Thesis/kai_2024/notebook/qnetwork.pth')\n",
        "# Assume `model` is your neural network\n",
        "torch.save(target_network.state_dict(), '/Users/kai/Library/CloudStorage/OneDrive-HochschuleLuzern/HSLU/00_Thesis/kai_2024/notebook/target_network.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_L0kTFeZ8UG"
      },
      "outputs": [],
      "source": [
        "# LOADING THE NETWARK\n",
        "# model = QNetwork(*args, **kwargs)\n",
        "# model.load_state_dict(torch.load(PATH, weights_only=True))\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3_xwXyNZ8UG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxTyaGKxZ8UG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}