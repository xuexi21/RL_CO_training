{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCeWC9rX-bjU"
      },
      "source": [
        "<!-- ## prepare the data -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n4kS8vKR-bjV"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.datasets import make_moons as moon\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# define the classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e7oFEh4o-bjW"
      },
      "outputs": [],
      "source": [
        "# set the dataset.\n",
        "dataset = moon(5000, noise=0.3, random_state=42)\n",
        "X,y = dataset\n",
        "\n",
        "# split the training(labeled) as 10% of dataset\n",
        "X_l, X_ul, y_l, y_ul = train_test_split(X, y, test_size=0.8, random_state=0)\n",
        "\n",
        "\n",
        "# split the training(labeled) as 50% of  labeled dataset\n",
        "X_l_train, X_l_test, y_l_train, y_l_test = train_test_split(X_l, y_l, test_size=0.5, random_state=0)\n",
        "\n",
        "# 2-classifier\n",
        "clf_1 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"knn\", KNeighborsClassifier(n_neighbors=11))\n",
        "        ]\n",
        ")\n",
        "\n",
        "# clf 1\n",
        "clf_2 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"RF\", RandomForestClassifier())\n",
        "        ]\n",
        ")\n",
        "\n",
        "clfs = [clf_1,clf_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AJqps6K9-bjW"
      },
      "outputs": [],
      "source": [
        "# define ENV\n",
        "\n",
        "# for clustering the unlabeld data\n",
        "\n",
        "\n",
        "\n",
        "class Env():\n",
        "    def __init__(self, classifiers, input_ul_data, k, X_test, y_test, X_reset, y_reset):\n",
        "        # super().__init__\n",
        "\n",
        "        self.models = classifiers\n",
        "        # UN LABEL DATA\n",
        "        self.X_ul = input_ul_data\n",
        "        # define the evaluate data, later use for the reward\n",
        "        self.X_eval = X_test\n",
        "        self.y_eval = y_test\n",
        "        self.X_reset = X_reset\n",
        "        self.y_reset = y_reset\n",
        "        # cluster the data\n",
        "        self.action_size = k * 2  # because always cotraining has 2 classifiers, so action size is (K * 2) shape \n",
        "        self.kmeans = KMeans(n_clusters=k,  n_init=10)\n",
        "        self.cluster_label = self.kmeans.fit_predict(self.X_ul)\n",
        "        self.u_cluster_label = np.unique(self.cluster_label)\n",
        "        self.centroids = self.kmeans.cluster_centers_\n",
        "        self.observation_size = self.get_state(reset=True).shape[1]\n",
        "        self.prev_macro_f1 = 0.0\n",
        "\n",
        "    ##############################\n",
        "    # ###### FIX ###########    \n",
        "    ###############################\n",
        "    # update 2 clf\n",
        "    def train_2_clf(self, X, y):\n",
        "        for clf in self.models:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "    def get_state(self,reset=False):\n",
        "\n",
        "        if reset:\n",
        "            self.train_2_clf(self.X_reset, self.y_reset)\n",
        "            print(\"reset\")\n",
        "\n",
        "        #########################\n",
        "\n",
        "        ########fix#################\n",
        "        # only keep the most confidence of each classifier as the state views\n",
        "        ''' \n",
        "        # The state vector does not need to contain the complete \n",
        "        # class probability distribution for each cluster and classifier\n",
        "        # since the classifier’s largest confidence is enough to help the agents decision.”\n",
        "        '''\n",
        "        #########################\n",
        "        out_1 = self.models[0].predict_proba(self.centroids).max(axis=1)\n",
        "        out_2 = self.models[1].predict_proba(self.centroids).max(axis=1)\n",
        "        # state_proba = np.concatenate((out_1, out_2), axis=0)\n",
        "        ##############################\n",
        "        #### UPDATED 5-OCT-2024\n",
        "        ## When choosen the action neeed calculation: cluster( '//2'), classifer ('%2')\n",
        "        ###########\n",
        "        state_proba = [[out_1[i],out_2[i]] for i in range(len(out_1))]\n",
        "        return  torch.from_numpy(np.array(state_proba).flatten()).to(torch.float32).reshape(1, -1)\n",
        "\n",
        "\n",
        "    def get_f1(self):\n",
        "        classifier_weights = [clf.score(self.X_eval, self.y_eval) for clf in self.models]  # Weights based on validation accuracy\n",
        "        combined_probabilities = np.average(\n",
        "            [clf.predict_proba(self.X_eval) for clf in self.models],\n",
        "            axis=0,\n",
        "            weights=classifier_weights\n",
        "        )\n",
        "\n",
        "        # Get final predictions from combined probabilities\n",
        "        combined_predictions = np.argmax(combined_probabilities, axis=1)\n",
        "\n",
        "\n",
        "        # Calculate F1 scores per class (harmonic means)\n",
        "        precision, recall, f1_per_class, _ = precision_recall_fscore_support(self.y_eval, combined_predictions, average=None)\n",
        "\n",
        "        # Compute Macro-F1 as arithmetic mean of F1 scores\n",
        "        macro_f1 = np.mean(f1_per_class)\n",
        "\n",
        "        return macro_f1\n",
        "\n",
        "\n",
        "    ######\n",
        "    ######\n",
        "    def get_subset(self, action):\n",
        "        # choose subset\n",
        "        subset = self.X_ul[self.cluster_label == action]\n",
        "        return subset\n",
        "\n",
        "    def co_training(self, subset, clf_idx):\n",
        "\n",
        "        # Average class probabilities across classifiers\n",
        "        avg_probabilities = np.mean([clf.predict_proba(subset) for clf in self.models], axis=0)\n",
        "\n",
        "        # Assign the label with the highest average probability\n",
        "        y_ul_action = np.argmax(avg_probabilities, axis=1)\n",
        "\n",
        "        ###########################################\n",
        "        ########### update the label_set for traning\n",
        "        ###########################################\n",
        "        X_updated = np.concatenate((X_l_train, subset), axis=0)\n",
        "        y_updated = np.concatenate((y_l_train, y_ul_action), axis=0)\n",
        "\n",
        "        self.models[clf_idx].fit(X_updated, y_updated)\n",
        "\n",
        "        '''\n",
        "        # ## get posodu label\n",
        "        # clf_0_p_label = self.model_1.predict(subset)\n",
        "        # clf_1_p_label = self.model_2.predict(subset)\n",
        "\n",
        "        # ## get proba_\n",
        "        # clf_0_p_y = self.model_1.predict_proba(subset)\n",
        "        # clf_1_p_y = self.model_2.predict_proba(subset)\n",
        "\n",
        "        # #get the label size\n",
        "        # y_num = subset.shape[0]\n",
        "        # # set empty y  #type=ndarray\n",
        "        # y_ul_action = np.zeros(y_num,)\n",
        "\n",
        "        # #############\n",
        "        # # confidence_diff = 0\n",
        "        # # combine the lable from two classifier, choose the most conffidence\n",
        "        # for i in range(y_num):\n",
        "        #     if max(clf_0_p_y[i, ]) > max(clf_1_p_y[i, ]):\n",
        "        #         y_ul_action[i] = clf_0_p_label[i]\n",
        "        #         # print('0')\n",
        "        #     else:\n",
        "        #         y_ul_action[i] = clf_1_p_label[i]\n",
        "        #         # print('1')\n",
        "\n",
        "        # print(f'X shape is {X_updated.shape} \\ny shape is {y_updated.shape}')\n",
        "\n",
        "        ############# use the updated labeld dateset retrain those 2 classifier\n",
        "        # self.train_2_clf(X_updated, y_updated)\n",
        "\n",
        "        '''\n",
        "        # RETURN THE co-trained CLASSIFIER'S mean marcof1.\n",
        "        marco_f1 = self.get_f1()\n",
        "        return marco_f1\n",
        "\n",
        "    def step(self, action, clf_idx):\n",
        "        # GET THE bigining state accuracy, later use to calculate the reward\n",
        "        pre_marco_f1 = self.get_f1()\n",
        "\n",
        "        # choose subset\n",
        "        choosen_subset =  self.get_subset(action)\n",
        "\n",
        "        # cotraining the 2 classifier\n",
        "        new_marco_f1 = self.co_training(choosen_subset, clf_idx)\n",
        "\n",
        "        # get the next state_\n",
        "        n_state = self.get_state()\n",
        "\n",
        "        ##############\n",
        "        # calculate the reward\n",
        "        ##############\n",
        "        # if new_marco_f1 > pre_marco_f1:\n",
        "        #     reward_0 = new_marco_f1 - pre_marco_f1\n",
        "        # else:\n",
        "        #     reward_0 = 0\n",
        "\n",
        "        #########################\n",
        "\n",
        "        ########fix#############\n",
        "\n",
        "        #########################\n",
        "        reward_0 = new_marco_f1 - pre_marco_f1\n",
        "        return n_state, reward_0, new_marco_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AkP5ovW-bjX",
        "outputId": "eba85be5-304f-46ad-fe3c-a60b31227b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reset\n",
            "reset\n"
          ]
        }
      ],
      "source": [
        "k = 20\n",
        "env = Env(clfs, input_ul_data=X_ul, k=k, X_test=X_l_test, y_test=y_l_test, X_reset=X_l_train, y_reset=y_l_train)\n",
        "state_0 = env.get_state(reset=True)\n",
        "\n",
        "\n",
        "# state_0.shape\n",
        "# env.action_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuqYSBf7-bjX"
      },
      "source": [
        "<!-- #### DQN\n",
        "\n",
        "<img src=\"https://yinyoupoet.github.io/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_8coZ4g_pRtfyoHmsuzMH6g.png\" alt=\"Description of the image\" width=\"400\" height=\"300\">\n",
        "\n",
        "##### loss\n",
        "\n",
        "\n",
        "<img src=\"https://yinyoupoet.github.io/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_YCgMUijhU4p_y3sctvu-kQ.png\" alt=\"Description of the image\" width=\"300\" height=\"50\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vm4jSekd-bjX"
      },
      "outputs": [],
      "source": [
        "## RL functions##\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Define the Q-network (a simple feedforward neural network)\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        # # bug - some times the state is not\n",
        "        # if len(state) != 4:\n",
        "        #     state = state[0]\n",
        "        experience_tuple = (state, action, reward, next_state)\n",
        "        # Append experience_tuple to the memory buffer\n",
        "        self.memory.append(experience_tuple)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Draw a random sample of size batch_size\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        # Transform batch into a tuple of lists\n",
        "        states, actions, rewards, next_states = (zip(*batch))\n",
        "        return states, actions, rewards, next_states\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5XYUqe9H-bjY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "#\n",
        "EPS_START = 1\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "steps_done = 0\n",
        "\n",
        "####\n",
        "buffer_size = 10000\n",
        "###\n",
        "episodes = 500\n",
        "max_step = 100\n",
        "batch_size = 64\n",
        "TAU = 0.005\n",
        "gamma = 0.99\n",
        "\n",
        "# hyper parameter\n",
        "observation_size = env.observation_size\n",
        "action_size = env.action_size\n",
        "lr = 1e-4\n",
        "\n",
        "###\n",
        "# Initialize networks and optimizer\n",
        "q_network = QNetwork(observation_size, action_size)\n",
        "target_network = QNetwork(observation_size, action_size)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "# Replay memory\n",
        "replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return the largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            action = torch.argmax(q_network(state)).item()\n",
        "            return action\n",
        "    else:\n",
        "        return np.random.choice(range(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FSFEIzc7-bjY"
      },
      "outputs": [],
      "source": [
        "# Function to update the Q-network\n",
        "def train():\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "        # prepare the training data\n",
        "    states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "\n",
        "    rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "    actions = torch.tensor(np.array(actions)).unsqueeze(1)\n",
        "    states = torch.tensor(np.array(states)).squeeze(1)\n",
        "    next_states = torch.tensor(np.array(next_states)).squeeze(1)\n",
        "\n",
        "    # # Compute current Q values\n",
        "    q_values = q_network(states).gather(1, actions)\n",
        "\n",
        "\n",
        "\n",
        "        # Compute target Q values\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_network(next_states).max(1).values.unsqueeze(1)\n",
        "    targets = rewards + (gamma * next_q_values)\n",
        "\n",
        "\n",
        "        # Update the network\n",
        "    loss = loss_fn(q_values, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_haF2H6b-bjZ",
        "outputId": "2a9dbc31-8d98-4317-d09c-44d2104d87db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0, marco_f1: 0.9019117205484937, Total Reward: 0.010051870182419265\n",
            "Episode: 1, marco_f1: 0.901886580887506, Total Reward: -2.513966098771636e-05\n",
            "Episode: 2, marco_f1: 0.8998702318204392, Total Reward: -0.002016349067066714\n",
            "Episode: 3, marco_f1: 0.9038754225476218, Total Reward: 0.004005190727182528\n",
            "Episode: 4, marco_f1: 0.9018582833611736, Total Reward: -0.0020171391864481913\n",
            "Episode: 5, marco_f1: 0.897908117305575, Total Reward: -0.003950166055598614\n",
            "Episode: 6, marco_f1: 0.9038754225476218, Total Reward: 0.005967305242046805\n",
            "Episode: 7, marco_f1: 0.9038754225476218, Total Reward: 0.0\n",
            "Episode: 8, marco_f1: 0.9019525450317953, Total Reward: -0.0019228775158264266\n",
            "Episode: 9, marco_f1: 0.8998974950349158, Total Reward: -0.0020550499968795544\n",
            "Episode: 10, marco_f1: 0.8999215384861732, Total Reward: 2.4043451257393933e-05\n",
            "Episode: 11, marco_f1: 0.9058640677137787, Total Reward: 0.00594252922760552\n",
            "Episode: 12, marco_f1: 0.8978197540461373, Total Reward: -0.008044313667641356\n",
            "Episode: 13, marco_f1: 0.893734335839599, Total Reward: -0.004085418206538294\n",
            "Episode: 14, marco_f1: 0.8917071762044568, Total Reward: -0.002027159635142217\n",
            "Episode: 15, marco_f1: 0.9058640677137787, Total Reward: 0.014156891509321867\n",
            "Episode: 16, marco_f1: 0.8877413560844185, Total Reward: -0.01812271162936019\n",
            "Episode: 17, marco_f1: 0.9058640677137787, Total Reward: 0.01812271162936019\n",
            "Episode: 18, marco_f1: 0.8978197540461373, Total Reward: -0.008044313667641356\n",
            "Episode: 19, marco_f1: 0.8978819515359756, Total Reward: 6.219748983826712e-05\n",
            "Episode: 20, marco_f1: 0.9038754225476218, Total Reward: 0.0059934710116461565\n",
            "Episode: 21, marco_f1: 0.8957180215302177, Total Reward: -0.008157401017404031\n",
            "Episode: 22, marco_f1: 0.8999215384861732, Total Reward: 0.004203516955955444\n",
            "Episode: 23, marco_f1: 0.8917905064204299, Total Reward: -0.008131032065743282\n",
            "Episode: 24, marco_f1: 0.8938773221844453, Total Reward: 0.0020868157640153706\n",
            "Episode: 25, marco_f1: 0.9019117205484937, Total Reward: 0.0080343983640484\n",
            "Episode: 26, marco_f1: 0.9038754225476218, Total Reward: 0.001963701999128098\n",
            "Episode: 27, marco_f1: 0.9059364130151982, Total Reward: 0.0020609904675764623\n",
            "Episode: 28, marco_f1: 0.9019117205484937, Total Reward: -0.00402469246670456\n",
            "Episode: 29, marco_f1: 0.9139225302772496, Total Reward: 0.0120108097287559\n",
            "Episode: 30, marco_f1: 0.9039246769467262, Total Reward: -0.009997853330523387\n",
            "Episode: 31, marco_f1: 0.8999215384861732, Total Reward: -0.004003138460553002\n",
            "Episode: 32, marco_f1: 0.9059364130151982, Total Reward: 0.006014874529025049\n",
            "Episode: 33, marco_f1: 0.9058912102390363, Total Reward: -4.5202776161934466e-05\n",
            "Episode: 34, marco_f1: 0.8999215384861732, Total Reward: -0.005969671752863115\n",
            "Episode: 35, marco_f1: 0.9039015952335191, Total Reward: 0.003980056747345917\n",
            "Episode: 36, marco_f1: 0.9058338909836953, Total Reward: 0.0019322957501761717\n",
            "Episode: 37, marco_f1: 0.9039015952335191, Total Reward: -0.0019322957501761717\n",
            "Episode: 38, marco_f1: 0.8958650410932569, Total Reward: -0.008036554140262164\n",
            "Episode: 39, marco_f1: 0.9079056954321225, Total Reward: 0.012040654338865586\n",
            "Episode: 40, marco_f1: 0.9018582833611736, Total Reward: -0.006047412070948943\n",
            "Episode: 41, marco_f1: 0.8957982654418956, Total Reward: -0.006060017919278016\n",
            "Episode: 42, marco_f1: 0.9099189270343309, Total Reward: 0.014120661592435302\n",
            "Episode: 43, marco_f1: 0.8958933948363124, Total Reward: -0.014025532198018498\n",
            "Episode: 44, marco_f1: 0.9079056954321225, Total Reward: 0.012012300595810155\n",
            "Episode: 45, marco_f1: 0.9059153237914123, Total Reward: -0.0019903716407102534\n",
            "Episode: 46, marco_f1: 0.9039246769467262, Total Reward: -0.0019906468446860837\n",
            "Episode: 47, marco_f1: 0.9039015952335191, Total Reward: -2.3081713207084853e-05\n",
            "Episode: 48, marco_f1: 0.9058912102390363, Total Reward: 0.001989615005517198\n",
            "Episode: 49, marco_f1: 0.8958333333333334, Total Reward: -0.010057876905702923\n",
            "Episode: 50, marco_f1: 0.9038137834848265, Total Reward: 0.00798045015149318\n",
            "Episode: 51, marco_f1: 0.9078525641025641, Total Reward: 0.004038780617737547\n",
            "Episode: 52, marco_f1: 0.8979669412889777, Total Reward: -0.009885622813586403\n",
            "Episode: 53, marco_f1: 0.8978524990085683, Total Reward: -0.00011444228040935478\n",
            "Episode: 54, marco_f1: 0.897908117305575, Total Reward: 5.561829700662102e-05\n",
            "Episode: 55, marco_f1: 0.901886580887506, Total Reward: 0.003978463581930991\n",
            "Episode: 56, marco_f1: 0.9058912102390363, Total Reward: 0.004004629351530342\n",
            "Episode: 57, marco_f1: 0.9018582833611736, Total Reward: -0.004032926877862719\n",
            "Episode: 58, marco_f1: 0.8978524990085683, Total Reward: -0.004005784352605235\n",
            "Episode: 59, marco_f1: 0.8898409303033581, Total Reward: -0.008011568705210248\n",
            "Episode: 60, marco_f1: 0.9038754225476218, Total Reward: 0.014034492244263674\n",
            "Episode: 61, marco_f1: 0.9018268225149164, Total Reward: -0.00204860003270535\n",
            "Episode: 62, marco_f1: 0.9079469774590163, Total Reward: 0.006120154944099898\n",
            "Episode: 63, marco_f1: 0.901886580887506, Total Reward: -0.0060603965715103625\n",
            "Episode: 64, marco_f1: 0.8938467146559632, Total Reward: -0.008039866231542736\n",
            "Episode: 65, marco_f1: 0.9039015952335191, Total Reward: 0.01005488057755588\n",
            "Episode: 66, marco_f1: 0.8978819515359756, Total Reward: -0.006019643697543486\n",
            "Episode: 67, marco_f1: 0.8938126855773585, Total Reward: -0.004069265958617119\n",
            "Episode: 68, marco_f1: 0.8917505933671178, Total Reward: -0.0020620922102406736\n",
            "Episode: 69, marco_f1: 0.8999215384861732, Total Reward: 0.008170945119055362\n",
            "Episode: 70, marco_f1: 0.8978524990085683, Total Reward: -0.0020690394776048393\n",
            "Episode: 71, marco_f1: 0.9058640677137787, Total Reward: 0.008011568705210359\n",
            "Episode: 72, marco_f1: 0.8998702318204392, Total Reward: -0.005993835893339461\n",
            "Episode: 73, marco_f1: 0.8938467146559632, Total Reward: -0.006023517164476022\n",
            "Episode: 74, marco_f1: 0.9079056954321225, Total Reward: 0.014058980776159302\n",
            "Episode: 75, marco_f1: 0.9098409594524741, Total Reward: 0.0019352640203516014\n",
            "Episode: 76, marco_f1: 0.9078806132748041, Total Reward: -0.001960346177670047\n",
            "Episode: 77, marco_f1: 0.891889294637709, Total Reward: -0.01599131863709502\n",
            "Episode: 78, marco_f1: 0.9038461538461539, Total Reward: 0.011956859208444803\n",
            "Episode: 79, marco_f1: 0.9118858040019866, Total Reward: 0.008039650155832745\n",
            "Episode: 80, marco_f1: 0.9018582833611736, Total Reward: -0.010027520640813026\n",
            "Episode: 81, marco_f1: 0.8998702318204392, Total Reward: -0.001988051540734337\n",
            "Episode: 82, marco_f1: 0.9018582833611736, Total Reward: 0.001988051540734337\n",
            "Episode: 83, marco_f1: 0.8998974950349158, Total Reward: -0.0019607883262577896\n",
            "Episode: 84, marco_f1: 0.8978524990085683, Total Reward: -0.0020449960263474454\n",
            "Episode: 85, marco_f1: 0.9039446721311475, Total Reward: 0.0060921731225791476\n",
            "Episode: 86, marco_f1: 0.8977837103310605, Total Reward: -0.0061609618000869615\n",
            "Episode: 87, marco_f1: 0.8958650410932569, Total Reward: -0.001918669237803594\n",
            "Episode: 88, marco_f1: 0.8959400614754098, Total Reward: 7.502038215290163e-05\n",
            "Episode: 89, marco_f1: 0.9038461538461539, Total Reward: 0.007906092370744022\n",
            "Episode: 90, marco_f1: 0.8877413560844185, Total Reward: -0.016104797761735345\n",
            "Episode: 91, marco_f1: 0.9079278154072793, Total Reward: 0.020186459322860784\n",
            "Episode: 92, marco_f1: 0.893775228383259, Total Reward: -0.014152587024020336\n",
            "Episode: 93, marco_f1: 0.8978197540461373, Total Reward: 0.004044525662878384\n",
            "Episode: 94, marco_f1: 0.9018582833611736, Total Reward: 0.004038529315036232\n",
            "Episode: 95, marco_f1: 0.9119492827868853, Total Reward: 0.010090999425711678\n",
            "Episode: 96, marco_f1: 0.9019117205484937, Total Reward: -0.010037562238391584\n",
            "Episode: 97, marco_f1: 0.8958333333333334, Total Reward: -0.006078387215160297\n",
            "Episode: 98, marco_f1: 0.9038754225476218, Total Reward: 0.008042089214288395\n",
            "Episode: 99, marco_f1: 0.8998974950349158, Total Reward: -0.003977927512705981\n",
            "Episode: 100, marco_f1: 0.9019525450317953, Total Reward: 0.0020550499968795544\n",
            "Episode: 101, marco_f1: 0.8958933948363124, Total Reward: -0.006059150195482976\n",
            "Episode: 102, marco_f1: 0.897908117305575, Total Reward: 0.0020147224692625976\n",
            "Episode: 103, marco_f1: 0.9058912102390363, Total Reward: 0.007983092933461333\n",
            "Episode: 104, marco_f1: 0.9018582833611736, Total Reward: -0.004032926877862719\n",
            "Episode: 105, marco_f1: 0.8998974950349158, Total Reward: -0.0019607883262577896\n",
            "Episode: 106, marco_f1: 0.9058912102390363, Total Reward: 0.005993715204120509\n",
            "Episode: 107, marco_f1: 0.901886580887506, Total Reward: -0.004004629351530342\n",
            "Episode: 108, marco_f1: 0.8979506080943176, Total Reward: -0.003935972793188358\n",
            "Episode: 109, marco_f1: 0.9039246769467262, Total Reward: 0.005974068852408587\n",
            "Episode: 110, marco_f1: 0.8997690679325165, Total Reward: -0.004155609014209638\n",
            "Episode: 111, marco_f1: 0.9058912102390363, Total Reward: 0.006122142306519751\n",
            "Episode: 112, marco_f1: 0.9019117205484937, Total Reward: -0.003979489690542626\n",
            "Episode: 113, marco_f1: 0.897908117305575, Total Reward: -0.004003603242918707\n",
            "Episode: 114, marco_f1: 0.8938467146559632, Total Reward: -0.004061402649611745\n",
            "Episode: 115, marco_f1: 0.8938126855773585, Total Reward: -3.4029078604724816e-05\n",
            "Episode: 116, marco_f1: 0.8958933948363124, Total Reward: 0.0020807092589538723\n",
            "Episode: 117, marco_f1: 0.8978819515359756, Total Reward: 0.0019885566996632464\n",
            "Episode: 118, marco_f1: 0.901886580887506, Total Reward: 0.004004629351530342\n",
            "Episode: 119, marco_f1: 0.8958933948363124, Total Reward: -0.005993186051193589\n",
            "Episode: 120, marco_f1: 0.8918269230769231, Total Reward: -0.004066471759389234\n",
            "Episode: 121, marco_f1: 0.901886580887506, Total Reward: 0.010059657810582823\n",
            "Episode: 122, marco_f1: 0.8978819515359756, Total Reward: -0.004004629351530342\n",
            "Episode: 123, marco_f1: 0.8998974950349158, Total Reward: 0.0020155434989401755\n",
            "Episode: 124, marco_f1: 0.8999215384861732, Total Reward: 2.4043451257393933e-05\n",
            "Episode: 125, marco_f1: 0.8978819515359756, Total Reward: -0.0020395869501975694\n",
            "Episode: 126, marco_f1: 0.8978197540461373, Total Reward: -6.219748983826712e-05\n",
            "Episode: 127, marco_f1: 0.8958650410932569, Total Reward: -0.001954712952880411\n",
            "Episode: 128, marco_f1: 0.8978524990085683, Total Reward: 0.001987457915311408\n",
            "Episode: 129, marco_f1: 0.897908117305575, Total Reward: 5.561829700662102e-05\n",
            "Episode: 130, marco_f1: 0.8938773221844453, Total Reward: -0.004030795121129693\n",
            "Episode: 131, marco_f1: 0.9019117205484937, Total Reward: 0.0080343983640484\n",
            "Episode: 132, marco_f1: 0.9058912102390363, Total Reward: 0.003979489690542626\n",
            "Episode: 133, marco_f1: 0.8998974950349158, Total Reward: -0.005993715204120509\n",
            "Episode: 134, marco_f1: 0.8998397435897436, Total Reward: -5.775144517217168e-05\n",
            "Episode: 135, marco_f1: 0.8958650410932569, Total Reward: -0.003974702496486682\n",
            "Episode: 136, marco_f1: 0.8977837103310605, Total Reward: 0.001918669237803594\n",
            "Episode: 137, marco_f1: 0.8958650410932569, Total Reward: -0.001918669237803594\n",
            "Episode: 138, marco_f1: 0.9079278154072793, Total Reward: 0.012062774314022362\n",
            "Episode: 139, marco_f1: 0.9018582833611736, Total Reward: -0.006069532046105719\n",
            "Episode: 140, marco_f1: 0.8978197540461373, Total Reward: -0.004038529315036232\n",
            "Episode: 141, marco_f1: 0.8958333333333334, Total Reward: -0.0019864207128039713\n",
            "Episode: 142, marco_f1: 0.9039015952335191, Total Reward: 0.008068261900185725\n",
            "Episode: 143, marco_f1: 0.9058640677137787, Total Reward: 0.0019624724802596027\n",
            "Episode: 144, marco_f1: 0.9038461538461539, Total Reward: -0.002017913867624843\n",
            "Episode: 145, marco_f1: 0.9038754225476218, Total Reward: 2.9268701467910496e-05\n",
            "Episode: 146, marco_f1: 0.8957982654418956, Total Reward: -0.008077157105726207\n",
            "Episode: 147, marco_f1: 0.8978197540461373, Total Reward: 0.0020214886042417834\n",
            "Episode: 148, marco_f1: 0.9018268225149164, Total Reward: 0.004007068468779074\n",
            "Episode: 149, marco_f1: 0.9018268225149164, Total Reward: 0.0\n",
            "Episode: 150, marco_f1: 0.8998974950349158, Total Reward: -0.001929327480000631\n",
            "Episode: 151, marco_f1: 0.8938126855773585, Total Reward: -0.006084809457557294\n",
            "Episode: 152, marco_f1: 0.9017921922788621, Total Reward: 0.007979506701503603\n",
            "Episode: 153, marco_f1: 0.9038461538461539, Total Reward: 0.002053961567291762\n",
            "Episode: 154, marco_f1: 0.8998397435897436, Total Reward: -0.004006410256410242\n",
            "Episode: 155, marco_f1: 0.9079056954321225, Total Reward: 0.008065951842378905\n",
            "Episode: 156, marco_f1: 0.9038754225476218, Total Reward: -0.004030272884500752\n",
            "Episode: 157, marco_f1: 0.8958650410932569, Total Reward: -0.008010381454364834\n",
            "Episode: 158, marco_f1: 0.9039246769467262, Total Reward: 0.00805963585346925\n",
            "Episode: 159, marco_f1: 0.8918269230769231, Total Reward: -0.012097753869803052\n",
            "Episode: 160, marco_f1: 0.8978819515359756, Total Reward: 0.0060550284590524805\n",
            "Episode: 161, marco_f1: 0.9058912102390363, Total Reward: 0.008009258703060684\n",
            "Episode: 162, marco_f1: 0.9058338909836953, Total Reward: -5.731925534102622e-05\n",
            "Episode: 163, marco_f1: 0.8998974950349158, Total Reward: -0.005936395948779483\n",
            "Episode: 164, marco_f1: 0.891889294637709, Total Reward: -0.008008200397206733\n",
            "Episode: 165, marco_f1: 0.8978197540461373, Total Reward: 0.00593045940842829\n",
            "Episode: 166, marco_f1: 0.8998397435897436, Total Reward: 0.002019989543606271\n",
            "Episode: 167, marco_f1: 0.897908117305575, Total Reward: -0.0019316262841686527\n",
            "Episode: 168, marco_f1: 0.8938467146559632, Total Reward: -0.004061402649611745\n",
            "Episode: 169, marco_f1: 0.9079469774590163, Total Reward: 0.014100262803053099\n",
            "Episode: 170, marco_f1: 0.8978524990085683, Total Reward: -0.010094478450447975\n",
            "Episode: 171, marco_f1: 0.9079469774590163, Total Reward: 0.010094478450447975\n",
            "Episode: 172, marco_f1: 0.9098698520663839, Total Reward: 0.0019228746073676195\n",
            "Episode: 173, marco_f1: 0.8958650410932569, Total Reward: -0.014004810973127002\n",
            "Episode: 174, marco_f1: 0.8978819515359756, Total Reward: 0.002016910442718678\n",
            "Episode: 175, marco_f1: 0.8998397435897436, Total Reward: 0.001957792053768004\n",
            "Episode: 176, marco_f1: 0.8938773221844453, Total Reward: -0.005962421405298346\n",
            "Episode: 177, marco_f1: 0.8958650410932569, Total Reward: 0.0019877189088116642\n",
            "Episode: 178, marco_f1: 0.9099189270343309, Total Reward: 0.01405388594107393\n",
            "Episode: 179, marco_f1: 0.8999215384861732, Total Reward: -0.009997388548157682\n",
            "Episode: 180, marco_f1: 0.9018268225149164, Total Reward: 0.001905284028743237\n",
            "Episode: 181, marco_f1: 0.9019117205484937, Total Reward: 8.489803357725201e-05\n",
            "Episode: 182, marco_f1: 0.9079056954321225, Total Reward: 0.00599397488362885\n",
            "Episode: 183, marco_f1: 0.899806024463361, Total Reward: -0.008099670968761519\n",
            "Episode: 184, marco_f1: 0.9038754225476218, Total Reward: 0.004069398084260767\n",
            "Episode: 185, marco_f1: 0.9039015952335191, Total Reward: 2.6172685897329906e-05\n",
            "Episode: 186, marco_f1: 0.8958933948363124, Total Reward: -0.008008200397206733\n",
            "Episode: 187, marco_f1: 0.9058338909836953, Total Reward: 0.009940496147382905\n",
            "Episode: 188, marco_f1: 0.9079056954321225, Total Reward: 0.00207180444842725\n",
            "Episode: 189, marco_f1: 0.901886580887506, Total Reward: -0.006019114544616566\n",
            "Episode: 190, marco_f1: 0.9079469774590163, Total Reward: 0.0060603965715103625\n",
            "Episode: 191, marco_f1: 0.8978197540461373, Total Reward: -0.010127223412878972\n",
            "Episode: 192, marco_f1: 0.9099391188443388, Total Reward: 0.012119364798201415\n",
            "Episode: 193, marco_f1: 0.9019337071860578, Total Reward: -0.008005411658280948\n",
            "Episode: 194, marco_f1: 0.9038137834848265, Total Reward: 0.0018800762987687403\n",
            "Episode: 195, marco_f1: 0.9019117205484937, Total Reward: -0.0019020629363328823\n",
            "Episode: 196, marco_f1: 0.8958333333333334, Total Reward: -0.006078387215160297\n",
            "Episode: 197, marco_f1: 0.8958650410932569, Total Reward: 3.170775992356045e-05\n",
            "Episode: 198, marco_f1: 0.8978524990085683, Total Reward: 0.001987457915311408\n",
            "Episode: 199, marco_f1: 0.8958933948363124, Total Reward: -0.0019591041722559766\n",
            "Episode: 200, marco_f1: 0.8938467146559632, Total Reward: -0.0020466801803491474\n",
            "Episode: 201, marco_f1: 0.8958333333333334, Total Reward: 0.0019866186773701555\n",
            "Episode: 202, marco_f1: 0.9038461538461539, Total Reward: 0.008012820512820484\n",
            "Episode: 203, marco_f1: 0.8938773221844453, Total Reward: -0.009968831661708588\n",
            "Episode: 204, marco_f1: 0.8998397435897436, Total Reward: 0.005962421405298346\n",
            "Episode: 205, marco_f1: 0.8978524990085683, Total Reward: -0.0019872445811752737\n",
            "Episode: 206, marco_f1: 0.8998974950349158, Total Reward: 0.0020449960263474454\n",
            "Episode: 207, marco_f1: 0.8978524990085683, Total Reward: -0.0020449960263474454\n",
            "Episode: 208, marco_f1: 0.897908117305575, Total Reward: 5.561829700662102e-05\n",
            "Episode: 209, marco_f1: 0.9019117205484937, Total Reward: 0.004003603242918707\n",
            "Episode: 210, marco_f1: 0.9019117205484937, Total Reward: 0.0\n",
            "Episode: 211, marco_f1: 0.8938126855773585, Total Reward: -0.008099034971135177\n",
            "Episode: 212, marco_f1: 0.9079469774590163, Total Reward: 0.014134291881657823\n",
            "Episode: 213, marco_f1: 0.8999215384861732, Total Reward: -0.008025438972843135\n",
            "Episode: 214, marco_f1: 0.8957982654418956, Total Reward: -0.00412327304427762\n",
            "Episode: 215, marco_f1: 0.8998974950349158, Total Reward: 0.004099229593020226\n",
            "Episode: 216, marco_f1: 0.9099564189067508, Total Reward: 0.010058923871835046\n",
            "Episode: 217, marco_f1: 0.901886580887506, Total Reward: -0.00806983801924488\n",
            "Episode: 218, marco_f1: 0.8958933948363124, Total Reward: -0.005993186051193589\n",
            "Episode: 219, marco_f1: 0.9059153237914123, Total Reward: 0.010021928955099901\n",
            "Episode: 220, marco_f1: 0.8978819515359756, Total Reward: -0.008033372255436655\n",
            "Episode: 221, marco_f1: 0.9059544819692731, Total Reward: 0.008072530433297476\n",
            "Episode: 222, marco_f1: 0.9079056954321225, Total Reward: 0.0019512134628494326\n",
            "Episode: 223, marco_f1: 0.9058912102390363, Total Reward: -0.002014485193086224\n",
            "Episode: 224, marco_f1: 0.9039015952335191, Total Reward: -0.001989615005517198\n",
            "Episode: 225, marco_f1: 0.9019117205484937, Total Reward: -0.001989874685025428\n",
            "Episode: 226, marco_f1: 0.9038754225476218, Total Reward: 0.001963701999128098\n",
            "Episode: 227, marco_f1: 0.8958933948363124, Total Reward: -0.007982027711309403\n",
            "Episode: 228, marco_f1: 0.8978197540461373, Total Reward: 0.0019263592098249793\n",
            "Episode: 229, marco_f1: 0.8998974950349158, Total Reward: 0.0020777409887784426\n",
            "Episode: 230, marco_f1: 0.9038461538461539, Total Reward: 0.0039486588112380705\n",
            "Episode: 231, marco_f1: 0.8998974950349158, Total Reward: -0.0039486588112380705\n",
            "Episode: 232, marco_f1: 0.8998397435897436, Total Reward: -5.775144517217168e-05\n",
            "Episode: 233, marco_f1: 0.9078525641025641, Total Reward: 0.008012820512820484\n",
            "Episode: 234, marco_f1: 0.9078806132748041, Total Reward: 2.8049172239974496e-05\n",
            "Episode: 235, marco_f1: 0.9058338909836953, Total Reward: -0.0020467222911088045\n",
            "Episode: 236, marco_f1: 0.9059153237914123, Total Reward: 8.143280771699679e-05\n",
            "Episode: 237, marco_f1: 0.9039246769467262, Total Reward: -0.0019906468446860837\n",
            "Episode: 238, marco_f1: 0.9039246769467262, Total Reward: 0.0\n",
            "Episode: 239, marco_f1: 0.9038754225476218, Total Reward: -4.925439910441476e-05\n",
            "Episode: 240, marco_f1: 0.8998974950349158, Total Reward: -0.003977927512705981\n",
            "Episode: 241, marco_f1: 0.9019337071860578, Total Reward: 0.002036212151142025\n",
            "Episode: 242, marco_f1: 0.9019337071860578, Total Reward: 0.0\n",
            "Episode: 243, marco_f1: 0.9018268225149164, Total Reward: -0.000106884671141394\n",
            "Episode: 244, marco_f1: 0.9059153237914123, Total Reward: 0.004088501276495848\n",
            "Episode: 245, marco_f1: 0.9019117205484937, Total Reward: -0.004003603242918596\n",
            "Episode: 246, marco_f1: 0.8998397435897436, Total Reward: -0.0020719769587500547\n",
            "Episode: 247, marco_f1: 0.9039015952335191, Total Reward: 0.004061851643775483\n",
            "Episode: 248, marco_f1: 0.9058338909836953, Total Reward: 0.0019322957501761717\n",
            "Episode: 249, marco_f1: 0.9018582833611736, Total Reward: -0.003975607622521693\n",
            "Episode: 250, marco_f1: 0.9039446721311475, Total Reward: 0.0020863887699739125\n",
            "Episode: 251, marco_f1: 0.9038754225476218, Total Reward: -6.924958352572119e-05\n",
            "Episode: 252, marco_f1: 0.9039015952335191, Total Reward: 2.6172685897329906e-05\n",
            "Episode: 253, marco_f1: 0.8958333333333334, Total Reward: -0.008068261900185725\n",
            "Episode: 254, marco_f1: 0.8957982654418956, Total Reward: -3.506789143781219e-05\n",
            "Episode: 255, marco_f1: 0.899806024463361, Total Reward: 0.00400775902146544\n",
            "Episode: 256, marco_f1: 0.9039015952335191, Total Reward: 0.004095570770158097\n",
            "Episode: 257, marco_f1: 0.901886580887506, Total Reward: -0.0020150143460131442\n",
            "Episode: 258, marco_f1: 0.8978197540461373, Total Reward: -0.004066826841368609\n",
            "Episode: 259, marco_f1: 0.8938773221844453, Total Reward: -0.003942431861692075\n",
            "Episode: 260, marco_f1: 0.9039015952335191, Total Reward: 0.010024273049073829\n",
            "Episode: 261, marco_f1: 0.9039246769467262, Total Reward: 2.3081713207084853e-05\n",
            "Episode: 262, marco_f1: 0.9018268225149164, Total Reward: -0.0020978544318097647\n",
            "Episode: 263, marco_f1: 0.9019117205484937, Total Reward: 8.489803357725201e-05\n",
            "Episode: 264, marco_f1: 0.9098958395905667, Total Reward: 0.007984119042073079\n",
            "Episode: 265, marco_f1: 0.9079469774590163, Total Reward: -0.0019488621315504329\n",
            "Episode: 266, marco_f1: 0.899806024463361, Total Reward: -0.008140952995655315\n",
            "Episode: 267, marco_f1: 0.9078525641025641, Total Reward: 0.008046539639203099\n",
            "Episode: 268, marco_f1: 0.901886580887506, Total Reward: -0.005965983215058146\n",
            "Episode: 269, marco_f1: 0.9039015952335191, Total Reward: 0.0020150143460131442\n",
            "Episode: 270, marco_f1: 0.8959400614754098, Total Reward: -0.007961533758109263\n",
            "Episode: 271, marco_f1: 0.9078215425062922, Total Reward: 0.01188148103088238\n",
            "Episode: 272, marco_f1: 0.897908117305575, Total Reward: -0.009913425200717252\n",
            "Episode: 273, marco_f1: 0.8917505933671178, Total Reward: -0.006157523938457143\n",
            "Episode: 274, marco_f1: 0.8999215384861732, Total Reward: 0.008170945119055362\n",
            "Episode: 275, marco_f1: 0.9038754225476218, Total Reward: 0.003953884061448587\n",
            "Episode: 276, marco_f1: 0.8938773221844453, Total Reward: -0.009998100363176499\n",
            "Episode: 277, marco_f1: 0.8978197540461373, Total Reward: 0.003942431861692075\n",
            "Episode: 278, marco_f1: 0.897908117305575, Total Reward: 8.836325943761825e-05\n",
            "Episode: 279, marco_f1: 0.9058912102390363, Total Reward: 0.007983092933461333\n",
            "Episode: 280, marco_f1: 0.8998702318204392, Total Reward: -0.006020978418597056\n",
            "Episode: 281, marco_f1: 0.9079469774590163, Total Reward: 0.008076745638577076\n",
            "Episode: 282, marco_f1: 0.8898056171085795, Total Reward: -0.018141360350436786\n",
            "Episode: 283, marco_f1: 0.8917905064204299, Total Reward: 0.0019848893118503685\n",
            "Episode: 284, marco_f1: 0.8998702318204392, Total Reward: 0.008079725400009341\n",
            "Episode: 285, marco_f1: 0.8938126855773585, Total Reward: -0.006057546243080747\n",
            "Episode: 286, marco_f1: 0.9038137834848265, Total Reward: 0.01000109790746806\n",
            "Episode: 287, marco_f1: 0.9018582833611736, Total Reward: -0.0019555001236529757\n",
            "Episode: 288, marco_f1: 0.9059153237914123, Total Reward: 0.00405704043023869\n",
            "Episode: 289, marco_f1: 0.9099189270343309, Total Reward: 0.004003603242918596\n",
            "Episode: 290, marco_f1: 0.9119309538678324, Total Reward: 0.0020120268335015457\n",
            "Episode: 291, marco_f1: 0.9098958395905667, Total Reward: -0.0020351142772656594\n",
            "Episode: 292, marco_f1: 0.8999215384861732, Total Reward: -0.009974301104393568\n",
            "Episode: 293, marco_f1: 0.8958933948363124, Total Reward: -0.004028143649860816\n",
            "Episode: 294, marco_f1: 0.9079278154072793, Total Reward: 0.01203442057096693\n",
            "Episode: 295, marco_f1: 0.9039015952335191, Total Reward: -0.004026220173760198\n",
            "Episode: 296, marco_f1: 0.9059153237914123, Total Reward: 0.0020137285578931685\n",
            "Episode: 297, marco_f1: 0.9038461538461539, Total Reward: -0.002069169945258409\n",
            "Episode: 298, marco_f1: 0.9099564189067508, Total Reward: 0.0061102650605969755\n",
            "Episode: 299, marco_f1: 0.9079469774590163, Total Reward: -0.002009441447734517\n",
            "Episode: 300, marco_f1: 0.8978819515359756, Total Reward: -0.010065025923040705\n",
            "Episode: 301, marco_f1: 0.9039015952335191, Total Reward: 0.006019643697543486\n",
            "Episode: 302, marco_f1: 0.8958650410932569, Total Reward: -0.008036554140262164\n",
            "Episode: 303, marco_f1: 0.8918269230769231, Total Reward: -0.004038118016333803\n",
            "Episode: 304, marco_f1: 0.9038461538461539, Total Reward: 0.012019230769230727\n",
            "Episode: 305, marco_f1: 0.8958333333333334, Total Reward: -0.008012820512820484\n",
            "Episode: 306, marco_f1: 0.8978197540461373, Total Reward: 0.0019864207128039713\n",
            "Episode: 307, marco_f1: 0.8998974950349158, Total Reward: 0.0020777409887784426\n",
            "Episode: 308, marco_f1: 0.9059544819692731, Total Reward: 0.0060569869343573\n",
            "Episode: 309, marco_f1: 0.9058640677137787, Total Reward: -9.041425549438653e-05\n",
            "Episode: 310, marco_f1: 0.9079056954321225, Total Reward: 0.002041627718343819\n",
            "Episode: 311, marco_f1: 0.9059544819692731, Total Reward: -0.0019512134628494326\n",
            "Episode: 312, marco_f1: 0.9039015952335191, Total Reward: -0.0020528867357539893\n",
            "Episode: 313, marco_f1: 0.9018268225149164, Total Reward: -0.00207477271860268\n",
            "Episode: 314, marco_f1: 0.8978524990085683, Total Reward: -0.003974323506348076\n",
            "Episode: 315, marco_f1: 0.9018582833611736, Total Reward: 0.004005784352605235\n",
            "Episode: 316, marco_f1: 0.9058640677137787, Total Reward: 0.004005784352605124\n",
            "Episode: 317, marco_f1: 0.9059153237914123, Total Reward: 5.1256077633565766e-05\n",
            "Episode: 318, marco_f1: 0.9059544819692731, Total Reward: 3.9158177860820764e-05\n",
            "Episode: 319, marco_f1: 0.8978524990085683, Total Reward: -0.008101982960704746\n",
            "Episode: 320, marco_f1: 0.8938126855773585, Total Reward: -0.004039813431209849\n",
            "Episode: 321, marco_f1: 0.8938467146559632, Total Reward: 3.4029078604724816e-05\n",
            "Episode: 322, marco_f1: 0.9038754225476218, Total Reward: 0.01002870789165855\n",
            "Episode: 323, marco_f1: 0.8998974950349158, Total Reward: -0.003977927512705981\n",
            "Episode: 324, marco_f1: 0.8938467146559632, Total Reward: -0.006050780378952569\n",
            "Episode: 325, marco_f1: 0.8957598306498171, Total Reward: 0.0019131159938539088\n",
            "Episode: 326, marco_f1: 0.8998974950349158, Total Reward: 0.0041376643850986605\n",
            "Episode: 327, marco_f1: 0.901886580887506, Total Reward: 0.0019890858525901667\n",
            "Episode: 328, marco_f1: 0.8978524990085683, Total Reward: -0.004034081878937612\n",
            "Episode: 329, marco_f1: 0.8938467146559632, Total Reward: -0.004005784352605124\n",
            "Episode: 330, marco_f1: 0.9038754225476218, Total Reward: 0.01002870789165855\n",
            "Episode: 331, marco_f1: 0.9078215425062922, Total Reward: 0.0039461199586704465\n",
            "Episode: 332, marco_f1: 0.8958650410932569, Total Reward: -0.011956501413035281\n",
            "Episode: 333, marco_f1: 0.9038461538461539, Total Reward: 0.007981112752896924\n",
            "Episode: 334, marco_f1: 0.9059153237914123, Total Reward: 0.002069169945258409\n",
            "Episode: 335, marco_f1: 0.8998974950349158, Total Reward: -0.006017828756496479\n",
            "Episode: 336, marco_f1: 0.9079469774590163, Total Reward: 0.00804948242410053\n",
            "Episode: 337, marco_f1: 0.8998974950349158, Total Reward: -0.00804948242410053\n",
            "Episode: 338, marco_f1: 0.897908117305575, Total Reward: -0.0019893777293408244\n",
            "Episode: 339, marco_f1: 0.8958933948363124, Total Reward: -0.0020147224692625976\n",
            "Episode: 340, marco_f1: 0.8958933948363124, Total Reward: 0.0\n",
            "Episode: 341, marco_f1: 0.8999215384861732, Total Reward: 0.004028143649860816\n",
            "Episode: 342, marco_f1: 0.9019117205484937, Total Reward: 0.001990182062320489\n",
            "Episode: 343, marco_f1: 0.8918269230769231, Total Reward: -0.010084797471570539\n",
            "Episode: 344, marco_f1: 0.8918269230769231, Total Reward: 0.0\n",
            "Episode: 345, marco_f1: 0.8998974950349158, Total Reward: 0.008070571957992656\n",
            "Episode: 346, marco_f1: 0.8958933948363124, Total Reward: -0.004004100198603422\n",
            "Episode: 347, marco_f1: 0.8978524990085683, Total Reward: 0.0019591041722559766\n",
            "Episode: 348, marco_f1: 0.9058640677137787, Total Reward: 0.008011568705210359\n",
            "Episode: 349, marco_f1: 0.8998397435897436, Total Reward: -0.006024324124035085\n",
            "Episode: 350, marco_f1: 0.8999215384861732, Total Reward: 8.179489642956561e-05\n",
            "Episode: 351, marco_f1: 0.9038461538461539, Total Reward: 0.0039246153599806766\n",
            "Episode: 352, marco_f1: 0.8918598503660744, Total Reward: -0.011986303480079452\n",
            "Episode: 353, marco_f1: 0.901886580887506, Total Reward: 0.010026730521431548\n",
            "Episode: 354, marco_f1: 0.9019337071860578, Total Reward: 4.712629855185835e-05\n",
            "Episode: 355, marco_f1: 0.8898409303033581, Total Reward: -0.012092776882699718\n",
            "Episode: 356, marco_f1: 0.8999215384861732, Total Reward: 0.010080608182815087\n",
            "Episode: 357, marco_f1: 0.8998397435897436, Total Reward: -8.179489642956561e-05\n",
            "Episode: 358, marco_f1: 0.9079631852741097, Total Reward: 0.008123441684366073\n",
            "Episode: 359, marco_f1: 0.9058338909836953, Total Reward: -0.0021292942904144185\n",
            "Episode: 360, marco_f1: 0.9059544819692731, Total Reward: 0.00012059098557781756\n",
            "Episode: 361, marco_f1: 0.9079278154072793, Total Reward: 0.0019733334380062084\n",
            "Episode: 362, marco_f1: 0.9019337071860578, Total Reward: -0.005994108221221484\n",
            "Episode: 363, marco_f1: 0.9099391188443388, Total Reward: 0.008005411658280948\n",
            "Episode: 364, marco_f1: 0.9018582833611736, Total Reward: -0.008080835483165183\n",
            "Episode: 365, marco_f1: 0.8939282955277768, Total Reward: -0.007929987833396823\n",
            "Episode: 366, marco_f1: 0.8999215384861732, Total Reward: 0.0059932429583964275\n",
            "Episode: 367, marco_f1: 0.9017921922788621, Total Reward: 0.0018706537926889144\n",
            "Episode: 368, marco_f1: 0.9119097956307258, Total Reward: 0.010117603351863735\n",
            "Episode: 369, marco_f1: 0.8918269230769231, Total Reward: -0.0200828725538027\n",
            "Episode: 370, marco_f1: 0.9059364130151982, Total Reward: 0.0141094899382751\n",
            "Episode: 371, marco_f1: 0.8977837103310605, Total Reward: -0.008152702684137703\n",
            "Episode: 372, marco_f1: 0.8978197540461373, Total Reward: 3.6043715076816696e-05\n",
            "Episode: 373, marco_f1: 0.8998397435897436, Total Reward: 0.002019989543606271\n",
            "Episode: 374, marco_f1: 0.8938126855773585, Total Reward: -0.0060270580123851225\n",
            "Episode: 375, marco_f1: 0.9039015952335191, Total Reward: 0.010088909656160605\n",
            "Episode: 376, marco_f1: 0.8999423668032787, Total Reward: -0.003959228430240436\n",
            "Episode: 377, marco_f1: 0.9099391188443388, Total Reward: 0.009996752041060097\n",
            "Episode: 378, marco_f1: 0.8998702318204392, Total Reward: -0.01006888702389952\n",
            "Episode: 379, marco_f1: 0.8978197540461373, Total Reward: -0.0020504777743018954\n",
            "Episode: 380, marco_f1: 0.901886580887506, Total Reward: 0.004066826841368609\n",
            "Episode: 381, marco_f1: 0.901886580887506, Total Reward: 0.0\n",
            "Episode: 382, marco_f1: 0.9038754225476218, Total Reward: 0.0019888416601158143\n",
            "Episode: 383, marco_f1: 0.8998974950349158, Total Reward: -0.003977927512705981\n",
            "Episode: 384, marco_f1: 0.9038754225476218, Total Reward: 0.003977927512705981\n",
            "Episode: 385, marco_f1: 0.9059153237914123, Total Reward: 0.0020399012437904984\n",
            "Episode: 386, marco_f1: 0.9039246769467262, Total Reward: -0.0019906468446860837\n",
            "Episode: 387, marco_f1: 0.8998702318204392, Total Reward: -0.004054445126286943\n",
            "Episode: 388, marco_f1: 0.9059364130151982, Total Reward: 0.0060661811947589905\n",
            "Episode: 389, marco_f1: 0.8998974950349158, Total Reward: -0.006038917980282443\n",
            "Episode: 390, marco_f1: 0.9079056954321225, Total Reward: 0.008008200397206733\n",
            "Episode: 391, marco_f1: 0.8917505933671178, Total Reward: -0.0161551020650047\n",
            "Episode: 392, marco_f1: 0.8918598503660744, Total Reward: 0.00010925699895658614\n",
            "Episode: 393, marco_f1: 0.8938126855773585, Total Reward: 0.0019528352112840874\n",
            "Episode: 394, marco_f1: 0.9039015952335191, Total Reward: 0.010088909656160605\n",
            "Episode: 395, marco_f1: 0.8999215384861732, Total Reward: -0.003980056747345917\n",
            "Episode: 396, marco_f1: 0.9078215425062922, Total Reward: 0.007900004020119034\n",
            "Episode: 397, marco_f1: 0.9038754225476218, Total Reward: -0.0039461199586704465\n",
            "Episode: 398, marco_f1: 0.8998702318204392, Total Reward: -0.004005190727182528\n",
            "Episode: 399, marco_f1: 0.897908117305575, Total Reward: -0.001962114514864277\n",
            "Episode: 400, marco_f1: 0.9038461538461539, Total Reward: 0.005938036540578895\n",
            "Episode: 401, marco_f1: 0.9079056954321225, Total Reward: 0.004059541585968662\n",
            "Episode: 402, marco_f1: 0.8979506080943176, Total Reward: -0.009955087337804924\n",
            "Episode: 403, marco_f1: 0.8978819515359756, Total Reward: -6.865655834198403e-05\n",
            "Episode: 404, marco_f1: 0.8958650410932569, Total Reward: -0.002016910442718678\n",
            "Episode: 405, marco_f1: 0.8898409303033581, Total Reward: -0.00602411078989884\n",
            "Episode: 406, marco_f1: 0.8998702318204392, Total Reward: 0.010029301517081146\n",
            "Episode: 407, marco_f1: 0.8978524990085683, Total Reward: -0.002017732811870898\n",
            "Episode: 408, marco_f1: 0.9017921922788621, Total Reward: 0.003939693270293754\n",
            "Episode: 409, marco_f1: 0.9159138958293291, Total Reward: 0.014121703550467046\n",
            "Episode: 410, marco_f1: 0.8978819515359756, Total Reward: -0.01803194429335353\n",
            "Episode: 411, marco_f1: 0.8978197540461373, Total Reward: -6.219748983826712e-05\n",
            "Episode: 412, marco_f1: 0.9018582833611736, Total Reward: 0.004038529315036232\n",
            "Episode: 413, marco_f1: 0.9038461538461539, Total Reward: 0.001987870484980281\n",
            "Episode: 414, marco_f1: 0.9019117205484937, Total Reward: -0.0019344332976601875\n",
            "Episode: 415, marco_f1: 0.899806024463361, Total Reward: -0.002105696085132669\n",
            "Episode: 416, marco_f1: 0.8917905064204299, Total Reward: -0.008015518042931102\n",
            "Episode: 417, marco_f1: 0.8958650410932569, Total Reward: 0.004074534672827035\n",
            "Episode: 418, marco_f1: 0.8957982654418956, Total Reward: -6.677565136137265e-05\n",
            "Episode: 419, marco_f1: 0.8978819515359756, Total Reward: 0.0020836860940800506\n",
            "Episode: 420, marco_f1: 0.9019117205484937, Total Reward: 0.0040297690125180585\n",
            "Episode: 421, marco_f1: 0.9058912102390363, Total Reward: 0.003979489690542626\n",
            "Episode: 422, marco_f1: 0.8938773221844453, Total Reward: -0.012013888054591026\n",
            "Episode: 423, marco_f1: 0.9078525641025641, Total Reward: 0.01397524191811883\n",
            "Episode: 424, marco_f1: 0.901886580887506, Total Reward: -0.005965983215058146\n",
            "Episode: 425, marco_f1: 0.9079469774590163, Total Reward: 0.0060603965715103625\n",
            "Episode: 426, marco_f1: 0.9039246769467262, Total Reward: -0.004022300512290133\n",
            "Episode: 427, marco_f1: 0.9078806132748041, Total Reward: 0.003955936328077891\n",
            "Episode: 428, marco_f1: 0.9079278154072793, Total Reward: 4.7202132475221426e-05\n",
            "Episode: 429, marco_f1: 0.901886580887506, Total Reward: -0.006041234519773342\n",
            "Episode: 430, marco_f1: 0.8978524990085683, Total Reward: -0.004034081878937612\n",
            "Episode: 431, marco_f1: 0.9078215425062922, Total Reward: 0.009969043497723873\n",
            "Episode: 432, marco_f1: 0.9038754225476218, Total Reward: -0.0039461199586704465\n",
            "Episode: 433, marco_f1: 0.9058912102390363, Total Reward: 0.002015787691414528\n",
            "Episode: 434, marco_f1: 0.8978819515359756, Total Reward: -0.008009258703060684\n",
            "Episode: 435, marco_f1: 0.9038754225476218, Total Reward: 0.0059934710116461565\n",
            "Episode: 436, marco_f1: 0.9038461538461539, Total Reward: -2.9268701467910496e-05\n",
            "Episode: 437, marco_f1: 0.9058640677137787, Total Reward: 0.002017913867624843\n",
            "Episode: 438, marco_f1: 0.901886580887506, Total Reward: -0.003977486826272747\n",
            "Episode: 439, marco_f1: 0.9039246769467262, Total Reward: 0.002038096059220229\n",
            "Episode: 440, marco_f1: 0.8998702318204392, Total Reward: -0.004054445126286943\n",
            "Episode: 441, marco_f1: 0.9079056954321225, Total Reward: 0.00803546361168328\n",
            "Episode: 442, marco_f1: 0.8978197540461373, Total Reward: -0.010085941385985175\n",
            "Episode: 443, marco_f1: 0.9038461538461539, Total Reward: 0.006026399800016513\n",
            "Episode: 444, marco_f1: 0.9017921922788621, Total Reward: -0.002053961567291762\n",
            "Episode: 445, marco_f1: 0.9018268225149164, Total Reward: 3.4630236054322694e-05\n",
            "Episode: 446, marco_f1: 0.8978197540461373, Total Reward: -0.004007068468779074\n",
            "Episode: 447, marco_f1: 0.9059364130151982, Total Reward: 0.008116658969060886\n",
            "Episode: 448, marco_f1: 0.8938126855773585, Total Reward: -0.012123727437839737\n",
            "Episode: 449, marco_f1: 0.9039246769467262, Total Reward: 0.01011199136936769\n",
            "Episode: 450, marco_f1: 0.9039015952335191, Total Reward: -2.3081713207084853e-05\n",
            "Episode: 451, marco_f1: 0.8957982654418956, Total Reward: -0.008103329791623537\n",
            "Episode: 452, marco_f1: 0.9059153237914123, Total Reward: 0.010117058349516705\n",
            "Episode: 453, marco_f1: 0.9038137834848265, Total Reward: -0.002101540306585714\n",
            "Episode: 454, marco_f1: 0.901886580887506, Total Reward: -0.0019272025973205986\n",
            "Episode: 455, marco_f1: 0.9038754225476218, Total Reward: 0.0019888416601158143\n",
            "Episode: 456, marco_f1: 0.9079056954321225, Total Reward: 0.004030272884500752\n",
            "Episode: 457, marco_f1: 0.9039246769467262, Total Reward: -0.003981018485396337\n",
            "Episode: 458, marco_f1: 0.8938467146559632, Total Reward: -0.010077962290762965\n",
            "Episode: 459, marco_f1: 0.9058640677137787, Total Reward: 0.012017353057815483\n",
            "Episode: 460, marco_f1: 0.9039246769467262, Total Reward: -0.001939390767052518\n",
            "Episode: 461, marco_f1: 0.9079278154072793, Total Reward: 0.004003138460553113\n",
            "Episode: 462, marco_f1: 0.8998702318204392, Total Reward: -0.008057583586840056\n",
            "Episode: 463, marco_f1: 0.8978197540461373, Total Reward: -0.0020504777743018954\n",
            "Episode: 464, marco_f1: 0.8997690679325165, Total Reward: 0.0019493138863792003\n",
            "Episode: 465, marco_f1: 0.9038461538461539, Total Reward: 0.004077085913637313\n",
            "Episode: 466, marco_f1: 0.8877827473989643, Total Reward: -0.01606340644718951\n",
            "Episode: 467, marco_f1: 0.9039246769467262, Total Reward: 0.016141929547761835\n",
            "Episode: 468, marco_f1: 0.9019117205484937, Total Reward: -0.0020129563982325127\n",
            "Episode: 469, marco_f1: 0.8958333333333334, Total Reward: -0.006078387215160297\n",
            "Episode: 470, marco_f1: 0.9018582833611736, Total Reward: 0.0060249500278402035\n",
            "Episode: 471, marco_f1: 0.8838141025641025, Total Reward: -0.01804418079707104\n",
            "Episode: 472, marco_f1: 0.8957982654418956, Total Reward: 0.011984162877793025\n",
            "Episode: 473, marco_f1: 0.9099189270343309, Total Reward: 0.014120661592435302\n",
            "Episode: 474, marco_f1: 0.897908117305575, Total Reward: -0.0120108097287559\n",
            "Episode: 475, marco_f1: 0.8998974950349158, Total Reward: 0.0019893777293408244\n",
            "Episode: 476, marco_f1: 0.901886580887506, Total Reward: 0.0019890858525901667\n",
            "Episode: 477, marco_f1: 0.8978197540461373, Total Reward: -0.004066826841368609\n",
            "Episode: 478, marco_f1: 0.8998974950349158, Total Reward: 0.0020777409887784426\n",
            "Episode: 479, marco_f1: 0.901886580887506, Total Reward: 0.0019890858525901667\n",
            "Episode: 480, marco_f1: 0.9038754225476218, Total Reward: 0.0019888416601158143\n",
            "Episode: 481, marco_f1: 0.9098958395905667, Total Reward: 0.006020417042944981\n",
            "Episode: 482, marco_f1: 0.8917905064204299, Total Reward: -0.01810533317013685\n",
            "Episode: 483, marco_f1: 0.8978197540461373, Total Reward: 0.006029247625707446\n",
            "Episode: 484, marco_f1: 0.9038754225476218, Total Reward: 0.006055668501484424\n",
            "Episode: 485, marco_f1: 0.8978524990085683, Total Reward: -0.006022923539053426\n",
            "Episode: 486, marco_f1: 0.8998702318204392, Total Reward: 0.002017732811870898\n",
            "Episode: 487, marco_f1: 0.9139004689420971, Total Reward: 0.014030237121657851\n",
            "Episode: 488, marco_f1: 0.8978197540461373, Total Reward: -0.016080714895959747\n",
            "Episode: 489, marco_f1: 0.897908117305575, Total Reward: 8.836325943761825e-05\n",
            "Episode: 490, marco_f1: 0.9017921922788621, Total Reward: 0.0038840749732871327\n",
            "Episode: 491, marco_f1: 0.9078806132748041, Total Reward: 0.006088420995941979\n",
            "Episode: 492, marco_f1: 0.8998702318204392, Total Reward: -0.008010381454364834\n",
            "Episode: 493, marco_f1: 0.8978524990085683, Total Reward: -0.002017732811870898\n",
            "Episode: 494, marco_f1: 0.901886580887506, Total Reward: 0.004034081878937612\n",
            "Episode: 495, marco_f1: 0.9058338909836953, Total Reward: 0.003947310096189316\n",
            "Episode: 496, marco_f1: 0.8998974950349158, Total Reward: -0.005936395948779483\n",
            "Episode: 497, marco_f1: 0.901886580887506, Total Reward: 0.0019890858525901667\n",
            "Episode: 498, marco_f1: 0.9059695341290579, Total Reward: 0.004082953241551945\n",
            "Episode: 499, marco_f1: 0.8978524990085683, Total Reward: -0.008117035120489557\n"
          ]
        }
      ],
      "source": [
        "#training loop\n",
        "results = []\n",
        "for episode in range(episodes):\n",
        "    # Change random seed for regularization purposes (randomness in training)\n",
        "    seed = random.randint(0, 10000)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    \n",
        "    state = env.get_state()\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(max_step):\n",
        "        # Agent chooses an action (which cluster and which classifier to train)\n",
        "        action = select_action(state)\n",
        "        action_cluster_idx = action // 2  # co trainig normally use 2 classifier,  fdin wich cluster todo\n",
        "        action_clf_idx = action % 2  # find witch classifier to retrain with the new psoduolabel\n",
        "\n",
        "        observation, reward, marco_f1 = env.step(action_cluster_idx, action_clf_idx)\n",
        "# '''\n",
        "#         done = terminated or truncated\n",
        "#         if terminated:\n",
        "#             next_state = None\n",
        "#         else:\n",
        "#             next_state = observation\n",
        "# '''\n",
        "        next_state = observation\n",
        "\n",
        "\n",
        "        replay_buffer.push(state, action, reward, next_state)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        train()\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        target_net_state_dict = q_network.state_dict()\n",
        "        policy_net_state_dict = target_network.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_network.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        # if terminated or truncated:\n",
        "        #     break\n",
        "\n",
        "    results.append((episode, total_reward, marco_f1))\n",
        "    print(f\"Episode: {episode}, marco_f1: {marco_f1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    # # for debug - train() - just get some replay memory.\n",
        "    # if episode == 1:\n",
        "    #     print('break')\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "fc1.weight \t torch.Size([128, 40])\n",
            "fc1.bias \t torch.Size([128])\n",
            "fc2.weight \t torch.Size([128, 128])\n",
            "fc2.bias \t torch.Size([128])\n",
            "fc3.weight \t torch.Size([40, 128])\n",
            "fc3.bias \t torch.Size([40])\n",
            "Optimizer's state_dict:\n",
            "state \t {0: {'step': tensor(49937.), 'exp_avg': tensor([[-1.3373e-06, -1.4110e-06, -1.2477e-06,  ..., -1.7410e-06,\n",
            "         -1.7315e-06, -1.6749e-06],\n",
            "        [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
            "          5.6052e-45,  5.6052e-45],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
            "          5.6052e-45,  5.6052e-45],\n",
            "        [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
            "          5.6052e-45,  5.6052e-45],\n",
            "        [-9.2238e-07, -4.5835e-07, -9.5820e-07,  ..., -1.0556e-06,\n",
            "         -1.1746e-06, -1.1986e-06]]), 'exp_avg_sq': tensor([[2.5861e-10, 2.0127e-10, 2.4081e-10,  ..., 3.6000e-10, 4.0082e-10,\n",
            "         3.9616e-10],\n",
            "        [1.2869e-35, 1.8842e-35, 7.7851e-36,  ..., 1.6627e-35, 1.9224e-35,\n",
            "         1.8842e-35],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        ...,\n",
            "        [1.6572e-14, 1.1932e-14, 1.3922e-14,  ..., 2.1512e-14, 2.4081e-14,\n",
            "         2.3757e-14],\n",
            "        [6.0464e-15, 4.1829e-15, 4.9586e-15,  ..., 7.6643e-15, 8.6163e-15,\n",
            "         8.5103e-15],\n",
            "        [1.3056e-10, 1.1213e-10, 1.3972e-10,  ..., 1.9494e-10, 2.1861e-10,\n",
            "         2.1613e-10]])}, 1: {'step': tensor(49937.), 'exp_avg': tensor([-1.7315e-06,  5.6052e-45,  0.0000e+00,  0.0000e+00,  5.6052e-45,\n",
            "        -2.5381e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -9.1151e-06,  1.3304e-05,  6.4190e-07,  0.0000e+00,  1.0136e-06,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  2.2111e-06,  0.0000e+00,\n",
            "         0.0000e+00, -1.2508e-06,  5.6052e-45,  0.0000e+00,  0.0000e+00,\n",
            "         5.6052e-45,  7.3215e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         5.2189e-06,  5.6052e-45,  5.6052e-45,  5.6052e-45,  0.0000e+00,\n",
            "         3.8791e-06,  0.0000e+00,  0.0000e+00,  5.6052e-45,  0.0000e+00,\n",
            "         0.0000e+00,  5.6052e-45,  0.0000e+00, -5.6242e-08,  5.6052e-45,\n",
            "         0.0000e+00,  0.0000e+00,  5.6052e-45,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  1.0405e-05,  0.0000e+00,  1.2106e-05,  0.0000e+00,\n",
            "         5.6052e-45,  1.0677e-05,  5.6052e-45,  5.6052e-45,  0.0000e+00,\n",
            "         5.6052e-45,  5.1040e-06,  5.6052e-45,  0.0000e+00, -1.0471e-06,\n",
            "         2.2562e-06,  5.6052e-45,  5.6052e-45,  0.0000e+00,  0.0000e+00,\n",
            "         1.6611e-06,  8.6612e-07,  5.6052e-45,  0.0000e+00,  0.0000e+00,\n",
            "        -5.7379e-07,  0.0000e+00,  9.5932e-06,  9.4011e-06,  5.6052e-45,\n",
            "         4.4038e-06,  4.7775e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         6.0735e-06,  1.8166e-06,  6.6692e-06,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         2.1462e-22,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.9795e-06,\n",
            "         5.6052e-45,  7.0588e-06,  5.9478e-06,  0.0000e+00,  5.6052e-45,\n",
            "         9.4705e-06,  0.0000e+00,  0.0000e+00,  1.3407e-05,  0.0000e+00,\n",
            "         2.4756e-06,  0.0000e+00,  5.6052e-45,  0.0000e+00,  5.6052e-45,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         1.7792e-05,  9.5685e-07,  0.0000e+00,  6.6046e-06,  0.0000e+00,\n",
            "         5.6052e-45,  5.6052e-45, -1.1746e-06]), 'exp_avg_sq': tensor([4.0082e-10, 1.9224e-35, 0.0000e+00, 0.0000e+00, 3.6114e-24, 9.7830e-11,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3030e-09, 2.9110e-09,\n",
            "        2.9841e-10, 0.0000e+00, 1.9116e-10, 1.6773e-21, 2.4555e-25, 4.1300e-20,\n",
            "        6.5532e-10, 0.0000e+00, 0.0000e+00, 1.9379e-10, 3.6676e-13, 0.0000e+00,\n",
            "        0.0000e+00, 4.6733e-30, 1.1981e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        7.1305e-10, 4.0059e-21, 3.2165e-32, 2.4734e-29, 0.0000e+00, 5.3965e-10,\n",
            "        0.0000e+00, 0.0000e+00, 1.0123e-16, 0.0000e+00, 0.0000e+00, 3.6167e-21,\n",
            "        0.0000e+00, 3.6027e-10, 1.1430e-36, 0.0000e+00, 0.0000e+00, 1.8552e-13,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5177e-09, 0.0000e+00, 1.3576e-09,\n",
            "        0.0000e+00, 2.9571e-17, 1.4884e-09, 8.4877e-23, 1.4881e-25, 0.0000e+00,\n",
            "        3.6364e-22, 4.3383e-10, 4.0322e-18, 0.0000e+00, 2.0813e-10, 2.5265e-10,\n",
            "        4.9273e-24, 1.0181e-15, 0.0000e+00, 0.0000e+00, 6.1001e-10, 2.2903e-10,\n",
            "        6.3254e-13, 0.0000e+00, 0.0000e+00, 1.8460e-10, 0.0000e+00, 1.4184e-09,\n",
            "        8.6140e-10, 1.5215e-13, 3.9535e-10, 8.1799e-11, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 3.2742e-10, 3.4667e-10, 7.8922e-10, 2.1075e-30, 1.7623e-21,\n",
            "        2.4837e-21, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2965e-12,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7199e-10, 2.6931e-23, 1.3585e-09,\n",
            "        4.3800e-10, 0.0000e+00, 7.5705e-13, 1.3634e-09, 0.0000e+00, 0.0000e+00,\n",
            "        1.6077e-09, 0.0000e+00, 5.5926e-10, 0.0000e+00, 7.8730e-29, 0.0000e+00,\n",
            "        6.4739e-13, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        1.9974e-09, 8.8181e-10, 0.0000e+00, 5.9978e-10, 0.0000e+00, 2.4081e-14,\n",
            "        8.6163e-15, 2.1861e-10])}, 2: {'step': tensor(49937.), 'exp_avg': tensor([[-2.6974e-07,  5.6052e-45,  0.0000e+00,  ..., -5.6052e-45,\n",
            "          5.6052e-45,  3.2430e-08],\n",
            "        [ 5.6052e-45,  0.0000e+00,  0.0000e+00,  ...,  5.6052e-45,\n",
            "          5.6052e-45,  5.6052e-45],\n",
            "        [ 5.6052e-45,  0.0000e+00,  0.0000e+00,  ...,  5.6052e-45,\n",
            "          5.6052e-45,  5.6052e-45],\n",
            "        ...,\n",
            "        [-7.8066e-08,  5.6052e-45,  0.0000e+00,  ..., -5.6052e-45,\n",
            "          5.6052e-45, -5.4568e-07],\n",
            "        [ 5.6052e-45,  0.0000e+00,  0.0000e+00,  ...,  5.6052e-45,\n",
            "          5.6052e-45,  5.6052e-45],\n",
            "        [ 5.6052e-45,  5.6052e-45,  0.0000e+00,  ...,  5.6052e-45,\n",
            "          5.6052e-45,  5.6052e-45]]), 'exp_avg_sq': tensor([[4.2251e-11, 1.4041e-37, 0.0000e+00,  ..., 5.4274e-15, 1.9392e-15,\n",
            "         1.6089e-11],\n",
            "        [4.6099e-37, 0.0000e+00, 0.0000e+00,  ..., 1.4632e-37, 1.4070e-35,\n",
            "         1.4219e-35],\n",
            "        [3.1068e-36, 0.0000e+00, 0.0000e+00,  ..., 2.4508e-37, 6.9921e-35,\n",
            "         8.2917e-35],\n",
            "        ...,\n",
            "        [2.4388e-11, 1.9325e-40, 0.0000e+00,  ..., 2.3032e-15, 1.0375e-15,\n",
            "         1.0325e-11],\n",
            "        [3.9633e-26, 0.0000e+00, 0.0000e+00,  ..., 1.0457e-26, 3.6191e-26,\n",
            "         6.3998e-25],\n",
            "        [3.3592e-20, 3.4450e-39, 0.0000e+00,  ..., 1.0968e-20, 2.4738e-20,\n",
            "         1.5000e-19]])}, 3: {'step': tensor(49937.), 'exp_avg': tensor([ 1.2514e-06,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         0.0000e+00,  5.6052e-45,  5.6052e-45,  3.4798e-05,  0.0000e+00,\n",
            "         0.0000e+00,  5.6052e-45,  5.6052e-45,  0.0000e+00,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  1.8093e-05,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  1.5569e-05,  5.6052e-45,  3.8554e-06,  8.6850e-07,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  0.0000e+00,  0.0000e+00,\n",
            "         1.1494e-05,  5.6052e-45,  5.6052e-45, -1.1201e-06,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  0.0000e+00,  5.6052e-45,  0.0000e+00,\n",
            "        -5.6052e-45,  0.0000e+00,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         7.5930e-06,  5.6052e-45,  0.0000e+00,  0.0000e+00,  1.7032e-05,\n",
            "         5.6052e-45,  1.8056e-05,  0.0000e+00,  5.6052e-45,  3.0101e-05,\n",
            "         5.6052e-45,  1.0613e-05,  5.6052e-45,  0.0000e+00,  2.6819e-05,\n",
            "         5.6052e-45,  2.3806e-05,  0.0000e+00,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  2.5445e-05,  5.6052e-45,  5.6052e-45,  1.9850e-05,\n",
            "        -9.9177e-06,  2.1120e-05,  5.6052e-45,  5.6052e-45,  0.0000e+00,\n",
            "         5.6052e-45,  1.5059e-05,  5.6052e-45,  5.6052e-45,  0.0000e+00,\n",
            "         2.0516e-05,  5.6052e-45,  5.6052e-45,  5.6052e-45,  0.0000e+00,\n",
            "         5.6052e-45,  4.5307e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         1.4119e-05,  2.9697e-09,  5.6052e-45,  3.0039e-05,  0.0000e+00,\n",
            "         1.3560e-05,  2.2530e-05,  5.6052e-45,  3.5356e-05,  5.6052e-45,\n",
            "         5.6052e-45,  2.5244e-05,  5.6052e-45,  8.7091e-06,  5.6052e-45,\n",
            "         5.6052e-45,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  5.6052e-45,  5.6052e-45,  0.0000e+00,  5.6052e-45,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -6.9776e-06,  5.6052e-45,\n",
            "         2.3806e-05,  5.6052e-45,  5.6052e-45,  5.6052e-45,  0.0000e+00,\n",
            "        -2.3789e-06,  5.6052e-45,  5.6052e-45]), 'exp_avg_sq': tensor([5.6955e-09, 1.2248e-35, 4.8926e-35, 5.3735e-13, 3.1465e-15, 0.0000e+00,\n",
            "        1.1277e-28, 7.0045e-29, 7.3274e-09, 0.0000e+00, 0.0000e+00, 1.6482e-11,\n",
            "        1.5803e-17, 0.0000e+00, 7.5974e-28, 1.5418e-16, 8.9872e-25, 8.4682e-09,\n",
            "        5.4439e-14, 5.1379e-31, 1.2070e-17, 5.8652e-09, 3.8150e-16, 4.7542e-09,\n",
            "        6.8759e-09, 2.2688e-16, 3.6110e-19, 1.6970e-19, 0.0000e+00, 0.0000e+00,\n",
            "        8.5635e-09, 6.9538e-29, 1.0508e-23, 5.4011e-09, 1.0388e-17, 7.5095e-17,\n",
            "        1.8088e-21, 0.0000e+00, 9.4958e-18, 0.0000e+00, 3.1586e-23, 0.0000e+00,\n",
            "        6.2857e-15, 9.5752e-18, 5.6218e-15, 5.0903e-09, 3.3512e-18, 0.0000e+00,\n",
            "        0.0000e+00, 4.0260e-09, 3.8446e-32, 3.8899e-09, 0.0000e+00, 6.5063e-13,\n",
            "        8.5395e-09, 1.7942e-29, 2.6135e-09, 6.4382e-13, 0.0000e+00, 6.1578e-09,\n",
            "        1.0709e-20, 2.5789e-09, 0.0000e+00, 1.5759e-19, 3.7610e-18, 2.4044e-21,\n",
            "        1.0364e-08, 1.3380e-15, 1.1519e-19, 8.5568e-09, 4.5032e-09, 4.3199e-09,\n",
            "        5.0090e-24, 5.0277e-27, 0.0000e+00, 4.6204e-29, 4.6550e-09, 9.5502e-24,\n",
            "        1.2945e-22, 0.0000e+00, 7.1211e-09, 3.1834e-20, 6.7140e-20, 4.1112e-24,\n",
            "        0.0000e+00, 2.4221e-27, 6.4269e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        2.4269e-09, 1.7683e-12, 5.1518e-28, 7.8825e-09, 0.0000e+00, 8.2735e-09,\n",
            "        4.7567e-09, 5.0534e-20, 1.1215e-08, 1.6707e-15, 3.6883e-12, 6.4324e-09,\n",
            "        3.6205e-15, 8.1276e-09, 1.8696e-28, 2.4242e-14, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8049e-22, 2.3362e-15, 0.0000e+00,\n",
            "        2.8739e-17, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1261e-09, 5.5964e-28,\n",
            "        8.3394e-09, 2.5248e-15, 2.0281e-15, 4.0792e-28, 0.0000e+00, 3.9154e-09,\n",
            "        1.3052e-24, 1.2153e-17])}, 4: {'step': tensor(49937.), 'exp_avg': tensor([[ 2.5525e-05,  0.0000e+00,  0.0000e+00,  ...,  2.2876e-05,\n",
            "          5.6052e-45,  5.6052e-45],\n",
            "        [ 2.5763e-05,  0.0000e+00,  0.0000e+00,  ...,  2.2681e-05,\n",
            "         -5.6052e-45,  5.6052e-45],\n",
            "        [ 1.6973e-05,  0.0000e+00,  5.6052e-45,  ...,  1.5031e-05,\n",
            "         -5.6052e-45,  5.6052e-45],\n",
            "        ...,\n",
            "        [ 1.6150e-06,  0.0000e+00,  0.0000e+00,  ...,  1.5767e-06,\n",
            "          5.6052e-45,  5.6052e-45],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]]), 'exp_avg_sq': tensor([[3.9228e-09, 0.0000e+00, 0.0000e+00,  ..., 2.9795e-09, 4.5878e-28,\n",
            "         2.1170e-20],\n",
            "        [1.5152e-08, 0.0000e+00, 0.0000e+00,  ..., 1.1592e-08, 3.5335e-28,\n",
            "         3.6368e-20],\n",
            "        [1.5477e-09, 0.0000e+00, 9.8177e-38,  ..., 1.2199e-09, 1.9748e-27,\n",
            "         2.4832e-20],\n",
            "        ...,\n",
            "        [2.3545e-09, 0.0000e+00, 0.0000e+00,  ..., 1.9362e-09, 9.5677e-30,\n",
            "         8.9145e-21],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00]])}, 5: {'step': tensor(49937.), 'exp_avg': tensor([ 1.3478e-04,  1.3423e-04,  8.9195e-05,  4.1556e-05, -6.1052e-05,\n",
            "         7.3734e-05, -2.6183e-06,  1.3650e-04,  4.1146e-05, -1.1249e-04,\n",
            "         1.2569e-05, -3.3414e-05,  1.2364e-04,  1.8206e-04,  1.4436e-05,\n",
            "        -6.0661e-05,  1.1686e-05,  2.3473e-05, -6.8896e-06,  6.0343e-05,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  7.9318e-06,  0.0000e+00,  0.0000e+00]), 'exp_avg_sq': tensor([7.2354e-08, 2.9962e-07, 3.0018e-08, 2.3606e-08, 2.0981e-08, 3.1120e-07,\n",
            "        8.2059e-09, 2.5348e-07, 3.6564e-08, 3.0303e-07, 1.6589e-08, 9.0815e-08,\n",
            "        9.5046e-08, 2.0833e-07, 5.2561e-09, 7.5990e-08, 1.5753e-08, 3.6749e-08,\n",
            "        2.3412e-08, 3.3279e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 4.6520e-08, 0.0000e+00, 0.0000e+00])}}\n",
            "param_groups \t [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in q_network.state_dict():\n",
        "    print(param_tensor, \"\\t\", q_network.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#SAVING THE NETWORK.\n",
        "import os\n",
        "\n",
        "cwd = os.getcwd()\n",
        "# Assume `model` is your neural network\n",
        "torch.save(q_network.state_dict(), cwd +'qnetwork.pth')\n",
        "# Assume `model` is your neural network\n",
        "torch.save(target_network.state_dict(), cwd+'target_network.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "# Save the deque to a file using pickle\n",
        "with open('results.pkl', 'wb') as file:\n",
        "    pickle.dump(results, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 0.010051870182419265, 0.9019117205484937),\n",
              " (1, -2.513966098771636e-05, 0.901886580887506),\n",
              " (2, -0.002016349067066714, 0.8998702318204392),\n",
              " (3, 0.004005190727182528, 0.9038754225476218),\n",
              " (4, -0.0020171391864481913, 0.9018582833611736),\n",
              " (5, -0.003950166055598614, 0.897908117305575),\n",
              " (6, 0.005967305242046805, 0.9038754225476218),\n",
              " (7, 0.0, 0.9038754225476218),\n",
              " (8, -0.0019228775158264266, 0.9019525450317953),\n",
              " (9, -0.0020550499968795544, 0.8998974950349158),\n",
              " (10, 2.4043451257393933e-05, 0.8999215384861732),\n",
              " (11, 0.00594252922760552, 0.9058640677137787),\n",
              " (12, -0.008044313667641356, 0.8978197540461373),\n",
              " (13, -0.004085418206538294, 0.893734335839599),\n",
              " (14, -0.002027159635142217, 0.8917071762044568),\n",
              " (15, 0.014156891509321867, 0.9058640677137787),\n",
              " (16, -0.01812271162936019, 0.8877413560844185),\n",
              " (17, 0.01812271162936019, 0.9058640677137787),\n",
              " (18, -0.008044313667641356, 0.8978197540461373),\n",
              " (19, 6.219748983826712e-05, 0.8978819515359756),\n",
              " (20, 0.0059934710116461565, 0.9038754225476218),\n",
              " (21, -0.008157401017404031, 0.8957180215302177),\n",
              " (22, 0.004203516955955444, 0.8999215384861732),\n",
              " (23, -0.008131032065743282, 0.8917905064204299),\n",
              " (24, 0.0020868157640153706, 0.8938773221844453),\n",
              " (25, 0.0080343983640484, 0.9019117205484937),\n",
              " (26, 0.001963701999128098, 0.9038754225476218),\n",
              " (27, 0.0020609904675764623, 0.9059364130151982),\n",
              " (28, -0.00402469246670456, 0.9019117205484937),\n",
              " (29, 0.0120108097287559, 0.9139225302772496),\n",
              " (30, -0.009997853330523387, 0.9039246769467262),\n",
              " (31, -0.004003138460553002, 0.8999215384861732),\n",
              " (32, 0.006014874529025049, 0.9059364130151982),\n",
              " (33, -4.5202776161934466e-05, 0.9058912102390363),\n",
              " (34, -0.005969671752863115, 0.8999215384861732),\n",
              " (35, 0.003980056747345917, 0.9039015952335191),\n",
              " (36, 0.0019322957501761717, 0.9058338909836953),\n",
              " (37, -0.0019322957501761717, 0.9039015952335191),\n",
              " (38, -0.008036554140262164, 0.8958650410932569),\n",
              " (39, 0.012040654338865586, 0.9079056954321225),\n",
              " (40, -0.006047412070948943, 0.9018582833611736),\n",
              " (41, -0.006060017919278016, 0.8957982654418956),\n",
              " (42, 0.014120661592435302, 0.9099189270343309),\n",
              " (43, -0.014025532198018498, 0.8958933948363124),\n",
              " (44, 0.012012300595810155, 0.9079056954321225),\n",
              " (45, -0.0019903716407102534, 0.9059153237914123),\n",
              " (46, -0.0019906468446860837, 0.9039246769467262),\n",
              " (47, -2.3081713207084853e-05, 0.9039015952335191),\n",
              " (48, 0.001989615005517198, 0.9058912102390363),\n",
              " (49, -0.010057876905702923, 0.8958333333333334),\n",
              " (50, 0.00798045015149318, 0.9038137834848265),\n",
              " (51, 0.004038780617737547, 0.9078525641025641),\n",
              " (52, -0.009885622813586403, 0.8979669412889777),\n",
              " (53, -0.00011444228040935478, 0.8978524990085683),\n",
              " (54, 5.561829700662102e-05, 0.897908117305575),\n",
              " (55, 0.003978463581930991, 0.901886580887506),\n",
              " (56, 0.004004629351530342, 0.9058912102390363),\n",
              " (57, -0.004032926877862719, 0.9018582833611736),\n",
              " (58, -0.004005784352605235, 0.8978524990085683),\n",
              " (59, -0.008011568705210248, 0.8898409303033581),\n",
              " (60, 0.014034492244263674, 0.9038754225476218),\n",
              " (61, -0.00204860003270535, 0.9018268225149164),\n",
              " (62, 0.006120154944099898, 0.9079469774590163),\n",
              " (63, -0.0060603965715103625, 0.901886580887506),\n",
              " (64, -0.008039866231542736, 0.8938467146559632),\n",
              " (65, 0.01005488057755588, 0.9039015952335191),\n",
              " (66, -0.006019643697543486, 0.8978819515359756),\n",
              " (67, -0.004069265958617119, 0.8938126855773585),\n",
              " (68, -0.0020620922102406736, 0.8917505933671178),\n",
              " (69, 0.008170945119055362, 0.8999215384861732),\n",
              " (70, -0.0020690394776048393, 0.8978524990085683),\n",
              " (71, 0.008011568705210359, 0.9058640677137787),\n",
              " (72, -0.005993835893339461, 0.8998702318204392),\n",
              " (73, -0.006023517164476022, 0.8938467146559632),\n",
              " (74, 0.014058980776159302, 0.9079056954321225),\n",
              " (75, 0.0019352640203516014, 0.9098409594524741),\n",
              " (76, -0.001960346177670047, 0.9078806132748041),\n",
              " (77, -0.01599131863709502, 0.891889294637709),\n",
              " (78, 0.011956859208444803, 0.9038461538461539),\n",
              " (79, 0.008039650155832745, 0.9118858040019866),\n",
              " (80, -0.010027520640813026, 0.9018582833611736),\n",
              " (81, -0.001988051540734337, 0.8998702318204392),\n",
              " (82, 0.001988051540734337, 0.9018582833611736),\n",
              " (83, -0.0019607883262577896, 0.8998974950349158),\n",
              " (84, -0.0020449960263474454, 0.8978524990085683),\n",
              " (85, 0.0060921731225791476, 0.9039446721311475),\n",
              " (86, -0.0061609618000869615, 0.8977837103310605),\n",
              " (87, -0.001918669237803594, 0.8958650410932569),\n",
              " (88, 7.502038215290163e-05, 0.8959400614754098),\n",
              " (89, 0.007906092370744022, 0.9038461538461539),\n",
              " (90, -0.016104797761735345, 0.8877413560844185),\n",
              " (91, 0.020186459322860784, 0.9079278154072793),\n",
              " (92, -0.014152587024020336, 0.893775228383259),\n",
              " (93, 0.004044525662878384, 0.8978197540461373),\n",
              " (94, 0.004038529315036232, 0.9018582833611736),\n",
              " (95, 0.010090999425711678, 0.9119492827868853),\n",
              " (96, -0.010037562238391584, 0.9019117205484937),\n",
              " (97, -0.006078387215160297, 0.8958333333333334),\n",
              " (98, 0.008042089214288395, 0.9038754225476218),\n",
              " (99, -0.003977927512705981, 0.8998974950349158),\n",
              " (100, 0.0020550499968795544, 0.9019525450317953),\n",
              " (101, -0.006059150195482976, 0.8958933948363124),\n",
              " (102, 0.0020147224692625976, 0.897908117305575),\n",
              " (103, 0.007983092933461333, 0.9058912102390363),\n",
              " (104, -0.004032926877862719, 0.9018582833611736),\n",
              " (105, -0.0019607883262577896, 0.8998974950349158),\n",
              " (106, 0.005993715204120509, 0.9058912102390363),\n",
              " (107, -0.004004629351530342, 0.901886580887506),\n",
              " (108, -0.003935972793188358, 0.8979506080943176),\n",
              " (109, 0.005974068852408587, 0.9039246769467262),\n",
              " (110, -0.004155609014209638, 0.8997690679325165),\n",
              " (111, 0.006122142306519751, 0.9058912102390363),\n",
              " (112, -0.003979489690542626, 0.9019117205484937),\n",
              " (113, -0.004003603242918707, 0.897908117305575),\n",
              " (114, -0.004061402649611745, 0.8938467146559632),\n",
              " (115, -3.4029078604724816e-05, 0.8938126855773585),\n",
              " (116, 0.0020807092589538723, 0.8958933948363124),\n",
              " (117, 0.0019885566996632464, 0.8978819515359756),\n",
              " (118, 0.004004629351530342, 0.901886580887506),\n",
              " (119, -0.005993186051193589, 0.8958933948363124),\n",
              " (120, -0.004066471759389234, 0.8918269230769231),\n",
              " (121, 0.010059657810582823, 0.901886580887506),\n",
              " (122, -0.004004629351530342, 0.8978819515359756),\n",
              " (123, 0.0020155434989401755, 0.8998974950349158),\n",
              " (124, 2.4043451257393933e-05, 0.8999215384861732),\n",
              " (125, -0.0020395869501975694, 0.8978819515359756),\n",
              " (126, -6.219748983826712e-05, 0.8978197540461373),\n",
              " (127, -0.001954712952880411, 0.8958650410932569),\n",
              " (128, 0.001987457915311408, 0.8978524990085683),\n",
              " (129, 5.561829700662102e-05, 0.897908117305575),\n",
              " (130, -0.004030795121129693, 0.8938773221844453),\n",
              " (131, 0.0080343983640484, 0.9019117205484937),\n",
              " (132, 0.003979489690542626, 0.9058912102390363),\n",
              " (133, -0.005993715204120509, 0.8998974950349158),\n",
              " (134, -5.775144517217168e-05, 0.8998397435897436),\n",
              " (135, -0.003974702496486682, 0.8958650410932569),\n",
              " (136, 0.001918669237803594, 0.8977837103310605),\n",
              " (137, -0.001918669237803594, 0.8958650410932569),\n",
              " (138, 0.012062774314022362, 0.9079278154072793),\n",
              " (139, -0.006069532046105719, 0.9018582833611736),\n",
              " (140, -0.004038529315036232, 0.8978197540461373),\n",
              " (141, -0.0019864207128039713, 0.8958333333333334),\n",
              " (142, 0.008068261900185725, 0.9039015952335191),\n",
              " (143, 0.0019624724802596027, 0.9058640677137787),\n",
              " (144, -0.002017913867624843, 0.9038461538461539),\n",
              " (145, 2.9268701467910496e-05, 0.9038754225476218),\n",
              " (146, -0.008077157105726207, 0.8957982654418956),\n",
              " (147, 0.0020214886042417834, 0.8978197540461373),\n",
              " (148, 0.004007068468779074, 0.9018268225149164),\n",
              " (149, 0.0, 0.9018268225149164),\n",
              " (150, -0.001929327480000631, 0.8998974950349158),\n",
              " (151, -0.006084809457557294, 0.8938126855773585),\n",
              " (152, 0.007979506701503603, 0.9017921922788621),\n",
              " (153, 0.002053961567291762, 0.9038461538461539),\n",
              " (154, -0.004006410256410242, 0.8998397435897436),\n",
              " (155, 0.008065951842378905, 0.9079056954321225),\n",
              " (156, -0.004030272884500752, 0.9038754225476218),\n",
              " (157, -0.008010381454364834, 0.8958650410932569),\n",
              " (158, 0.00805963585346925, 0.9039246769467262),\n",
              " (159, -0.012097753869803052, 0.8918269230769231),\n",
              " (160, 0.0060550284590524805, 0.8978819515359756),\n",
              " (161, 0.008009258703060684, 0.9058912102390363),\n",
              " (162, -5.731925534102622e-05, 0.9058338909836953),\n",
              " (163, -0.005936395948779483, 0.8998974950349158),\n",
              " (164, -0.008008200397206733, 0.891889294637709),\n",
              " (165, 0.00593045940842829, 0.8978197540461373),\n",
              " (166, 0.002019989543606271, 0.8998397435897436),\n",
              " (167, -0.0019316262841686527, 0.897908117305575),\n",
              " (168, -0.004061402649611745, 0.8938467146559632),\n",
              " (169, 0.014100262803053099, 0.9079469774590163),\n",
              " (170, -0.010094478450447975, 0.8978524990085683),\n",
              " (171, 0.010094478450447975, 0.9079469774590163),\n",
              " (172, 0.0019228746073676195, 0.9098698520663839),\n",
              " (173, -0.014004810973127002, 0.8958650410932569),\n",
              " (174, 0.002016910442718678, 0.8978819515359756),\n",
              " (175, 0.001957792053768004, 0.8998397435897436),\n",
              " (176, -0.005962421405298346, 0.8938773221844453),\n",
              " (177, 0.0019877189088116642, 0.8958650410932569),\n",
              " (178, 0.01405388594107393, 0.9099189270343309),\n",
              " (179, -0.009997388548157682, 0.8999215384861732),\n",
              " (180, 0.001905284028743237, 0.9018268225149164),\n",
              " (181, 8.489803357725201e-05, 0.9019117205484937),\n",
              " (182, 0.00599397488362885, 0.9079056954321225),\n",
              " (183, -0.008099670968761519, 0.899806024463361),\n",
              " (184, 0.004069398084260767, 0.9038754225476218),\n",
              " (185, 2.6172685897329906e-05, 0.9039015952335191),\n",
              " (186, -0.008008200397206733, 0.8958933948363124),\n",
              " (187, 0.009940496147382905, 0.9058338909836953),\n",
              " (188, 0.00207180444842725, 0.9079056954321225),\n",
              " (189, -0.006019114544616566, 0.901886580887506),\n",
              " (190, 0.0060603965715103625, 0.9079469774590163),\n",
              " (191, -0.010127223412878972, 0.8978197540461373),\n",
              " (192, 0.012119364798201415, 0.9099391188443388),\n",
              " (193, -0.008005411658280948, 0.9019337071860578),\n",
              " (194, 0.0018800762987687403, 0.9038137834848265),\n",
              " (195, -0.0019020629363328823, 0.9019117205484937),\n",
              " (196, -0.006078387215160297, 0.8958333333333334),\n",
              " (197, 3.170775992356045e-05, 0.8958650410932569),\n",
              " (198, 0.001987457915311408, 0.8978524990085683),\n",
              " (199, -0.0019591041722559766, 0.8958933948363124),\n",
              " (200, -0.0020466801803491474, 0.8938467146559632),\n",
              " (201, 0.0019866186773701555, 0.8958333333333334),\n",
              " (202, 0.008012820512820484, 0.9038461538461539),\n",
              " (203, -0.009968831661708588, 0.8938773221844453),\n",
              " (204, 0.005962421405298346, 0.8998397435897436),\n",
              " (205, -0.0019872445811752737, 0.8978524990085683),\n",
              " (206, 0.0020449960263474454, 0.8998974950349158),\n",
              " (207, -0.0020449960263474454, 0.8978524990085683),\n",
              " (208, 5.561829700662102e-05, 0.897908117305575),\n",
              " (209, 0.004003603242918707, 0.9019117205484937),\n",
              " (210, 0.0, 0.9019117205484937),\n",
              " (211, -0.008099034971135177, 0.8938126855773585),\n",
              " (212, 0.014134291881657823, 0.9079469774590163),\n",
              " (213, -0.008025438972843135, 0.8999215384861732),\n",
              " (214, -0.00412327304427762, 0.8957982654418956),\n",
              " (215, 0.004099229593020226, 0.8998974950349158),\n",
              " (216, 0.010058923871835046, 0.9099564189067508),\n",
              " (217, -0.00806983801924488, 0.901886580887506),\n",
              " (218, -0.005993186051193589, 0.8958933948363124),\n",
              " (219, 0.010021928955099901, 0.9059153237914123),\n",
              " (220, -0.008033372255436655, 0.8978819515359756),\n",
              " (221, 0.008072530433297476, 0.9059544819692731),\n",
              " (222, 0.0019512134628494326, 0.9079056954321225),\n",
              " (223, -0.002014485193086224, 0.9058912102390363),\n",
              " (224, -0.001989615005517198, 0.9039015952335191),\n",
              " (225, -0.001989874685025428, 0.9019117205484937),\n",
              " (226, 0.001963701999128098, 0.9038754225476218),\n",
              " (227, -0.007982027711309403, 0.8958933948363124),\n",
              " (228, 0.0019263592098249793, 0.8978197540461373),\n",
              " (229, 0.0020777409887784426, 0.8998974950349158),\n",
              " (230, 0.0039486588112380705, 0.9038461538461539),\n",
              " (231, -0.0039486588112380705, 0.8998974950349158),\n",
              " (232, -5.775144517217168e-05, 0.8998397435897436),\n",
              " (233, 0.008012820512820484, 0.9078525641025641),\n",
              " (234, 2.8049172239974496e-05, 0.9078806132748041),\n",
              " (235, -0.0020467222911088045, 0.9058338909836953),\n",
              " (236, 8.143280771699679e-05, 0.9059153237914123),\n",
              " (237, -0.0019906468446860837, 0.9039246769467262),\n",
              " (238, 0.0, 0.9039246769467262),\n",
              " (239, -4.925439910441476e-05, 0.9038754225476218),\n",
              " (240, -0.003977927512705981, 0.8998974950349158),\n",
              " (241, 0.002036212151142025, 0.9019337071860578),\n",
              " (242, 0.0, 0.9019337071860578),\n",
              " (243, -0.000106884671141394, 0.9018268225149164),\n",
              " (244, 0.004088501276495848, 0.9059153237914123),\n",
              " (245, -0.004003603242918596, 0.9019117205484937),\n",
              " (246, -0.0020719769587500547, 0.8998397435897436),\n",
              " (247, 0.004061851643775483, 0.9039015952335191),\n",
              " (248, 0.0019322957501761717, 0.9058338909836953),\n",
              " (249, -0.003975607622521693, 0.9018582833611736),\n",
              " (250, 0.0020863887699739125, 0.9039446721311475),\n",
              " (251, -6.924958352572119e-05, 0.9038754225476218),\n",
              " (252, 2.6172685897329906e-05, 0.9039015952335191),\n",
              " (253, -0.008068261900185725, 0.8958333333333334),\n",
              " (254, -3.506789143781219e-05, 0.8957982654418956),\n",
              " (255, 0.00400775902146544, 0.899806024463361),\n",
              " (256, 0.004095570770158097, 0.9039015952335191),\n",
              " (257, -0.0020150143460131442, 0.901886580887506),\n",
              " (258, -0.004066826841368609, 0.8978197540461373),\n",
              " (259, -0.003942431861692075, 0.8938773221844453),\n",
              " (260, 0.010024273049073829, 0.9039015952335191),\n",
              " (261, 2.3081713207084853e-05, 0.9039246769467262),\n",
              " (262, -0.0020978544318097647, 0.9018268225149164),\n",
              " (263, 8.489803357725201e-05, 0.9019117205484937),\n",
              " (264, 0.007984119042073079, 0.9098958395905667),\n",
              " (265, -0.0019488621315504329, 0.9079469774590163),\n",
              " (266, -0.008140952995655315, 0.899806024463361),\n",
              " (267, 0.008046539639203099, 0.9078525641025641),\n",
              " (268, -0.005965983215058146, 0.901886580887506),\n",
              " (269, 0.0020150143460131442, 0.9039015952335191),\n",
              " (270, -0.007961533758109263, 0.8959400614754098),\n",
              " (271, 0.01188148103088238, 0.9078215425062922),\n",
              " (272, -0.009913425200717252, 0.897908117305575),\n",
              " (273, -0.006157523938457143, 0.8917505933671178),\n",
              " (274, 0.008170945119055362, 0.8999215384861732),\n",
              " (275, 0.003953884061448587, 0.9038754225476218),\n",
              " (276, -0.009998100363176499, 0.8938773221844453),\n",
              " (277, 0.003942431861692075, 0.8978197540461373),\n",
              " (278, 8.836325943761825e-05, 0.897908117305575),\n",
              " (279, 0.007983092933461333, 0.9058912102390363),\n",
              " (280, -0.006020978418597056, 0.8998702318204392),\n",
              " (281, 0.008076745638577076, 0.9079469774590163),\n",
              " (282, -0.018141360350436786, 0.8898056171085795),\n",
              " (283, 0.0019848893118503685, 0.8917905064204299),\n",
              " (284, 0.008079725400009341, 0.8998702318204392),\n",
              " (285, -0.006057546243080747, 0.8938126855773585),\n",
              " (286, 0.01000109790746806, 0.9038137834848265),\n",
              " (287, -0.0019555001236529757, 0.9018582833611736),\n",
              " (288, 0.00405704043023869, 0.9059153237914123),\n",
              " (289, 0.004003603242918596, 0.9099189270343309),\n",
              " (290, 0.0020120268335015457, 0.9119309538678324),\n",
              " (291, -0.0020351142772656594, 0.9098958395905667),\n",
              " (292, -0.009974301104393568, 0.8999215384861732),\n",
              " (293, -0.004028143649860816, 0.8958933948363124),\n",
              " (294, 0.01203442057096693, 0.9079278154072793),\n",
              " (295, -0.004026220173760198, 0.9039015952335191),\n",
              " (296, 0.0020137285578931685, 0.9059153237914123),\n",
              " (297, -0.002069169945258409, 0.9038461538461539),\n",
              " (298, 0.0061102650605969755, 0.9099564189067508),\n",
              " (299, -0.002009441447734517, 0.9079469774590163),\n",
              " (300, -0.010065025923040705, 0.8978819515359756),\n",
              " (301, 0.006019643697543486, 0.9039015952335191),\n",
              " (302, -0.008036554140262164, 0.8958650410932569),\n",
              " (303, -0.004038118016333803, 0.8918269230769231),\n",
              " (304, 0.012019230769230727, 0.9038461538461539),\n",
              " (305, -0.008012820512820484, 0.8958333333333334),\n",
              " (306, 0.0019864207128039713, 0.8978197540461373),\n",
              " (307, 0.0020777409887784426, 0.8998974950349158),\n",
              " (308, 0.0060569869343573, 0.9059544819692731),\n",
              " (309, -9.041425549438653e-05, 0.9058640677137787),\n",
              " (310, 0.002041627718343819, 0.9079056954321225),\n",
              " (311, -0.0019512134628494326, 0.9059544819692731),\n",
              " (312, -0.0020528867357539893, 0.9039015952335191),\n",
              " (313, -0.00207477271860268, 0.9018268225149164),\n",
              " (314, -0.003974323506348076, 0.8978524990085683),\n",
              " (315, 0.004005784352605235, 0.9018582833611736),\n",
              " (316, 0.004005784352605124, 0.9058640677137787),\n",
              " (317, 5.1256077633565766e-05, 0.9059153237914123),\n",
              " (318, 3.9158177860820764e-05, 0.9059544819692731),\n",
              " (319, -0.008101982960704746, 0.8978524990085683),\n",
              " (320, -0.004039813431209849, 0.8938126855773585),\n",
              " (321, 3.4029078604724816e-05, 0.8938467146559632),\n",
              " (322, 0.01002870789165855, 0.9038754225476218),\n",
              " (323, -0.003977927512705981, 0.8998974950349158),\n",
              " (324, -0.006050780378952569, 0.8938467146559632),\n",
              " (325, 0.0019131159938539088, 0.8957598306498171),\n",
              " (326, 0.0041376643850986605, 0.8998974950349158),\n",
              " (327, 0.0019890858525901667, 0.901886580887506),\n",
              " (328, -0.004034081878937612, 0.8978524990085683),\n",
              " (329, -0.004005784352605124, 0.8938467146559632),\n",
              " (330, 0.01002870789165855, 0.9038754225476218),\n",
              " (331, 0.0039461199586704465, 0.9078215425062922),\n",
              " (332, -0.011956501413035281, 0.8958650410932569),\n",
              " (333, 0.007981112752896924, 0.9038461538461539),\n",
              " (334, 0.002069169945258409, 0.9059153237914123),\n",
              " (335, -0.006017828756496479, 0.8998974950349158),\n",
              " (336, 0.00804948242410053, 0.9079469774590163),\n",
              " (337, -0.00804948242410053, 0.8998974950349158),\n",
              " (338, -0.0019893777293408244, 0.897908117305575),\n",
              " (339, -0.0020147224692625976, 0.8958933948363124),\n",
              " (340, 0.0, 0.8958933948363124),\n",
              " (341, 0.004028143649860816, 0.8999215384861732),\n",
              " (342, 0.001990182062320489, 0.9019117205484937),\n",
              " (343, -0.010084797471570539, 0.8918269230769231),\n",
              " (344, 0.0, 0.8918269230769231),\n",
              " (345, 0.008070571957992656, 0.8998974950349158),\n",
              " (346, -0.004004100198603422, 0.8958933948363124),\n",
              " (347, 0.0019591041722559766, 0.8978524990085683),\n",
              " (348, 0.008011568705210359, 0.9058640677137787),\n",
              " (349, -0.006024324124035085, 0.8998397435897436),\n",
              " (350, 8.179489642956561e-05, 0.8999215384861732),\n",
              " (351, 0.0039246153599806766, 0.9038461538461539),\n",
              " (352, -0.011986303480079452, 0.8918598503660744),\n",
              " (353, 0.010026730521431548, 0.901886580887506),\n",
              " (354, 4.712629855185835e-05, 0.9019337071860578),\n",
              " (355, -0.012092776882699718, 0.8898409303033581),\n",
              " (356, 0.010080608182815087, 0.8999215384861732),\n",
              " (357, -8.179489642956561e-05, 0.8998397435897436),\n",
              " (358, 0.008123441684366073, 0.9079631852741097),\n",
              " (359, -0.0021292942904144185, 0.9058338909836953),\n",
              " (360, 0.00012059098557781756, 0.9059544819692731),\n",
              " (361, 0.0019733334380062084, 0.9079278154072793),\n",
              " (362, -0.005994108221221484, 0.9019337071860578),\n",
              " (363, 0.008005411658280948, 0.9099391188443388),\n",
              " (364, -0.008080835483165183, 0.9018582833611736),\n",
              " (365, -0.007929987833396823, 0.8939282955277768),\n",
              " (366, 0.0059932429583964275, 0.8999215384861732),\n",
              " (367, 0.0018706537926889144, 0.9017921922788621),\n",
              " (368, 0.010117603351863735, 0.9119097956307258),\n",
              " (369, -0.0200828725538027, 0.8918269230769231),\n",
              " (370, 0.0141094899382751, 0.9059364130151982),\n",
              " (371, -0.008152702684137703, 0.8977837103310605),\n",
              " (372, 3.6043715076816696e-05, 0.8978197540461373),\n",
              " (373, 0.002019989543606271, 0.8998397435897436),\n",
              " (374, -0.0060270580123851225, 0.8938126855773585),\n",
              " (375, 0.010088909656160605, 0.9039015952335191),\n",
              " (376, -0.003959228430240436, 0.8999423668032787),\n",
              " (377, 0.009996752041060097, 0.9099391188443388),\n",
              " (378, -0.01006888702389952, 0.8998702318204392),\n",
              " (379, -0.0020504777743018954, 0.8978197540461373),\n",
              " (380, 0.004066826841368609, 0.901886580887506),\n",
              " (381, 0.0, 0.901886580887506),\n",
              " (382, 0.0019888416601158143, 0.9038754225476218),\n",
              " (383, -0.003977927512705981, 0.8998974950349158),\n",
              " (384, 0.003977927512705981, 0.9038754225476218),\n",
              " (385, 0.0020399012437904984, 0.9059153237914123),\n",
              " (386, -0.0019906468446860837, 0.9039246769467262),\n",
              " (387, -0.004054445126286943, 0.8998702318204392),\n",
              " (388, 0.0060661811947589905, 0.9059364130151982),\n",
              " (389, -0.006038917980282443, 0.8998974950349158),\n",
              " (390, 0.008008200397206733, 0.9079056954321225),\n",
              " (391, -0.0161551020650047, 0.8917505933671178),\n",
              " (392, 0.00010925699895658614, 0.8918598503660744),\n",
              " (393, 0.0019528352112840874, 0.8938126855773585),\n",
              " (394, 0.010088909656160605, 0.9039015952335191),\n",
              " (395, -0.003980056747345917, 0.8999215384861732),\n",
              " (396, 0.007900004020119034, 0.9078215425062922),\n",
              " (397, -0.0039461199586704465, 0.9038754225476218),\n",
              " (398, -0.004005190727182528, 0.8998702318204392),\n",
              " (399, -0.001962114514864277, 0.897908117305575),\n",
              " (400, 0.005938036540578895, 0.9038461538461539),\n",
              " (401, 0.004059541585968662, 0.9079056954321225),\n",
              " (402, -0.009955087337804924, 0.8979506080943176),\n",
              " (403, -6.865655834198403e-05, 0.8978819515359756),\n",
              " (404, -0.002016910442718678, 0.8958650410932569),\n",
              " (405, -0.00602411078989884, 0.8898409303033581),\n",
              " (406, 0.010029301517081146, 0.8998702318204392),\n",
              " (407, -0.002017732811870898, 0.8978524990085683),\n",
              " (408, 0.003939693270293754, 0.9017921922788621),\n",
              " (409, 0.014121703550467046, 0.9159138958293291),\n",
              " (410, -0.01803194429335353, 0.8978819515359756),\n",
              " (411, -6.219748983826712e-05, 0.8978197540461373),\n",
              " (412, 0.004038529315036232, 0.9018582833611736),\n",
              " (413, 0.001987870484980281, 0.9038461538461539),\n",
              " (414, -0.0019344332976601875, 0.9019117205484937),\n",
              " (415, -0.002105696085132669, 0.899806024463361),\n",
              " (416, -0.008015518042931102, 0.8917905064204299),\n",
              " (417, 0.004074534672827035, 0.8958650410932569),\n",
              " (418, -6.677565136137265e-05, 0.8957982654418956),\n",
              " (419, 0.0020836860940800506, 0.8978819515359756),\n",
              " (420, 0.0040297690125180585, 0.9019117205484937),\n",
              " (421, 0.003979489690542626, 0.9058912102390363),\n",
              " (422, -0.012013888054591026, 0.8938773221844453),\n",
              " (423, 0.01397524191811883, 0.9078525641025641),\n",
              " (424, -0.005965983215058146, 0.901886580887506),\n",
              " (425, 0.0060603965715103625, 0.9079469774590163),\n",
              " (426, -0.004022300512290133, 0.9039246769467262),\n",
              " (427, 0.003955936328077891, 0.9078806132748041),\n",
              " (428, 4.7202132475221426e-05, 0.9079278154072793),\n",
              " (429, -0.006041234519773342, 0.901886580887506),\n",
              " (430, -0.004034081878937612, 0.8978524990085683),\n",
              " (431, 0.009969043497723873, 0.9078215425062922),\n",
              " (432, -0.0039461199586704465, 0.9038754225476218),\n",
              " (433, 0.002015787691414528, 0.9058912102390363),\n",
              " (434, -0.008009258703060684, 0.8978819515359756),\n",
              " (435, 0.0059934710116461565, 0.9038754225476218),\n",
              " (436, -2.9268701467910496e-05, 0.9038461538461539),\n",
              " (437, 0.002017913867624843, 0.9058640677137787),\n",
              " (438, -0.003977486826272747, 0.901886580887506),\n",
              " (439, 0.002038096059220229, 0.9039246769467262),\n",
              " (440, -0.004054445126286943, 0.8998702318204392),\n",
              " (441, 0.00803546361168328, 0.9079056954321225),\n",
              " (442, -0.010085941385985175, 0.8978197540461373),\n",
              " (443, 0.006026399800016513, 0.9038461538461539),\n",
              " (444, -0.002053961567291762, 0.9017921922788621),\n",
              " (445, 3.4630236054322694e-05, 0.9018268225149164),\n",
              " (446, -0.004007068468779074, 0.8978197540461373),\n",
              " (447, 0.008116658969060886, 0.9059364130151982),\n",
              " (448, -0.012123727437839737, 0.8938126855773585),\n",
              " (449, 0.01011199136936769, 0.9039246769467262),\n",
              " (450, -2.3081713207084853e-05, 0.9039015952335191),\n",
              " (451, -0.008103329791623537, 0.8957982654418956),\n",
              " (452, 0.010117058349516705, 0.9059153237914123),\n",
              " (453, -0.002101540306585714, 0.9038137834848265),\n",
              " (454, -0.0019272025973205986, 0.901886580887506),\n",
              " (455, 0.0019888416601158143, 0.9038754225476218),\n",
              " (456, 0.004030272884500752, 0.9079056954321225),\n",
              " (457, -0.003981018485396337, 0.9039246769467262),\n",
              " (458, -0.010077962290762965, 0.8938467146559632),\n",
              " (459, 0.012017353057815483, 0.9058640677137787),\n",
              " (460, -0.001939390767052518, 0.9039246769467262),\n",
              " (461, 0.004003138460553113, 0.9079278154072793),\n",
              " (462, -0.008057583586840056, 0.8998702318204392),\n",
              " (463, -0.0020504777743018954, 0.8978197540461373),\n",
              " (464, 0.0019493138863792003, 0.8997690679325165),\n",
              " (465, 0.004077085913637313, 0.9038461538461539),\n",
              " (466, -0.01606340644718951, 0.8877827473989643),\n",
              " (467, 0.016141929547761835, 0.9039246769467262),\n",
              " (468, -0.0020129563982325127, 0.9019117205484937),\n",
              " (469, -0.006078387215160297, 0.8958333333333334),\n",
              " (470, 0.0060249500278402035, 0.9018582833611736),\n",
              " (471, -0.01804418079707104, 0.8838141025641025),\n",
              " (472, 0.011984162877793025, 0.8957982654418956),\n",
              " (473, 0.014120661592435302, 0.9099189270343309),\n",
              " (474, -0.0120108097287559, 0.897908117305575),\n",
              " (475, 0.0019893777293408244, 0.8998974950349158),\n",
              " (476, 0.0019890858525901667, 0.901886580887506),\n",
              " (477, -0.004066826841368609, 0.8978197540461373),\n",
              " (478, 0.0020777409887784426, 0.8998974950349158),\n",
              " (479, 0.0019890858525901667, 0.901886580887506),\n",
              " (480, 0.0019888416601158143, 0.9038754225476218),\n",
              " (481, 0.006020417042944981, 0.9098958395905667),\n",
              " (482, -0.01810533317013685, 0.8917905064204299),\n",
              " (483, 0.006029247625707446, 0.8978197540461373),\n",
              " (484, 0.006055668501484424, 0.9038754225476218),\n",
              " (485, -0.006022923539053426, 0.8978524990085683),\n",
              " (486, 0.002017732811870898, 0.8998702318204392),\n",
              " (487, 0.014030237121657851, 0.9139004689420971),\n",
              " (488, -0.016080714895959747, 0.8978197540461373),\n",
              " (489, 8.836325943761825e-05, 0.897908117305575),\n",
              " (490, 0.0038840749732871327, 0.9017921922788621),\n",
              " (491, 0.006088420995941979, 0.9078806132748041),\n",
              " (492, -0.008010381454364834, 0.8998702318204392),\n",
              " (493, -0.002017732811870898, 0.8978524990085683),\n",
              " (494, 0.004034081878937612, 0.901886580887506),\n",
              " (495, 0.003947310096189316, 0.9058338909836953),\n",
              " (496, -0.005936395948779483, 0.8998974950349158),\n",
              " (497, 0.0019890858525901667, 0.901886580887506),\n",
              " (498, 0.004082953241551945, 0.9059695341290579),\n",
              " (499, -0.008117035120489557, 0.8978524990085683)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
